{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f072c436",
   "metadata": {},
   "source": [
    "# WEO Economic Data Analysis & Recession Prediction\n",
    "\n",
    "**Objective:** Load World Economic Outlook (WEO) data, clean and transform it, then use machine learning models to predict global recessions.\n",
    "\n",
    "**Workflow:**\n",
    "1. Data loading and cleaning\n",
    "2. Feature engineering and recession flagging\n",
    "3. Exploratory data analysis\n",
    "4. Model training with full and reduced feature sets (comparing 13 vs 5 features)\n",
    "5. Economy-specific analysis (Upper vs Lower economies with both feature sets)\n",
    "6. Future predictions for all scenarios\n",
    "\n",
    "**Models Used:** Logistic Regression, Random Forest, Gradient Boosting, Linear SVM, Decision Tree, and Ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c77a1c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#1)Multiclass classification target distribution - aanpassen -done\n",
    "#2)Threshhold meerdere waardes vullen en dat in Overleaf plaatsen ter discussie -fail\n",
    "#3)ROC, AUC laten zien -done\n",
    "#4) Zoek verder op de auteur naar vergelijkbaar werk -wip\n",
    "#5) Use Arima as baseline -wip\n",
    "#6) Try neural network -wip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0881101a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core data manipulation and visualization\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# File handling\n",
    "import csv\n",
    "from pathlib import Path\n",
    "\n",
    "# Machine learning - model selection and preprocessing\n",
    "from sklearn.model_selection import train_test_split, TimeSeriesSplit\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Machine learning - models\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, VotingClassifier\n",
    "from sklearn.svm import LinearSVC, SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "# Machine learning - metrics\n",
    "from sklearn.metrics import (\n",
    "    classification_report, \n",
    "    accuracy_score, \n",
    "    precision_score, \n",
    "    recall_score, \n",
    "    f1_score, \n",
    "    roc_auc_score,\n",
    "    confusion_matrix,\n",
    "    ConfusionMatrixDisplay\n",
    ")\n",
    "\n",
    "# Imbalanced learning\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.pipeline import Pipeline as ImbPipeline\n",
    "\n",
    "# Optional XGBoost\n",
    "try:\n",
    "    from xgboost import XGBClassifier\n",
    "    HAS_XGB = True\n",
    "except ImportError:\n",
    "    HAS_XGB = False\n",
    "\n",
    "# Optional pycountry for continent mapping\n",
    "try:\n",
    "    import pycountry\n",
    "    import pycountry_convert as pc\n",
    "    HAS_PYCOUNTRY = True\n",
    "except ImportError:\n",
    "    HAS_PYCOUNTRY = False\n",
    "\n",
    "pd.set_option(\"display.max_rows\", 25)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5987b26",
   "metadata": {},
   "source": [
    "# 1. Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e640272a",
   "metadata": {},
   "outputs": [],
   "source": [
    "p = Path(r\"data.csv\")\n",
    "if not p.exists():\n",
    "    raise FileNotFoundError(p)\n",
    "\n",
    "# Detect encoding and delimiter\n",
    "encoding = \"utf-8\"\n",
    "try:\n",
    "    sample = p.read_text(encoding=encoding)[:8192]\n",
    "except UnicodeDecodeError:\n",
    "    encoding = \"latin-1\"\n",
    "    sample = p.read_text(encoding=encoding)[:8192]\n",
    "\n",
    "try:\n",
    "    delim = csv.Sniffer().sniff(sample).delimiter\n",
    "except Exception:\n",
    "    delim = \",\"\n",
    "\n",
    "df = pd.read_csv(p, sep=delim, encoding=encoding, low_memory=False, parse_dates=True)\n",
    "print(\"Shape:\", df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f200ae17",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Number of columns: {df.shape[1]}\")\n",
    "print(f\"Number of rows: {df.shape[0]}\")\n",
    "print(\"\\nColumn names:\", df.columns.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5a530cd",
   "metadata": {},
   "source": [
    "# 2. Data Cleaning & Transformation\n",
    "\n",
    "## Filter to Selected Economic Indicators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be117543",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(columns=[\"WEO Country Code\", \"ISO\", \"Country/Series-specific Notes\", \"Subject Notes\", \n",
    "                 \"Units\", \"Scale\", \"Estimates Start After\", \"Subject Descriptor\"], inplace=True)\n",
    "\n",
    "codes = {\n",
    "    # Core growth & external\n",
    "    \"NGDP_RPCH\", \"NGDPRPC\", \"PCPIPCH\", \"TX_RPCH\", \"TM_RPCH\", \"BCA_NGDPD\",\n",
    "    # Fiscal & debt aggregates\n",
    "    \"GGR_NGDP\", \"GGX_NGDP\", \"GGXWDN_NGDP\", \"GGXWDG_NGDP\",\n",
    "    # Savings & investment\n",
    "    \"NGSD_NGDP\", \"NID_NGDP\",\n",
    "    # Prices\n",
    "    \"PCPI\"\n",
    "}\n",
    "\n",
    "col = \"WEO Subject Code\"\n",
    "\n",
    "if col not in df.columns:\n",
    "    raise KeyError(f\"Column {col!r} not found in dataframe\")\n",
    "\n",
    "df = df[df[col].astype(str).str.strip().isin(codes)].copy()\n",
    "print(\"shape after filter:\", df.shape)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60010051",
   "metadata": {},
   "source": [
    "## Data Reshaping: Wide to Long to Wide"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab8289b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "year_cols = df.columns[2:]\n",
    "\n",
    "df[year_cols] = df[year_cols].replace({',': ''}, regex=True)\n",
    "df[year_cols] = df[year_cols].apply(pd.to_numeric, errors=\"coerce\")\n",
    "\n",
    "df[\"Country\"] = (\n",
    "    df[\"Country\"]\n",
    "    .str.replace(\" \", \"_\")\n",
    "    .str.replace(\"'\", \"\")\n",
    "    .str.replace(\"-\", \"_\")\n",
    ")\n",
    "\n",
    "df_long = df.melt(id_vars=[\"WEO Subject Code\", \"Country\"],\n",
    "                  var_name=\"Year\", value_name=\"Value\")\n",
    "\n",
    "df_long[\"Year\"] = df_long[\"Year\"].astype(str).str.strip()\n",
    "df_long = df_long[df_long[\"Year\"].str.fullmatch(r\"\\d{4}\")].copy()\n",
    "df_long[\"Year\"] = df_long[\"Year\"].astype(int)\n",
    "\n",
    "df_long[\"Value\"] = (\n",
    "    df_long[\"Value\"].astype(str)\n",
    "    .str.replace(\",\", \"\")\n",
    "    .replace({\"\": None, \"nan\": None})\n",
    "    .astype(float)\n",
    ")\n",
    "\n",
    "df_pivot = df_long.pivot_table(\n",
    "    index=[\"Country\", \"Year\"],\n",
    "    columns=\"WEO Subject Code\",\n",
    "    values=\"Value\",\n",
    "    aggfunc=\"first\"\n",
    ").reset_index()\n",
    "\n",
    "df_pivot.columns.name = None\n",
    "df_pivot = df_pivot.set_index(\"Year\")\n",
    "\n",
    "df_pivot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e2b1eba",
   "metadata": {},
   "source": [
    "# 3. Feature Engineering\n",
    "\n",
    "## Add Recession Target Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d940258",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Step 1: IMF-recognized global recession years ---\n",
    "global_recession_years = [1982, 1991, 2009, 2020]\n",
    "\n",
    "# --- Step 2: Ensure chronological order ---\n",
    "df_pivot = df_pivot.sort_index()\n",
    "\n",
    "# --- Step 3: Construct diagnostic flags (not stored in df) ---\n",
    "\n",
    "# GDP-based recession (two consecutive annual declines)\n",
    "flag_gdp = (\n",
    "    (df_pivot.groupby(\"Country\")[\"NGDPRPC\"].transform(lambda x: x.pct_change() < 0)) &\n",
    "    (df_pivot.groupby(\"Country\")[\"NGDPRPC\"].transform(lambda x: x.pct_change().shift(-1) < 0))\n",
    ").astype(int)\n",
    "\n",
    "# Investment collapse\n",
    "flag_invest = (\n",
    "    df_pivot.groupby(\"Country\")[\"NID_NGDP\"].transform(lambda x: x.diff() < -2)\n",
    ").astype(int)\n",
    "\n",
    "# Savings decline\n",
    "flag_savings = (\n",
    "    df_pivot.groupby(\"Country\")[\"NGSD_NGDP\"].transform(lambda x: x.diff() < -2)\n",
    ").astype(int)\n",
    "\n",
    "# Trade shock\n",
    "flag_trade = (\n",
    "    (df_pivot.groupby(\"Country\")[\"TX_RPCH\"].transform(lambda x: x < 0)) &\n",
    "    (df_pivot.groupby(\"Country\")[\"TM_RPCH\"].transform(lambda x: x < 0))\n",
    ").astype(int)\n",
    "\n",
    "# Inflation shock\n",
    "flag_inflation = (\n",
    "    (df_pivot.groupby(\"Country\")[\"PCPIPCH\"].transform(lambda x: x > 10)) &\n",
    "    (df_pivot.groupby(\"Country\")[\"NGDPRPC\"].transform(lambda x: x.pct_change() < 0))\n",
    ").astype(int)\n",
    "\n",
    "# --- Step 3: Combine signals into a single severity score ---\n",
    "local_signal_count = (\n",
    "    flag_gdp + flag_invest + flag_savings + flag_trade + flag_inflation\n",
    ")\n",
    "\n",
    "# Global recession override\n",
    "global_recession = df_pivot.index.isin(global_recession_years).astype(int)\n",
    "\n",
    "# --- Step 4: Multiclass recession risk label ---\n",
    "def classify_risk(global_flag, local_count):\n",
    "    if global_flag == 1:\n",
    "        return 3  # High risk\n",
    "    if local_count >= 3:\n",
    "        return 3  # High risk\n",
    "    if local_count == 2:\n",
    "        return 2  # Moderate risk\n",
    "    if local_count == 1:\n",
    "        return 1  # Mild risk\n",
    "    return 0      # No risk\n",
    "\n",
    "df_pivot[\"RecessionRisk\"] = [\n",
    "    classify_risk(g, l) for g, l in zip(global_recession, local_signal_count)\n",
    "]\n",
    "\n",
    "# --- Step 5: Clean dataset ---\n",
    "df_pivot = df_pivot.dropna().sort_index(ascending=True)\n",
    "\n",
    "# --- Output: only the multiclass label ---\n",
    "df_pivot\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d627b020",
   "metadata": {},
   "source": [
    "## Review Remaining Countries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "422b2731",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pivot[\"Country\"].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a1820ea",
   "metadata": {},
   "source": [
    "## Split Training and Prediction Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c339905f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filtered = df_pivot.loc[df_pivot.index <= 2025]\n",
    "df_filtered"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec37a376",
   "metadata": {},
   "source": [
    "# 4. Exploratory Data Analysis\n",
    "\n",
    "## Correlation Heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "364d1ffb",
   "metadata": {},
   "outputs": [],
   "source": [
    "corr = df_filtered.drop(columns=[\"Country\"]).corr()\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(corr, annot=True, fmt=\".2f\", cmap=\"coolwarm\", square=True)\n",
    "plt.title(\"Correlation Heatmap of Features\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59c5b2f5",
   "metadata": {},
   "source": [
    "## Prepare Features and Target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3382119f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_filtered.drop(columns=[\"RecessionRisk\", \"Country\"])\n",
    "y = df_filtered[\"RecessionRisk\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5baccf90",
   "metadata": {},
   "source": [
    "# 5. Machine Learning Models\n",
    "\n",
    "## Global Dataset - Full Features (13 Features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb849a40",
   "metadata": {},
   "source": [
    "### Define and Train All Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e8774c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "#             TRAIN ALL MODELS (MULTICLASS)\n",
    "# ============================================================\n",
    "def train_all_models(X_train, y_train, X_test, y_test, model_params=None, use_xgb=False):\n",
    "\n",
    "    if model_params is None:\n",
    "        model_params = {\n",
    "            'logit': {\n",
    "                'C': 0.2,\n",
    "                'penalty': 'l2',\n",
    "                'solver': 'lbfgs',\n",
    "                'max_iter': 5000,\n",
    "                'random_state': 42,\n",
    "                'multi_class': 'multinomial'\n",
    "            },\n",
    "            'rf': {\n",
    "                'n_estimators': 200,\n",
    "                'max_depth': 4,\n",
    "                'min_samples_leaf': 20,\n",
    "                'min_samples_split': 20,\n",
    "                'max_features': 0.3,\n",
    "                'random_state': 42\n",
    "            },\n",
    "            'gb': {\n",
    "                'n_estimators': 200,\n",
    "                'learning_rate': 0.03,\n",
    "                'max_depth': 2,\n",
    "                'min_samples_leaf': 20,\n",
    "                'subsample': 0.6,\n",
    "                'random_state': 42\n",
    "            },\n",
    "            'dt': {\n",
    "                'max_depth': 3,\n",
    "                'min_samples_leaf': 30,\n",
    "                'random_state': 42\n",
    "            },\n",
    "            'svm': {\n",
    "                'C': 1.0,\n",
    "                'kernel': 'rbf',\n",
    "                'probability': True,\n",
    "                'random_state': 42,\n",
    "                'decision_function_shape': 'ovr'\n",
    "            },\n",
    "            'xgb': {\n",
    "                'n_estimators': 200,\n",
    "                'learning_rate': 0.05,\n",
    "                'max_depth': 2,\n",
    "                'subsample': 0.7,\n",
    "                'colsample_bytree': 0.6,\n",
    "                'reg_alpha': 0.4,\n",
    "                'reg_lambda': 2.0,\n",
    "                'random_state': 42,\n",
    "                'objective': 'multi:softprob',\n",
    "                'num_class': len(np.unique(y_train))\n",
    "            }\n",
    "        }\n",
    "\n",
    "    # ----------------- Train models -----------------\n",
    "    logit = ImbPipeline([\n",
    "        (\"scaler\", StandardScaler()),\n",
    "        (\"smote\", SMOTE(random_state=42)),\n",
    "        (\"logit\", LogisticRegression(**model_params['logit']))\n",
    "    ]).fit(X_train, y_train)\n",
    "\n",
    "    rf = ImbPipeline([\n",
    "        (\"smote\", SMOTE(random_state=42)),\n",
    "        (\"rf\", RandomForestClassifier(**model_params['rf']))\n",
    "    ]).fit(X_train, y_train)\n",
    "\n",
    "    gb = ImbPipeline([\n",
    "        (\"smote\", SMOTE(random_state=42)),\n",
    "        (\"gb\", GradientBoostingClassifier(**model_params['gb']))\n",
    "    ]).fit(X_train, y_train)\n",
    "\n",
    "    dt = ImbPipeline([\n",
    "        (\"smote\", SMOTE(random_state=42)),\n",
    "        (\"dt\", DecisionTreeClassifier(**model_params['dt']))\n",
    "    ]).fit(X_train, y_train)\n",
    "\n",
    "    svm = ImbPipeline([\n",
    "        (\"scaler\", StandardScaler()),\n",
    "        (\"smote\", SMOTE(random_state=42)),\n",
    "        (\"svm\", SVC(**model_params['svm']))\n",
    "    ]).fit(X_train, y_train)\n",
    "\n",
    "    models = {\n",
    "        \"Logistic Regression\": logit,\n",
    "        \"Random Forest\": rf,\n",
    "        \"Gradient Boosting\": gb,\n",
    "        \"Decision Tree\": dt,\n",
    "        \"SVM\": svm,\n",
    "    }\n",
    "\n",
    "    if use_xgb:\n",
    "        xgb = ImbPipeline([\n",
    "            (\"smote\", SMOTE(random_state=42)),\n",
    "            (\"xgb\", XGBClassifier(**model_params['xgb']))\n",
    "        ]).fit(X_train, y_train)\n",
    "        models[\"XGBoost\"] = xgb\n",
    "\n",
    "    # ----------------- Ensemble -----------------\n",
    "    ensemble_estimators = [\n",
    "        (\"logit\", logit.named_steps[\"logit\"]),\n",
    "        (\"rf\", rf.named_steps[\"rf\"]),\n",
    "        (\"gb\", gb.named_steps[\"gb\"]),\n",
    "        (\"svm\", svm.named_steps[\"svm\"])\n",
    "    ]\n",
    "    if use_xgb:\n",
    "        ensemble_estimators.append((\"xgb\", xgb.named_steps[\"xgb\"]))\n",
    "\n",
    "    ensemble = VotingClassifier(estimators=ensemble_estimators, voting=\"soft\")\n",
    "    ensemble.fit(X_train, y_train)\n",
    "    models[\"Ensemble\"] = ensemble\n",
    "\n",
    "    # ----------------- Metrics -----------------\n",
    "    results = {}\n",
    "    confusion_mats = {}\n",
    "\n",
    "    for name, m in models.items():\n",
    "        y_pred_train = m.predict(X_train)\n",
    "        y_pred_test = m.predict(X_test)\n",
    "\n",
    "        results[name] = {\n",
    "            \"Train Accuracy\": accuracy_score(y_train, y_pred_train),\n",
    "            \"Test Accuracy\": accuracy_score(y_test, y_pred_test),\n",
    "            \"Precision (macro)\": precision_score(y_test, y_pred_test, average=\"macro\", zero_division=0),\n",
    "            \"Recall (macro)\": recall_score(y_test, y_pred_test, average=\"macro\", zero_division=0),\n",
    "            \"F1 (macro)\": f1_score(y_test, y_pred_test, average=\"macro\", zero_division=0)\n",
    "        }\n",
    "\n",
    "        confusion_mats[name] = confusion_matrix(y_test, y_pred_test)\n",
    "\n",
    "    results_df = pd.DataFrame(results).T\n",
    "    return models, results_df, confusion_mats\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "#             FEATURE IMPORTANCE (MULTICLASS)\n",
    "# ============================================================\n",
    "def plot_feature_importance(models, feature_names, title_prefix=\"\"):\n",
    "\n",
    "    logit = models.get(\"Logistic Regression\")\n",
    "    rf = models.get(\"Random Forest\")\n",
    "    gb = models.get(\"Gradient Boosting\")\n",
    "    dt = models.get(\"Decision Tree\")\n",
    "\n",
    "    coef_matrix = logit.named_steps['logit'].coef_\n",
    "    coef_mean_abs = np.mean(np.abs(coef_matrix), axis=0)\n",
    "\n",
    "    logit_importance = pd.DataFrame({\n",
    "        \"Feature\": feature_names,\n",
    "        \"Importance\": coef_mean_abs\n",
    "    }).sort_values(\"Importance\")\n",
    "\n",
    "    rf_importance = pd.DataFrame({\n",
    "        \"Feature\": feature_names,\n",
    "        \"Importance\": rf.named_steps['rf'].feature_importances_\n",
    "    }).sort_values(\"Importance\")\n",
    "\n",
    "    gb_importance = pd.DataFrame({\n",
    "        \"Feature\": feature_names,\n",
    "        \"Importance\": gb.named_steps['gb'].feature_importances_\n",
    "    }).sort_values(\"Importance\")\n",
    "\n",
    "    dt_importance = pd.DataFrame({\n",
    "        \"Feature\": feature_names,\n",
    "        \"Importance\": dt.named_steps['dt'].feature_importances_\n",
    "    }).sort_values(\"Importance\")\n",
    "\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(16, 10))\n",
    "\n",
    "    axes[0, 0].barh(logit_importance[\"Feature\"], logit_importance[\"Importance\"])\n",
    "    axes[0, 0].set_title(f\"{title_prefix}Logistic Regression\")\n",
    "\n",
    "    axes[0, 1].barh(rf_importance[\"Feature\"], rf_importance[\"Importance\"])\n",
    "    axes[0, 1].set_title(f\"{title_prefix}Random Forest\")\n",
    "\n",
    "    axes[1, 0].barh(gb_importance[\"Feature\"], gb_importance[\"Importance\"])\n",
    "    axes[1, 0].set_title(f\"{title_prefix}Gradient Boosting\")\n",
    "\n",
    "    axes[1, 1].barh(dt_importance[\"Feature\"], dt_importance[\"Importance\"])\n",
    "    axes[1, 1].set_title(f\"{title_prefix}Decision Tree\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "#             CONFUSION MATRIX DISPLAY (MULTICLASS)\n",
    "# ============================================================\n",
    "def show_confusion_matrices(confusion_mats, results_df):\n",
    "\n",
    "    n_models = len(confusion_mats)\n",
    "    fig, axes = plt.subplots(1, n_models, figsize=(5*n_models, 5))\n",
    "\n",
    "    if n_models == 1:\n",
    "        axes = [axes]\n",
    "\n",
    "    for ax, (name, cm) in zip(axes, confusion_mats.items()):\n",
    "        disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
    "        disp.plot(cmap=\"Blues\", ax=ax, colorbar=False)\n",
    "\n",
    "        acc = results_df.loc[name, \"Test Accuracy\"]\n",
    "        prec = results_df.loc[name, \"Precision (macro)\"]\n",
    "        rec = results_df.loc[name, \"Recall (macro)\"]\n",
    "        f1 = results_df.loc[name, \"F1 (macro)\"]\n",
    "\n",
    "        ax.set_title(\n",
    "            f\"{name}\\nAcc={acc:.2f}, Prec={prec:.2f}, Rec={rec:.2f}, F1={f1:.2f}\",\n",
    "            fontsize=11\n",
    "        )\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# ============================================================\n",
    "#             ROC CURVES + MACRO AUC (MULTICLASS)\n",
    "# ============================================================\n",
    "def show_roc_curves(models, X_test, y_test):\n",
    "    \"\"\"\n",
    "    Display ROC curves and macro AUC for all models.\n",
    "    Works for multiclass classification.\n",
    "    \"\"\"\n",
    "\n",
    "    n_models = len(models)\n",
    "    fig, axes = plt.subplots(1, n_models, figsize=(6*n_models, 5))\n",
    "\n",
    "    if n_models == 1:\n",
    "        axes = [axes]\n",
    "\n",
    "    classes = sorted(np.unique(y_test))\n",
    "\n",
    "    for ax, (name, model) in zip(axes, models.items()):\n",
    "\n",
    "        if not hasattr(model, \"predict_proba\"):\n",
    "            ax.set_title(f\"{name}\\n(No predict_proba)\")\n",
    "            ax.axis(\"off\")\n",
    "            continue\n",
    "\n",
    "        # Predict probabilities\n",
    "        y_proba = model.predict_proba(X_test)\n",
    "\n",
    "        aucs = []\n",
    "        for c in classes:\n",
    "            y_true_bin = (y_test == c).astype(int)\n",
    "            y_score = y_proba[:, c]\n",
    "\n",
    "            fpr, tpr, _ = roc_curve(y_true_bin, y_score)\n",
    "            auc_val = auc(fpr, tpr)\n",
    "            aucs.append(auc_val)\n",
    "\n",
    "            ax.plot(fpr, tpr, label=f\"Class {c} (AUC={auc_val:.2f})\")\n",
    "\n",
    "        macro_auc = np.mean(aucs)\n",
    "\n",
    "        ax.plot([0, 1], [0, 1], \"k--\", alpha=0.5)\n",
    "        ax.set_title(f\"{name}\\nMacro AUC={macro_auc:.2f}\")\n",
    "        ax.set_xlabel(\"False Positive Rate\")\n",
    "        ax.set_ylabel(\"True Positive Rate\")\n",
    "        ax.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d2cf4d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "split_index = int(len(X) * 0.8)\n",
    "X_train, X_test = X.iloc[:split_index], X.iloc[split_index:]\n",
    "y_train, y_test = y.iloc[:split_index], y.iloc[split_index:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "685c21a0",
   "metadata": {},
   "source": [
    "# Training the model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "355300d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "models, summary_df, confusion_mats = train_all_models(X_train, y_train, X_test, y_test)\n",
    "\n",
    "print(summary_df)\n",
    "plot_feature_importance(models, X_train.columns.tolist())\n",
    "# Show confusion matrices with metrics underneath\n",
    "show_confusion_matrices(confusion_mats, summary_df)\n",
    "show_roc_curves(models, X_test, y_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56b66c25",
   "metadata": {},
   "source": [
    "### Reduced Global Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "107301e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_features = ['TM_RPCH', 'NGDP_RPCH', 'TX_RPCH', 'PCPIPCH', 'BCA_NGDPD', 'NGDPRPC']\n",
    "X_train_reduced = X_train[selected_features]\n",
    "X_test_reduced = X_test[selected_features]\n",
    "\n",
    "# Unpack all three return values\n",
    "models_reduced, summary_df_reduced, confusion_mats_reduced = train_all_models(\n",
    "    X_train_reduced, y_train, X_test_reduced, y_test\n",
    ")\n",
    "\n",
    "# Show metrics table\n",
    "print(summary_df_reduced)\n",
    "# Plot feature importance\n",
    "plot_feature_importance(models_reduced, feature_names=selected_features, title_prefix=\"Reduced Features - \")\n",
    "\n",
    "# Show confusion matrices (all in one window)\n",
    "show_confusion_matrices(confusion_mats_reduced, summary_df_reduced)\n",
    "show_roc_curves(models_reduced, X_test_reduced, y_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12134a9a",
   "metadata": {},
   "source": [
    "### Split Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8430911",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map countries to continents (same logic as before)\n",
    "try:\n",
    "    import pycountry\n",
    "    import pycountry_convert as pc\n",
    "    \n",
    "    def country_to_continent(name):\n",
    "        try:\n",
    "            lookup_name = name.replace('_', ' ')\n",
    "            country = pycountry.countries.lookup(lookup_name)\n",
    "            alpha2 = country.alpha_2\n",
    "            cc = pc.country_alpha2_to_continent_code(alpha2)\n",
    "            continent_map = {\n",
    "                'AF': 'Africa',\n",
    "                'AS': 'Asia',\n",
    "                'EU': 'Europe',\n",
    "                'NA': 'North_America',\n",
    "                'OC': 'Oceania',\n",
    "                'SA': 'South_America'\n",
    "            }\n",
    "            return continent_map.get(cc, 'Unknown')\n",
    "        except Exception:\n",
    "            return 'Unknown'\n",
    "except ImportError:\n",
    "    # Fallback mapping for common countries (extend as needed)\n",
    "    fallback = {\n",
    "    'United_States': 'North_America', 'Canada': 'North_America', 'Mexico': 'North_America',\n",
    "    'China': 'Asia', 'India': 'Asia', 'Japan': 'Asia', 'Afghanistan': 'Asia',\n",
    "    'Korea': 'Asia', 'Indonesia': 'Asia', 'Thailand': 'Asia', 'Vietnam': 'Asia',\n",
    "    'Germany': 'Europe', 'France': 'Europe', 'United_Kingdom': 'Europe', 'Italy': 'Europe',\n",
    "    'Spain': 'Europe', 'Russia': 'Europe', 'Turkey': 'Europe', 'Poland': 'Europe',\n",
    "    'Brazil': 'South_America', 'Argentina': 'South_America', 'Chile': 'South_America',\n",
    "    'Colombia': 'South_America', 'Peru': 'South_America', 'Venezuela': 'South_America',\n",
    "    'Australia': 'Oceania', 'New_Zealand': 'Oceania',\n",
    "    'South_Africa': 'Africa', 'Nigeria': 'Africa', 'Egypt': 'Africa', 'Zimbabwe': 'Africa',\n",
    "    'Kenya': 'Africa', 'Ethiopia': 'Africa', 'Morocco': 'Africa',\n",
    "\n",
    "    'Albania': 'Europe', 'Algeria': 'Africa', 'Austria': 'Europe', 'Barbados': 'North_America',\n",
    "    'Belgium': 'Europe', 'Bolivia': 'South_America', 'Bosnia_and_Herzegovina': 'Europe',\n",
    "    'Bulgaria': 'Europe', 'Cabo_Verde': 'Africa', 'Costa_Rica': 'North_America',\n",
    "    'Croatia': 'Europe', 'Cyprus': 'Europe', 'Czech_Republic': 'Europe', 'Denmark': 'Europe',\n",
    "    'Dominican_Republic': 'North_America', 'Estonia': 'Europe', 'Finland': 'Europe',\n",
    "    'Hungary': 'Europe', 'Iceland': 'Europe', 'Ireland': 'Europe',\n",
    "    'Islamic_Republic_of_Iran': 'Asia', 'Israel': 'Asia', 'Jordan': 'Asia',\n",
    "    'Kazakhstan': 'Asia', 'Latvia': 'Europe', 'Lebanon': 'Asia', 'Lithuania': 'Europe',\n",
    "    'Luxembourg': 'Europe', 'Malta': 'Europe', 'Netherlands': 'Europe',\n",
    "    'North_Macedonia': 'Europe', 'Norway': 'Europe', 'Pakistan': 'Asia',\n",
    "    'Panama': 'North_America', 'Paraguay': 'South_America', 'Portugal': 'Europe',\n",
    "    'Romania': 'Europe', 'Saudi_Arabia': 'Asia', 'Serbia': 'Europe', 'Seychelles': 'Africa',\n",
    "    'Slovak_Republic': 'Europe', 'Slovenia': 'Europe', 'Sweden': 'Europe',\n",
    "    'Switzerland': 'Europe', 'Syria': 'Asia', 'Taiwan_Province_of_China': 'Asia',\n",
    "    'Trinidad_and_Tobago': 'North_America', 'Türkiye': 'Europe', 'Uruguay': 'South_America',\n",
    "    'Botswana': 'Africa', 'Cameroon': 'Africa', 'Djibouti': 'Africa', 'Equatorial_Guinea': 'Africa',\n",
    "    'Eswatini': 'Africa','Guyana': 'South_America','Lesotho': 'Africa','Mali': 'Africa',\n",
    "    'Mauritania': 'Africa','Namibia': 'Africa','Niger': 'Africa','Oman': 'Asia','Yemen': 'Asia',\n",
    "    'Zambia': 'Africa','St._Kitts_and_Nevis': 'North_America'\n",
    "}\n",
    "\n",
    "    def country_to_continent(name):\n",
    "        return fallback.get(name.replace(' ', '_'), 'Unknown')\n",
    "\n",
    "# --- Add Continent column ---\n",
    "df_filtered_copy = df_pivot.copy()\n",
    "df_filtered_copy['Continent'] = df_filtered_copy['Country'].astype(str).apply(country_to_continent)\n",
    "\n",
    "# --- Map continents to economy groups ---\n",
    "continent_to_economy = {\n",
    "    'Europe': 'Upper_Economies',\n",
    "    'North_America': 'Upper_Economies',\n",
    "    'Oceania': 'Upper_Economies',\n",
    "    'Africa': 'Lower_Economies',\n",
    "    'Asia': 'Lower_Economies',\n",
    "    'South_America': 'Lower_Economies'\n",
    "}\n",
    "\n",
    "df_filtered_copy['EconomyGroup'] = df_filtered_copy['Continent'].map(continent_to_economy)\n",
    "\n",
    "# --- Create Lower and Upper economy DataFrames ---\n",
    "df_Lower_Economies = df_filtered_copy[df_filtered_copy['EconomyGroup'] == 'Lower_Economies'].drop(columns=['Continent','EconomyGroup'])\n",
    "df_Upper_Economies = df_filtered_copy[df_filtered_copy['EconomyGroup'] == 'Upper_Economies'].drop(columns=['Continent','EconomyGroup'])\n",
    "\n",
    "# --- Print summary ---\n",
    "print(\"Created economy-specific DataFrames:\")\n",
    "print(f\" - Lower_Economies: df_Lower_Economies (rows: {len(df_Lower_Economies)})\")\n",
    "print(f\" - Upper_Economies: df_Upper_Economies (rows: {len(df_Upper_Economies)})\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9989b926",
   "metadata": {},
   "source": [
    "# 6. Economy-Specific Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afbeadb0",
   "metadata": {},
   "source": [
    "## Upper Economies - Full Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19751b1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data\n",
    "X = df_Upper_Economies.drop(columns=[\"RecessionRisk\", \"Country\"])\n",
    "y = df_Upper_Economies[\"RecessionRisk\"]\n",
    "\n",
    "split_index = int(len(X) * 0.8)\n",
    "X_train, X_test = X.iloc[:split_index], X.iloc[split_index:]\n",
    "y_train, y_test = y.iloc[:split_index], y.iloc[split_index:]\n",
    "\n",
    "# Train models and unpack all three return values\n",
    "models_upper, summary_df_upper, confusion_mats_upper = train_all_models(\n",
    "    X_train, y_train, X_test, y_test\n",
    ")\n",
    "\n",
    "# Show metrics table\n",
    "print(summary_df_upper)\n",
    "\n",
    "# Plot feature importance\n",
    "plot_feature_importance(models_upper, X_train.columns.tolist(), title_prefix=\"Upper Economies - \")\n",
    "\n",
    "# Show confusion matrices (all in one window)\n",
    "show_confusion_matrices(confusion_mats_upper, summary_df_upper)\n",
    "# ✅ Show ROC curves + AUC for upper-economy models\n",
    "show_roc_curves(models_upper, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd4b3137",
   "metadata": {},
   "source": [
    "## Lower Economies - Full Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24ecfe97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for Lower Economies\n",
    "X = df_Lower_Economies.drop(columns=[\"RecessionRisk\", \"Country\"])\n",
    "y = df_Lower_Economies[\"RecessionRisk\"]\n",
    "\n",
    "split_index = int(len(X) * 0.8)\n",
    "X_train, X_test = X.iloc[:split_index], X.iloc[split_index:]\n",
    "y_train, y_test = y.iloc[:split_index], y.iloc[split_index:]\n",
    "\n",
    "# Train models and unpack all three return values\n",
    "models_lower, summary_df_lower, confusion_mats_lower = train_all_models(\n",
    "    X_train, y_train, X_test, y_test\n",
    ")\n",
    "\n",
    "# Show metrics table\n",
    "print(summary_df_lower)\n",
    "\n",
    "# Plot feature importance\n",
    "plot_feature_importance(models_lower, X_train.columns.tolist(), title_prefix=\"Lower Economies - \")\n",
    "\n",
    "\n",
    "# Show confusion matrices (all in one window)\n",
    "show_confusion_matrices(confusion_mats_lower, summary_df_lower)\n",
    "# ✅ Show ROC curves + AUC for lower-economy models\n",
    "show_roc_curves(models_lower, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cfc2538",
   "metadata": {},
   "source": [
    "## Upper Economies - Reduced Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a25f6b34",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_upper = df_Upper_Economies[selected_features]\n",
    "y_upper = df_Upper_Economies[\"RecessionRisk\"]\n",
    "\n",
    "split_index_upper = int(len(X_upper) * 0.8)\n",
    "X_train_upper = X_upper.iloc[:split_index_upper]\n",
    "X_test_upper = X_upper.iloc[split_index_upper:]\n",
    "y_train_upper = y_upper.iloc[:split_index_upper]\n",
    "y_test_upper = y_upper.iloc[split_index_upper:]\n",
    "\n",
    "# ✅ Unpack all three return values\n",
    "models_upper_reduced, summary_df_upper_reduced, confusion_mats_upper_reduced = train_all_models(\n",
    "    X_train_upper, y_train_upper, X_test_upper, y_test_upper\n",
    ")\n",
    "\n",
    "print(\"Upper Economies Accuracy (Reduced Features):\")\n",
    "print(summary_df_upper_reduced)\n",
    "\n",
    "# Feature importance\n",
    "plot_feature_importance(\n",
    "    models_upper_reduced,\n",
    "    feature_names=selected_features,\n",
    "    title_prefix=\"Upper Economies - Reduced Features - \"\n",
    ")\n",
    "\n",
    "# ✅ Show confusion matrices (all in one window)\n",
    "show_confusion_matrices(confusion_mats_upper_reduced, summary_df_upper_reduced)\n",
    "\n",
    "# ✅ Show ROC curves + AUC for reduced-feature upper-economy models\n",
    "show_roc_curves(models_upper_reduced, X_test_upper, y_test_upper)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57f45978",
   "metadata": {},
   "source": [
    "## Lower Economies - Reduced Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff38534b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_lower = df_Lower_Economies[selected_features]\n",
    "y_lower = df_Lower_Economies[\"RecessionRisk\"]\n",
    "\n",
    "split_index_lower = int(len(X_lower) * 0.8)\n",
    "X_train_lower = X_lower.iloc[:split_index_lower]\n",
    "X_test_lower = X_lower.iloc[split_index_lower:]\n",
    "y_train_lower = y_lower.iloc[:split_index_lower]\n",
    "y_test_lower = y_lower.iloc[split_index_lower:]\n",
    "\n",
    "# ✅ Unpack all three return values\n",
    "models_lower_reduced, summary_df_lower_reduced, confusion_mats_lower_reduced = train_all_models(\n",
    "    X_train_lower, y_train_lower, X_test_lower, y_test_lower\n",
    ")\n",
    "\n",
    "print(\"Lower Economies Accuracy (Reduced Features):\")\n",
    "print(summary_df_lower_reduced)\n",
    "\n",
    "# Feature importance\n",
    "plot_feature_importance(\n",
    "    models_lower_reduced,\n",
    "    feature_names=selected_features,\n",
    "    title_prefix=\"Lower Economies - Reduced Features - \"\n",
    ")\n",
    "\n",
    "# ✅ Show confusion matrices (all in one window)\n",
    "show_confusion_matrices(confusion_mats_lower_reduced, summary_df_lower_reduced)\n",
    "\n",
    "# ✅ Show ROC curves + AUC for reduced-feature lower-economy models\n",
    "show_roc_curves(models_lower_reduced, X_test_lower, y_test_lower)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf5c244f",
   "metadata": {},
   "source": [
    "# Prediction 2026-2030"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a269a6cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_predict = df_pivot.loc[df_pivot.index > 2025]\n",
    "df_predict_original = df_predict.copy()\n",
    "df_predict = df_predict.drop(columns=[\"RecessionRisk\", \"Country\"])\n",
    "\n",
    "df_predict_original['Continent'] = df_predict_original['Country'].astype(str).apply(country_to_continent)\n",
    "\n",
    "continent_to_economy = {\n",
    "    'Europe': 'Upper_Economies',\n",
    "    'North_America': 'Upper_Economies',\n",
    "    'Oceania': 'Upper_Economies',\n",
    "    'Africa': 'Lower_Economies',\n",
    "    'Asia': 'Lower_Economies',\n",
    "    'South_America': 'Lower_Economies'\n",
    "}\n",
    "\n",
    "df_predict_original['EconomyGroup'] = df_predict_original['Continent'].map(continent_to_economy)\n",
    "\n",
    "df_predict_lower = df_predict_original[df_predict_original['EconomyGroup'] == 'Lower_Economies']\n",
    "\n",
    "df_predict_upper = df_predict_original[df_predict_original['EconomyGroup'] == 'Upper_Economies']\n",
    "\n",
    "print(\"Created economy-specific prediction DataFrames from df_predict_original:\")\n",
    "print(f\" - Lower_Economies predictions: {len(df_predict_lower)} rows\")\n",
    "print(f\" - Upper_Economies predictions: {len(df_predict_upper)} rows\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fb556a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "#                  PREDICTION FUNCTION (MULTICLASS)\n",
    "# ============================================================\n",
    "def make_predictions(models, df_predict):\n",
    "    \"\"\"\n",
    "    Return multiclass predictions (0,1,2,3) from every model in one dataframe.\n",
    "    \"\"\"\n",
    "    predictions = {}\n",
    "\n",
    "    for name, model in models.items():\n",
    "        # If model supports predict_proba → use argmax over classes\n",
    "        if hasattr(model, \"predict_proba\"):\n",
    "            proba = model.predict_proba(df_predict)\n",
    "            predictions[name] = proba.argmax(axis=1)\n",
    "        else:\n",
    "            predictions[name] = model.predict(df_predict)\n",
    "\n",
    "    return pd.DataFrame(predictions, index=df_predict.index)\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "#      MULTICLASS RECESSION RISK COUNTS PER MODEL (PLOT)\n",
    "# ============================================================\n",
    "def plot_recession_counts_per_model(df_with_country, title):\n",
    "    \"\"\"\n",
    "    Grouped bar plot per year per model:\n",
    "    - X: Year\n",
    "    - Bars: counts of each RecessionRisk class (0,1,2,3)\n",
    "    - Facets: one subplot per model\n",
    "\n",
    "    Expects df_with_country with columns:\n",
    "        ['Year','Country', <model prediction columns>]\n",
    "    where model columns contain multiclass predictions (0–3).\n",
    "    \"\"\"\n",
    "\n",
    "    # Melt to long format\n",
    "    df_long = df_with_country.melt(\n",
    "        id_vars=['Year', 'Country'],\n",
    "        var_name='Model',\n",
    "        value_name='Prediction'\n",
    "    )\n",
    "\n",
    "    # Count per (Year, Model, Prediction)\n",
    "    counts = (\n",
    "        df_long.groupby(['Year', 'Model', 'Prediction'])\n",
    "               .size()\n",
    "               .reset_index(name='Count')\n",
    "    )\n",
    "\n",
    "    # Pivot → columns become risk classes (0,1,2,3)\n",
    "    counts_pivot = (\n",
    "        counts.pivot(index=['Year', 'Model'], columns='Prediction', values='Count')\n",
    "              .fillna(0)\n",
    "    ).reset_index()\n",
    "\n",
    "    # Identify risk classes present\n",
    "    risk_classes = sorted([c for c in counts_pivot.columns if isinstance(c, int)])\n",
    "\n",
    "    # Reorder columns: Year, Model, then risk classes\n",
    "    counts_pivot = counts_pivot[['Year', 'Model'] + risk_classes]\n",
    "\n",
    "    # Plotting layout\n",
    "    models = counts_pivot['Model'].unique()\n",
    "    n_models = len(models)\n",
    "    n_cols = 3\n",
    "    n_rows = (n_models + n_cols - 1) // n_cols\n",
    "\n",
    "    fig, axes = plt.subplots(n_rows, n_cols, figsize=(5 * n_cols, 4 * n_rows), sharex=False, sharey=True)\n",
    "    axes = axes.flatten()\n",
    "\n",
    "    for i, model in enumerate(models):\n",
    "        ax = axes[i]\n",
    "\n",
    "        # Extract model-specific data\n",
    "        mdf = counts_pivot[counts_pivot['Model'] == model].set_index('Year')[risk_classes]\n",
    "\n",
    "        # Plot grouped bars\n",
    "        plot = mdf.plot(kind='bar', ax=ax)\n",
    "\n",
    "        ax.set_title(model)\n",
    "        ax.set_ylabel('Number of countries')\n",
    "        ax.set_xlabel('Year')\n",
    "        ax.legend(title='RecessionRisk')\n",
    "\n",
    "        # Annotate bars\n",
    "        for p in plot.patches:\n",
    "            height = p.get_height()\n",
    "            if height > 0:\n",
    "                ax.annotate(\n",
    "                    f'{int(height)}',\n",
    "                    (p.get_x() + p.get_width() / 2., height),\n",
    "                    ha='center', va='bottom',\n",
    "                    fontsize=8, color='black', xytext=(0, 2),\n",
    "                    textcoords='offset points'\n",
    "                )\n",
    "\n",
    "    # Hide unused axes\n",
    "    for j in range(i + 1, len(axes)):\n",
    "        axes[j].set_visible(False)\n",
    "\n",
    "    fig.suptitle(title, y=1.02, fontsize=12)\n",
    "    fig.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17f81a29",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_predict_original.drop(columns=[\"RecessionRisk\"], inplace=True)\n",
    "df_predict_original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bfef5b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_predict_upper.drop(columns=[\"RecessionRisk\"], inplace=True)\n",
    "df_predict_upper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b88263da",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_predict_lower.drop(columns=[\"RecessionRisk\"], inplace=True)\n",
    "df_predict_lower"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c7d54fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_predict_upper_copy = df_predict_upper.copy()\n",
    "df_predict_lower_copy = df_predict_lower.copy()\n",
    "df_predict_upper_copy.drop(columns=['Continent', 'EconomyGroup', 'Country'], inplace=True)\n",
    "df_predict_lower_copy.drop(columns=['Continent', 'EconomyGroup', 'Country'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54efeef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset index once for reuse (assumes Year is the index in df_predict_original)\n",
    "df_predict_original_reset = df_predict_original.reset_index()\n",
    "df_predict_upper = df_predict_upper.reset_index()\n",
    "df_predict_lower = df_predict_lower.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40231818",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Global (all countries, full feature set)\n",
    "# ============================================================\n",
    "predictions = make_predictions(models, df_predict)\n",
    "predictions_global_features_with_country = pd.concat(\n",
    "    [df_predict_original_reset[['Year', 'Country']], predictions.reset_index(drop=True)],\n",
    "    axis=1\n",
    ")\n",
    "print(predictions_global_features_with_country.head())\n",
    "plot_recession_counts_per_model(predictions_global_features_with_country, \"Global (Full features)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "792b3997",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Global Restricted (all countries, restricted feature set)\n",
    "# ============================================================\n",
    "df_predict_restricted = df_predict[selected_features]\n",
    "predictions_restricted_features = make_predictions(models_reduced, df_predict_restricted)\n",
    "predictions_global_restricted_features_with_country = pd.concat(\n",
    "    [df_predict_original_reset[['Year', 'Country']], predictions_restricted_features.reset_index(drop=True)],\n",
    "    axis=1\n",
    ")\n",
    "print(predictions_global_restricted_features_with_country.head())\n",
    "plot_recession_counts_per_model(predictions_global_restricted_features_with_country, \"Global (Restricted features)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4193f779",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ============================================================\n",
    "# Upper Economies (full feature set)\n",
    "# ============================================================\n",
    "df_predict_upper_reset = df_predict_upper_copy.reset_index(drop=True)\n",
    "\n",
    "X_predict_upper = df_predict_upper_reset.drop(columns=[\"RecessionRisk\", \"Country\"], errors=\"ignore\")\n",
    "predictions_upper_features = make_predictions(models_upper, X_predict_upper)\n",
    "\n",
    "predictions_upper_features_with_country = pd.concat(\n",
    "    [df_predict_upper[[\"Year\", \"Country\"]], predictions_upper_features.reset_index(drop=True)],\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "print(predictions_upper_features_with_country.head())\n",
    "plot_recession_counts_per_model(predictions_upper_features_with_country, \"Upper economies (Full features)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2742e92a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Lower Economies (full feature set)\n",
    "# ============================================================\n",
    "df_predict_lower_reset = df_predict_lower_copy.reset_index(drop=True)\n",
    "\n",
    "X_predict_lower = df_predict_lower_reset.drop(columns=[\"RecessionRisk\", \"Country\"], errors=\"ignore\")\n",
    "predictions_lower_features = make_predictions(models_lower, X_predict_lower)\n",
    "\n",
    "predictions_lower_features_with_country = pd.concat(\n",
    "    [df_predict_lower[[\"Year\", \"Country\"]], predictions_lower_features.reset_index(drop=True)],\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "print(predictions_lower_features_with_country.head())\n",
    "plot_recession_counts_per_model(predictions_lower_features_with_country, \"Lower economies (Full features)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aa3a9cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Upper Economies (restricted feature set)\n",
    "# ============================================================\n",
    "X_predict_upper_reduced = df_predict_upper[selected_features]\n",
    "df_predict_upper_reset = df_predict_upper_copy.reset_index(drop=True)\n",
    "\n",
    "X_predict_upper_reduced = df_predict_upper_reset[selected_features]\n",
    "predictions_upper_restricted_features = make_predictions(models_upper_reduced, X_predict_upper_reduced)\n",
    "\n",
    "predictions_upper_restricted_features_with_country = pd.concat(\n",
    "    [df_predict_upper[[\"Year\", \"Country\"]], predictions_upper_restricted_features.reset_index(drop=True)],\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "print(predictions_upper_restricted_features_with_country.head())\n",
    "plot_recession_counts_per_model(predictions_upper_restricted_features_with_country, \"Upper economies (Restricted features)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08238524",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Lower Economies (restricted feature set)\n",
    "# ============================================================\n",
    "X_predict_lower_reduced = df_predict_lower[selected_features]\n",
    "df_predict_lower_reset = df_predict_lower_copy.reset_index(drop=True)\n",
    "X_predict_lower_reduced = df_predict_lower_reset[selected_features]\n",
    "predictions_lower_restricted_features = make_predictions(models_lower_reduced, X_predict_lower_reduced)\n",
    "\n",
    "predictions_lower_restricted_features_with_country = pd.concat(\n",
    "    [df_predict_lower[[\"Year\", \"Country\"]], predictions_lower_restricted_features.reset_index(drop=True)],\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "print(predictions_lower_restricted_features_with_country.head())\n",
    "plot_recession_counts_per_model(predictions_lower_restricted_features_with_country, \"Lower economies (Restricted features)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9ca2ee4",
   "metadata": {},
   "source": [
    "# LTSM Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92ce9106",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "# Core layers\n",
    "from tensorflow.keras.layers import (\n",
    "    Input,\n",
    "    Dense,\n",
    "    LSTM,\n",
    "    GRU,\n",
    "    Dropout,\n",
    "    BatchNormalization,\n",
    "    LayerNormalization,\n",
    "    Bidirectional\n",
    ")\n",
    "\n",
    "# Advanced sequence layers\n",
    "from tensorflow.keras.layers import (\n",
    "    Conv1D,\n",
    "    GlobalAveragePooling1D,\n",
    "    GlobalMaxPooling1D,\n",
    "    MultiHeadAttention\n",
    ")\n",
    "\n",
    "# Model utilities\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "\n",
    "# Callbacks\n",
    "from tensorflow.keras.callbacks import (\n",
    "    EarlyStopping,\n",
    "    ReduceLROnPlateau,\n",
    "    ModelCheckpoint\n",
    ")\n",
    "\n",
    "# Optimizers\n",
    "from tensorflow.keras.optimizers import Adam, RMSprop\n",
    "\n",
    "# Losses & metrics\n",
    "from tensorflow.keras.losses import SparseCategoricalCrossentropy\n",
    "from tensorflow.keras.metrics import Accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32da196f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_until_2025 = df_pivot[df_pivot.index <= 2025]\n",
    "df_after_2025 = df_pivot[df_pivot.index > 2025]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "478dda10",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_until_2025"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d2a6b7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_after_2025"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "281dd3b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reshape_for_lstm(df, window=5, min_sequences=1, return_feature_names=False):\n",
    "    \"\"\"\n",
    "    Convert df_pivot (Year index, Country column) into LSTM-ready sequences.\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame with Year index and Country column\n",
    "        window: Number of historical years to use as input\n",
    "        min_sequences: Minimum sequences required per country (default 1)\n",
    "        return_feature_names: If True, return feature column names\n",
    "        \n",
    "    Returns:\n",
    "        X_seq: (num_sequences, window, num_features)\n",
    "        y_seq: (num_sequences,)\n",
    "        countries_seq: country for each sequence\n",
    "        years_seq: ending year for each sequence\n",
    "        feature_names: (optional) list of feature column names\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    \n",
    "    # Handle index - reset if needed for sorting\n",
    "    if isinstance(df.index, pd.RangeIndex):\n",
    "        raise ValueError(\"DataFrame must have Year as index\")\n",
    "    \n",
    "    # Reset index to make sorting easier, then sort\n",
    "    df_sorted = df.reset_index().sort_values([\"Country\", df.index.name])\n",
    "    year_col = df.index.name or \"Year\"\n",
    "    \n",
    "    # Define features (exclude target and identifiers)\n",
    "    feature_cols = [c for c in df_sorted.columns \n",
    "                   if c not in [\"RecessionRisk\", \"Country\", year_col]]\n",
    "    \n",
    "    X_seq = []\n",
    "    y_seq = []\n",
    "    countries_seq = []\n",
    "    years_seq = []\n",
    "    \n",
    "    # Group by country\n",
    "    for country, group in df_sorted.groupby(\"Country\"):\n",
    "        group = group.sort_values(year_col).reset_index(drop=True)\n",
    "        \n",
    "        # Check if country has enough data\n",
    "        if len(group) < window + 1:\n",
    "            print(f\"Warning: {country} has only {len(group)} years, need {window + 1}. Skipping.\")\n",
    "            continue\n",
    "        \n",
    "        X_values = group[feature_cols].values\n",
    "        y_values = group[\"RecessionRisk\"].values\n",
    "        years = group[year_col].values\n",
    "        \n",
    "        # Sliding window\n",
    "        num_sequences = len(group) - window\n",
    "        for i in range(num_sequences):\n",
    "            X_seq.append(X_values[i:i+window])\n",
    "            y_seq.append(y_values[i+window])\n",
    "            countries_seq.append(country)\n",
    "            years_seq.append(years[i+window])\n",
    "    \n",
    "    if len(X_seq) < min_sequences:\n",
    "        raise ValueError(f\"Only generated {len(X_seq)} sequences, need at least {min_sequences}\")\n",
    "    \n",
    "    results = (\n",
    "        np.array(X_seq),\n",
    "        np.array(y_seq),\n",
    "        np.array(countries_seq),\n",
    "        np.array(years_seq)\n",
    "    )\n",
    "    \n",
    "    if return_feature_names:\n",
    "        results = results + (feature_cols,)\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19fa586c",
   "metadata": {},
   "outputs": [],
   "source": [
    "WINDOW = 5\n",
    "\n",
    "X_seq, y_seq, countries_seq, years_seq = reshape_for_lstm(\n",
    "    df_until_2025,\n",
    "    window=WINDOW\n",
    ")\n",
    "\n",
    "print(\"X_seq shape:\", X_seq.shape)\n",
    "print(\"y_seq shape:\", y_seq.shape)\n",
    "print(\"Example sequence shape:\", X_seq[0].shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f1805a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option 1: Simple time-based split (most common for time series)\n",
    "# Train on earlier years, test on most recent years\n",
    "SPLIT_YEAR = 2021  # Adjust based on your data\n",
    "\n",
    "train_mask = years_seq <= SPLIT_YEAR\n",
    "test_mask = years_seq > SPLIT_YEAR\n",
    "\n",
    "# This gives you:\n",
    "# Train: all sequences predicting up to 2021\n",
    "# Test: sequences predicting 2022,2023, 2024, 2025\n",
    "\n",
    "X_train, X_test = X_seq[train_mask], X_seq[test_mask]\n",
    "y_train, y_test = y_seq[train_mask], y_seq[test_mask]\n",
    "countries_train = countries_seq[train_mask]\n",
    "countries_test  = countries_seq[test_mask]\n",
    "years_train = years_seq[train_mask]\n",
    "years_test  = years_seq[test_mask]\n",
    "\n",
    "print(f\"Train sequences: {len(X_train)} (predicting years {years_train.min()}-{years_train.max()})\")\n",
    "print(f\"Test sequences: {len(X_test)} (predicting years {years_test.min()}-{years_test.max()})\")\n",
    "print(f\"Train: {np.sum(train_mask)} sequences, Test: {np.sum(test_mask)} sequences\")\n",
    "print(f\"Split: {np.sum(train_mask)/(np.sum(train_mask)+np.sum(test_mask))*100:.1f}% train, {np.sum(test_mask)/(np.sum(train_mask)+np.sum(test_mask))*100:.1f}% test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b16a17e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Train sequences:\", X_train.shape)\n",
    "print(\"Test sequences:\", X_test.shape)\n",
    "print(\"Train years range:\", years_train.min(), \"→\", years_train.max())\n",
    "print(\"Test years range:\", years_test.min(), \"→\", years_test.max())\n",
    "\n",
    "# Show example sequences\n",
    "for i in range(3):\n",
    "    print(f\"\\n--- Sequence {i} ---\")\n",
    "    print(\"Country:\", countries_seq[i])\n",
    "    print(f\"Window years: {years_seq[i] - WINDOW} → {years_seq[i] - 1}\")\n",
    "    print(\"Target year:\", years_seq[i])\n",
    "    print(\"Target y_seq[i]:\", y_seq[i])\n",
    "    \n",
    "    # Extract the country and years for the window\n",
    "    country = countries_seq[i]\n",
    "    start_year = years_seq[i] - WINDOW\n",
    "    end_year = years_seq[i] - 1\n",
    "    \n",
    "    # Pull the corresponding y-values (RecessionRisk) from df_until_2025\n",
    "    # Need to filter by country first, then select the year range\n",
    "    country_data = df_until_2025[df_until_2025['Country'] == country]\n",
    "    \n",
    "    # Get the years in the window\n",
    "    window_years = range(start_year, end_year + 1)\n",
    "    y_window = country_data.loc[country_data.index.isin(window_years), 'RecessionRisk'].values\n",
    "    \n",
    "    print(\"\\nX_seq[i] shape:\", X_seq[i].shape)\n",
    "    print(\"First 2 timesteps of X:\")\n",
    "    print(X_seq[i][:2])\n",
    "    \n",
    "    print(f\"\\nCorresponding y window (input labels for years {start_year}-{end_year}):\")\n",
    "    print(y_window)\n",
    "    print(\"Target y (next year):\", y_seq[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2333cf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check your classes\n",
    "print(\"Unique RecessionRisk classes:\", np.unique(y_seq))\n",
    "print(\"Class distribution in full dataset:\")\n",
    "print(pd.Series(y_seq).value_counts().sort_index())\n",
    "print(\"\\nClass distribution in train:\")\n",
    "print(pd.Series(y_train).value_counts().sort_index())\n",
    "print(\"\\nClass distribution in test:\")\n",
    "print(pd.Series(y_test).value_counts().sort_index())\n",
    "\n",
    "# Check if classes are 0-indexed (0, 1, 2, 3) or 1-indexed (1, 2, 3, 4)\n",
    "num_classes = len(np.unique(y_seq))\n",
    "print(f\"\\nNumber of classes: {num_classes}\")\n",
    "\n",
    "# Build the model\n",
    "model = keras.Sequential([\n",
    "    keras.layers.Input(shape=(WINDOW, X_seq.shape[2])),\n",
    "    \n",
    "    # LSTM layer with more units for better learning\n",
    "    keras.layers.LSTM(64, return_sequences=False),\n",
    "    keras.layers.Dropout(0.3),\n",
    "    \n",
    "    # Dense layers\n",
    "    keras.layers.Dense(32, activation=\"relu\"),\n",
    "    keras.layers.Dropout(0.2),\n",
    "    \n",
    "    # Output layer - adjust num_classes if needed\n",
    "    keras.layers.Dense(num_classes, activation=\"softmax\")\n",
    "])\n",
    "\n",
    "# Compile with class weights if imbalanced\n",
    "class_weights = compute_class_weight(\n",
    "    class_weight='balanced',\n",
    "    classes=np.unique(y_train),\n",
    "    y=y_train\n",
    ")\n",
    "class_weight_dict = dict(enumerate(class_weights))\n",
    "print(\"\\nClass weights:\", class_weight_dict)\n",
    "\n",
    "model.compile(\n",
    "    optimizer=keras.optimizers.Adam(learning_rate=0.001),\n",
    "    loss=\"sparse_categorical_crossentropy\",\n",
    "    metrics=[\"accuracy\"]\n",
    ")\n",
    "\n",
    "model.summary()\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_data=(X_test, y_test),\n",
    "    epochs=50,\n",
    "    batch_size=32,\n",
    "    shuffle=False,  # Critical: preserve temporal order in time series\n",
    "    class_weight=class_weight_dict,  # Handle class imbalance\n",
    "    callbacks=[\n",
    "        keras.callbacks.EarlyStopping(\n",
    "            monitor='val_loss',\n",
    "            patience=10,\n",
    "            restore_best_weights=True\n",
    "        )\n",
    "    ],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Evaluate\n",
    "test_loss, test_acc = model.evaluate(X_test, y_test)\n",
    "print(f\"\\nTest Accuracy: {test_acc:.4f}\")\n",
    "\n",
    "# Predictions\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "\n",
    "# Confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred_classes)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()\n",
    "\n",
    "# Classification report\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred_classes))\n",
    "\n",
    "# Plot training history\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['loss'], label='Train Loss')\n",
    "plt.plot(history.history['val_loss'], label='Val Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.title('Training and Validation Loss')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['accuracy'], label='Train Accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='Val Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.title('Training and Validation Accuracy')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
