{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f072c436",
   "metadata": {},
   "source": [
    "# WEO Economic Data Analysis & Recession Prediction\n",
    "\n",
    "**Objective:** Load World Economic Outlook (WEO) data, clean and transform it, then use machine learning models to predict global recessions.\n",
    "\n",
    "**Workflow:**\n",
    "1. Data loading and cleaning\n",
    "2. Feature engineering and recession flagging\n",
    "3. Exploratory data analysis\n",
    "4. Model training with full and reduced feature sets (comparing 13 vs 5 features)\n",
    "5. Economy-specific analysis (Upper vs Lower economies with both feature sets)\n",
    "6. Future predictions for all scenarios\n",
    "\n",
    "**Models Used:** Logistic Regression, Random Forest, Gradient Boosting, Linear SVM, Decision Tree, and Ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0881101a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core data manipulation and visualization\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# File handling\n",
    "import csv\n",
    "from pathlib import Path\n",
    "\n",
    "# Machine learning - preprocessing\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Machine learning - models\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, VotingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Machine learning - metrics\n",
    "from sklearn.metrics import (\n",
    "    classification_report, \n",
    "    accuracy_score, \n",
    "    precision_score, \n",
    "    recall_score, \n",
    "    f1_score, \n",
    "    confusion_matrix,\n",
    "    ConfusionMatrixDisplay,\n",
    "    roc_curve,\n",
    "    auc\n",
    ")\n",
    "\n",
    "# Imbalanced learning\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.pipeline import Pipeline as ImbPipeline\n",
    "\n",
    "# Optional pycountry for continent mapping\n",
    "try:\n",
    "    import pycountry\n",
    "    import pycountry_convert as pc\n",
    "    HAS_PYCOUNTRY = True\n",
    "except ImportError:\n",
    "    HAS_PYCOUNTRY = False\n",
    "\n",
    "pd.set_option(\"display.max_rows\", 25)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5987b26",
   "metadata": {},
   "source": [
    "# 1. Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e640272a",
   "metadata": {},
   "outputs": [],
   "source": [
    "p = Path(r\"data.csv\")\n",
    "if not p.exists():\n",
    "    raise FileNotFoundError(p)\n",
    "\n",
    "# Detect encoding and delimiter\n",
    "encoding = \"utf-8\"\n",
    "try:\n",
    "    sample = p.read_text(encoding=encoding)[:8192]\n",
    "except UnicodeDecodeError:\n",
    "    encoding = \"latin-1\"\n",
    "    sample = p.read_text(encoding=encoding)[:8192]\n",
    "\n",
    "try:\n",
    "    delim = csv.Sniffer().sniff(sample).delimiter\n",
    "except Exception:\n",
    "    delim = \",\"\n",
    "\n",
    "df = pd.read_csv(p, sep=delim, encoding=encoding, low_memory=False, parse_dates=True)\n",
    "print(\"Shape:\", df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f200ae17",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Number of columns: {df.shape[1]}\")\n",
    "print(f\"Number of rows: {df.shape[0]}\")\n",
    "print(\"\\nColumn names:\", df.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be117543",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(columns=[\"WEO Country Code\", \"ISO\", \"Country/Series-specific Notes\", \"Subject Notes\", \n",
    "                 \"Units\", \"Scale\", \"Estimates Start After\", \"Subject Descriptor\"], inplace=True)\n",
    "\n",
    "codes = {\n",
    "    # Core growth & external\n",
    "    \"NGDP_RPCH\", \"NGDPRPC\", \"PCPIPCH\", \"TX_RPCH\", \"TM_RPCH\", \"BCA_NGDPD\",\n",
    "    # Fiscal & debt aggregates\n",
    "    \"GGR_NGDP\", \"GGX_NGDP\", \"GGXWDN_NGDP\", \"GGXWDG_NGDP\",\n",
    "    # Savings & investment\n",
    "    \"NGSD_NGDP\", \"NID_NGDP\",\n",
    "    # Prices\n",
    "    \"PCPI\", \"LUR\"\n",
    "}\n",
    "\n",
    "col = \"WEO Subject Code\"\n",
    "\n",
    "if col not in df.columns:\n",
    "    raise KeyError(f\"Column {col!r} not found in dataframe\")\n",
    "\n",
    "df = df[df[col].astype(str).str.strip().isin(codes)].copy()\n",
    "print(\"shape after filter:\", df.shape)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60010051",
   "metadata": {},
   "source": [
    "## Data Reshaping: Wide to Long to Wide"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab8289b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "year_cols = df.columns[2:]\n",
    "\n",
    "df[year_cols] = df[year_cols].replace({',': ''}, regex=True)\n",
    "df[year_cols] = df[year_cols].apply(pd.to_numeric, errors=\"coerce\")\n",
    "\n",
    "df[\"Country\"] = (\n",
    "    df[\"Country\"]\n",
    "    .str.replace(\" \", \"_\")\n",
    "    .str.replace(\"'\", \"\")\n",
    "    .str.replace(\"-\", \"_\")\n",
    ")\n",
    "\n",
    "df_long = df.melt(id_vars=[\"WEO Subject Code\", \"Country\"],\n",
    "                  var_name=\"Year\", value_name=\"Value\")\n",
    "\n",
    "df_long[\"Year\"] = df_long[\"Year\"].astype(str).str.strip()\n",
    "df_long = df_long[df_long[\"Year\"].str.fullmatch(r\"\\d{4}\")].copy()\n",
    "df_long[\"Year\"] = df_long[\"Year\"].astype(int)\n",
    "\n",
    "df_long[\"Value\"] = (\n",
    "    df_long[\"Value\"].astype(str)\n",
    "    .str.replace(\",\", \"\")\n",
    "    .replace({\"\": None, \"nan\": None})\n",
    "    .astype(float)\n",
    ")\n",
    "\n",
    "df_pivot = df_long.pivot_table(\n",
    "    index=[\"Country\", \"Year\"],\n",
    "    columns=\"WEO Subject Code\",\n",
    "    values=\"Value\",\n",
    "    aggfunc=\"first\"\n",
    ").reset_index()\n",
    "\n",
    "df_pivot.columns.name = None\n",
    "df_pivot = df_pivot.set_index(\"Year\")\n",
    "\n",
    "df_pivot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e2b1eba",
   "metadata": {},
   "source": [
    "# 3. Feature Engineering\n",
    "\n",
    "## Add Recession Target Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d940258",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Step 1: Ensure chronological order ---\n",
    "df_pivot = df_pivot.sort_index()\n",
    "\n",
    "# --- Step 2: Construct diagnostic flags (not stored in df) ---\n",
    "\n",
    "# 1. GDP-based recession (two consecutive annual declines)\n",
    "flag_gdp = (\n",
    "    (df_pivot.groupby(\"Country\")[\"NGDPRPC\"].transform(lambda x: x.pct_change() < 0)) &\n",
    "    (df_pivot.groupby(\"Country\")[\"NGDPRPC\"].transform(lambda x: x.pct_change().shift(-1) < 0))\n",
    ").astype(int)\n",
    "\n",
    "# 2. Investment collapse (sharp drop in investment)\n",
    "flag_invest = (\n",
    "    df_pivot.groupby(\"Country\")[\"NID_NGDP\"].transform(lambda x: x.diff() < -2)\n",
    ").astype(int)\n",
    "\n",
    "# 3. Savings decline (household/national savings falling)\n",
    "flag_savings = (\n",
    "    df_pivot.groupby(\"Country\")[\"NGSD_NGDP\"].transform(lambda x: x.diff() < -2)\n",
    ").astype(int)\n",
    "\n",
    "# 4. Trade shock (both exports and imports contracting)\n",
    "flag_trade = (\n",
    "    (df_pivot.groupby(\"Country\")[\"TX_RPCH\"].transform(lambda x: x < 0)) &\n",
    "    (df_pivot.groupby(\"Country\")[\"TM_RPCH\"].transform(lambda x: x < 0))\n",
    ").astype(int)\n",
    "\n",
    "# 5. Inflation shock (stagflation: high inflation + negative growth)\n",
    "flag_inflation = (\n",
    "    (df_pivot.groupby(\"Country\")[\"PCPIPCH\"].transform(lambda x: x > 10)) &\n",
    "    (df_pivot.groupby(\"Country\")[\"NGDPRPC\"].transform(lambda x: x.pct_change() < 0))\n",
    ").astype(int)\n",
    "\n",
    "# 6. Debt crisis (gross debt rising sharply above 90% of GDP)\n",
    "flag_debt = (\n",
    "    (df_pivot[\"GGXWDG_NGDP\"] > 90) &\n",
    "    (df_pivot.groupby(\"Country\")[\"GGXWDG_NGDP\"].transform(lambda x: x.diff() > 10))\n",
    ").astype(int)\n",
    "\n",
    "# 7. Fiscal crisis (large and growing deficit)\n",
    "flag_fiscal = (\n",
    "    (df_pivot.groupby(\"Country\")[\"GGR_NGDP\"].transform(lambda x: x) < \n",
    "     df_pivot.groupby(\"Country\")[\"GGX_NGDP\"].transform(lambda x: x) - 5) &  # Deficit > 5% GDP\n",
    "    (df_pivot.groupby(\"Country\")[\"GGX_NGDP\"].transform(lambda x: x.diff()) > 2)  # Rising spending\n",
    ").astype(int)\n",
    "\n",
    "# 8. Current account crisis (large deficit deteriorating)\n",
    "flag_current_account = (\n",
    "    (df_pivot[\"BCA_NGDPD\"] < -5) &  # Deficit > 5% of GDP\n",
    "    (df_pivot.groupby(\"Country\")[\"BCA_NGDPD\"].transform(lambda x: x.diff() < -2))  # Worsening\n",
    ").astype(int)\n",
    "\n",
    "# 9. Real GDP growth collapse (severe contraction)\n",
    "flag_growth_collapse = (\n",
    "    df_pivot.groupby(\"Country\")[\"NGDP_RPCH\"].transform(lambda x: x < -3)  # Growth < -3%\n",
    ").astype(int)\n",
    "\n",
    "# 10. Deflation risk (falling prices with economic weakness)\n",
    "flag_deflation = (\n",
    "    (df_pivot.groupby(\"Country\")[\"PCPIPCH\"].transform(lambda x: x < 0)) &  # Negative inflation\n",
    "    (df_pivot.groupby(\"Country\")[\"NGDP_RPCH\"].transform(lambda x: x < 1))  # Weak growth\n",
    ").astype(int)\n",
    "\n",
    "# 11. Credit crunch (investment falling while debt rising)\n",
    "flag_credit_crunch = (\n",
    "    (df_pivot.groupby(\"Country\")[\"NID_NGDP\"].transform(lambda x: x.diff() < -1)) &\n",
    "    (df_pivot.groupby(\"Country\")[\"GGXWDG_NGDP\"].transform(lambda x: x.diff() > 5))\n",
    ").astype(int)\n",
    "\n",
    "# 12. External shock (exports collapsing while imports stable/rising)\n",
    "flag_external_shock = (\n",
    "    (df_pivot.groupby(\"Country\")[\"TX_RPCH\"].transform(lambda x: x < -5)) &\n",
    "    (df_pivot.groupby(\"Country\")[\"TM_RPCH\"].transform(lambda x: x > -2))\n",
    ").astype(int)\n",
    "\n",
    "# --- Step 3: Combine signals into a single severity score ---\n",
    "local_signal_count = (\n",
    "    flag_gdp + \n",
    "    flag_invest + \n",
    "    flag_savings + \n",
    "    flag_trade + \n",
    "    flag_inflation +\n",
    "    flag_debt +\n",
    "    flag_fiscal +\n",
    "    flag_current_account +\n",
    "    flag_growth_collapse +\n",
    "    flag_deflation +\n",
    "    flag_credit_crunch +\n",
    "    flag_external_shock\n",
    ")\n",
    "\n",
    "# --- Step 4: Multiclass recession risk label ---\n",
    "def classify_risk(local_count):\n",
    "    \"\"\"\n",
    "    Same thresholds as original:\n",
    "    - 3+ signals: High risk\n",
    "    - 2 signals: Moderate risk  \n",
    "    - 1 signal: Mild risk\n",
    "    - 0 signals: No risk\n",
    "    \"\"\"\n",
    "    if local_count >= 3:\n",
    "        return 3  # High risk\n",
    "    if local_count == 2:\n",
    "        return 2  # Moderate risk\n",
    "    if local_count == 1:\n",
    "        return 1  # Mild risk\n",
    "    return 0      # No risk\n",
    "\n",
    "df_pivot[\"RecessionRisk\"] = [\n",
    "    classify_risk(l) for l in local_signal_count\n",
    "]\n",
    "\n",
    "# Store signal count\n",
    "df_pivot[\"SignalCount\"] = local_signal_count\n",
    "\n",
    "# --- Step 5: Fill NaN with mean instead of dropping ---\n",
    "# Fill numeric columns with their mean\n",
    "numeric_cols = df_pivot.select_dtypes(include=[np.number]).columns\n",
    "for col in numeric_cols:\n",
    "    df_pivot[col] = df_pivot[col].fillna(df_pivot[col].mean())\n",
    "\n",
    "# Fill non-numeric columns with forward fill or mode\n",
    "non_numeric_cols = df_pivot.select_dtypes(exclude=[np.number]).columns\n",
    "for col in non_numeric_cols:\n",
    "    if col == 'Country':\n",
    "        continue  # Don't fill Country column\n",
    "    df_pivot[col] = df_pivot[col].fillna(method='ffill').fillna(method='bfill')\n",
    "\n",
    "df_pivot = df_pivot.sort_index(ascending=True)\n",
    "\n",
    "# --- Step 6: Show distribution ---\n",
    "print(\"=\" * 70)\n",
    "print(\"RECESSION RISK CLASSIFICATION SUMMARY (12 Indicators)\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(\"\\nRecessionRisk class distribution:\")\n",
    "print(df_pivot[\"RecessionRisk\"].value_counts().sort_index())\n",
    "print(f\"\\nTotal samples: {len(df_pivot)}\")\n",
    "\n",
    "# Show average signals per risk class\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"AVERAGE SIGNALS TRIGGERED PER RISK CLASS:\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "signal_summary = df_pivot.groupby('RecessionRisk')['SignalCount'].agg(['mean', 'min', 'max', 'count'])\n",
    "print(signal_summary)\n",
    "\n",
    "# Show which signals are most common\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"INDIVIDUAL SIGNAL FREQUENCIES:\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Recalculate flags for cleaned data\n",
    "flag_gdp_clean = (\n",
    "    (df_pivot.groupby(\"Country\")[\"NGDPRPC\"].transform(lambda x: x.pct_change() < 0)) &\n",
    "    (df_pivot.groupby(\"Country\")[\"NGDPRPC\"].transform(lambda x: x.pct_change().shift(-1) < 0))\n",
    ").astype(int)\n",
    "\n",
    "flag_invest_clean = (\n",
    "    df_pivot.groupby(\"Country\")[\"NID_NGDP\"].transform(lambda x: x.diff() < -2)\n",
    ").astype(int)\n",
    "\n",
    "flag_savings_clean = (\n",
    "    df_pivot.groupby(\"Country\")[\"NGSD_NGDP\"].transform(lambda x: x.diff() < -2)\n",
    ").astype(int)\n",
    "\n",
    "flag_trade_clean = (\n",
    "    (df_pivot.groupby(\"Country\")[\"TX_RPCH\"].transform(lambda x: x < 0)) &\n",
    "    (df_pivot.groupby(\"Country\")[\"TM_RPCH\"].transform(lambda x: x < 0))\n",
    ").astype(int)\n",
    "\n",
    "flag_inflation_clean = (\n",
    "    (df_pivot.groupby(\"Country\")[\"PCPIPCH\"].transform(lambda x: x > 10)) &\n",
    "    (df_pivot.groupby(\"Country\")[\"NGDPRPC\"].transform(lambda x: x.pct_change() < 0))\n",
    ").astype(int)\n",
    "\n",
    "flag_debt_clean = (\n",
    "    (df_pivot[\"GGXWDG_NGDP\"] > 90) &\n",
    "    (df_pivot.groupby(\"Country\")[\"GGXWDG_NGDP\"].transform(lambda x: x.diff() > 10))\n",
    ").astype(int)\n",
    "\n",
    "flag_fiscal_clean = (\n",
    "    (df_pivot.groupby(\"Country\")[\"GGR_NGDP\"].transform(lambda x: x) < \n",
    "     df_pivot.groupby(\"Country\")[\"GGX_NGDP\"].transform(lambda x: x) - 5) &\n",
    "    (df_pivot.groupby(\"Country\")[\"GGX_NGDP\"].transform(lambda x: x.diff()) > 2)\n",
    ").astype(int)\n",
    "\n",
    "flag_current_account_clean = (\n",
    "    (df_pivot[\"BCA_NGDPD\"] < -5) &\n",
    "    (df_pivot.groupby(\"Country\")[\"BCA_NGDPD\"].transform(lambda x: x.diff() < -2))\n",
    ").astype(int)\n",
    "\n",
    "flag_growth_collapse_clean = (\n",
    "    df_pivot.groupby(\"Country\")[\"NGDP_RPCH\"].transform(lambda x: x < -3)\n",
    ").astype(int)\n",
    "\n",
    "flag_deflation_clean = (\n",
    "    (df_pivot.groupby(\"Country\")[\"PCPIPCH\"].transform(lambda x: x < 0)) &\n",
    "    (df_pivot.groupby(\"Country\")[\"NGDP_RPCH\"].transform(lambda x: x < 1))\n",
    ").astype(int)\n",
    "\n",
    "flag_credit_crunch_clean = (\n",
    "    (df_pivot.groupby(\"Country\")[\"NID_NGDP\"].transform(lambda x: x.diff() < -1)) &\n",
    "    (df_pivot.groupby(\"Country\")[\"GGXWDG_NGDP\"].transform(lambda x: x.diff() > 5))\n",
    ").astype(int)\n",
    "\n",
    "flag_external_shock_clean = (\n",
    "    (df_pivot.groupby(\"Country\")[\"TX_RPCH\"].transform(lambda x: x < -5)) &\n",
    "    (df_pivot.groupby(\"Country\")[\"TM_RPCH\"].transform(lambda x: x > -2))\n",
    ").astype(int)\n",
    "\n",
    "signal_names = [\n",
    "    'GDP Decline', 'Investment Collapse', 'Savings Decline', 'Trade Shock',\n",
    "    'Inflation Shock', 'Debt Crisis', 'Fiscal Crisis', 'Current Account Crisis',\n",
    "    'Growth Collapse', 'Deflation Risk', 'Credit Crunch', 'External Shock'\n",
    "]\n",
    "\n",
    "signals_clean = [\n",
    "    flag_gdp_clean, flag_invest_clean, flag_savings_clean, flag_trade_clean, \n",
    "    flag_inflation_clean, flag_debt_clean, flag_fiscal_clean, flag_current_account_clean,\n",
    "    flag_growth_collapse_clean, flag_deflation_clean, flag_credit_crunch_clean, \n",
    "    flag_external_shock_clean\n",
    "]\n",
    "\n",
    "for name, signal in zip(signal_names, signals_clean):\n",
    "    count = signal.sum()\n",
    "    pct = (count / len(df_pivot)) * 100\n",
    "    print(f\"{name:<25} {count:>5} ({pct:>5.2f}%)\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "\n",
    "# Remove temporary column\n",
    "df_pivot = df_pivot.drop(columns=['SignalCount'])\n",
    "\n",
    "# --- Output: dataframe with multiclass label ---\n",
    "df_pivot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "138dee31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RecessionRisk Class Distribution - Compact Plot (up to 2025)\n",
    "df_until_2025 = df_pivot[df_pivot.index <= 2025]\n",
    "risk_counts = df_until_2025[\"RecessionRisk\"].value_counts().sort_index()\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "colors = ['green', 'yellow', 'orange', 'red']\n",
    "plt.bar(risk_counts.index, risk_counts.values, color=colors, edgecolor='black', linewidth=1.5)\n",
    "plt.xlabel('RecessionRisk Class', fontsize=12, fontweight='bold')\n",
    "plt.ylabel('Count', fontsize=12, fontweight='bold')\n",
    "plt.title('RecessionRisk Class Distribution (up to 2025)', fontsize=14, fontweight='bold')\n",
    "plt.xticks([0, 1, 2, 3], ['No Risk', 'Mild', 'Moderate', 'High'])\n",
    "plt.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Add count labels on bars\n",
    "for cls, count in risk_counts.items():\n",
    "    plt.text(cls, count + 10, str(count), ha='center', va='bottom', fontsize=11, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Total samples (up to 2025): {len(df_until_2025)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a9c6b19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# All RecessionRisk Classes by Year - Line Graph (up to 2025)\n",
    "# Calculate counts for each class by year\n",
    "df0 = df_until_2025[df_until_2025[\"RecessionRisk\"] == 0].copy()\n",
    "df1 = df_until_2025[df_until_2025[\"RecessionRisk\"] == 1].copy()\n",
    "df2 = df_until_2025[df_until_2025[\"RecessionRisk\"] == 2].copy()\n",
    "df3 = df_until_2025[df_until_2025[\"RecessionRisk\"] == 3].copy()\n",
    "\n",
    "class0_by_year = df0.groupby(df0.index).size()\n",
    "class1_by_year = df1.groupby(df1.index).size()\n",
    "class2_by_year = df2.groupby(df2.index).size()\n",
    "class3_by_year = df3.groupby(df3.index).size()\n",
    "\n",
    "# Get all years (up to 2025)\n",
    "all_years = sorted(df_until_2025.index.unique())\n",
    "\n",
    "# Reindex to include all years (fill missing with 0)\n",
    "class0_by_year = class0_by_year.reindex(all_years, fill_value=0)\n",
    "class1_by_year = class1_by_year.reindex(all_years, fill_value=0)\n",
    "class2_by_year = class2_by_year.reindex(all_years, fill_value=0)\n",
    "class3_by_year = class3_by_year.reindex(all_years, fill_value=0)\n",
    "\n",
    "plt.figure(figsize=(14, 7))\n",
    "plt.plot(all_years, class0_by_year.values, color='green', linewidth=2.5, marker='o', label='Class 0 (No Risk)', markersize=5)\n",
    "plt.plot(all_years, class1_by_year.values, color='gold', linewidth=2.5, marker='s', label='Class 1 (Mild)', markersize=5)\n",
    "plt.plot(all_years, class2_by_year.values, color='orange', linewidth=2.5, marker='^', label='Class 2 (Moderate)', markersize=5)\n",
    "plt.plot(all_years, class3_by_year.values, color='red', linewidth=2.5, marker='D', label='Class 3 (High)', markersize=5)\n",
    "\n",
    "plt.xlabel('Year', fontsize=12, fontweight='bold')\n",
    "plt.ylabel('Number of Countries', fontsize=12, fontweight='bold')\n",
    "plt.title('RecessionRisk Classes Over Time (up to 2030)', fontsize=14, fontweight='bold')\n",
    "plt.legend(fontsize=11, loc='best')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d627b020",
   "metadata": {},
   "source": [
    "## Cutting Out Data that leads to biasness & Review Remaining Countries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf4aca09",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filtered = df_pivot.loc[(df_pivot.index >= 2000) & (df_pivot.index <= 2025)]\n",
    "df_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "422b2731",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_filtered[\"Country\"].unique())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c94f2e23",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filtered[\"Country\"].nunique()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec37a376",
   "metadata": {},
   "source": [
    "# 4. Exploratory Data Analysis\n",
    "\n",
    "## Correlation Heatmap"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59c5b2f5",
   "metadata": {},
   "source": [
    "## Prepare Features and Target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3382119f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_filtered.drop(columns=[\"RecessionRisk\", \"Country\"])\n",
    "y = df_filtered[\"RecessionRisk\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5baccf90",
   "metadata": {},
   "source": [
    "# 5. Machine Learning Models\n",
    "\n",
    "## Global Dataset - Full Features (13 Features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb849a40",
   "metadata": {},
   "source": [
    "### Define and Train All Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e8774c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "#             TRAIN ALL MODELS (MULTICLASS) - IMPROVED\n",
    "# ============================================================\n",
    "def train_all_models(X_train, y_train, X_val, y_val, X_test, y_test, model_params=None, use_xgb=False):\n",
    "    \"\"\"\n",
    "    Enhanced model training with validation support.\n",
    "    \"\"\"\n",
    "    if model_params is None:\n",
    "        model_params = {\n",
    "            'logit': {\n",
    "                'C': 0.3,  # Increased regularization (lower C)\n",
    "                'penalty': 'l2',\n",
    "                'solver': 'saga',\n",
    "                'max_iter': 10000,\n",
    "                'random_state': 42,\n",
    "                'class_weight': 'balanced'\n",
    "            },\n",
    "            'rf': {\n",
    "                'n_estimators': 200,  # Reduced from 300\n",
    "                'max_depth': 5,  # Reduced from 6\n",
    "                'min_samples_leaf': 20,  # Increased from 15\n",
    "                'min_samples_split': 40,  # Increased from 30\n",
    "                'max_features': 'sqrt',\n",
    "                'random_state': 42,\n",
    "                'class_weight': 'balanced',\n",
    "                'n_jobs': -1,\n",
    "                'max_samples': 0.8  # Bootstrap with 80% samples\n",
    "            },\n",
    "            'gb': {\n",
    "                'n_estimators': 150,  # Reduced from 300\n",
    "                'learning_rate': 0.03,  # Reduced from 0.05\n",
    "                'max_depth': 3,\n",
    "                'min_samples_leaf': 25,  # Increased from 15\n",
    "                'min_samples_split': 50,  # Added constraint\n",
    "                'subsample': 0.7,\n",
    "                'max_features': 'sqrt',\n",
    "                'random_state': 42,\n",
    "                'validation_fraction': 0.1,  # Early stopping support\n",
    "                'n_iter_no_change': 20,  # Stop if no improvement\n",
    "                'tol': 0.0001\n",
    "            },\n",
    "            'dt': {\n",
    "                'max_depth': 4,\n",
    "                'min_samples_leaf': 30,  # Increased from 25\n",
    "                'min_samples_split': 60,  # Increased from 50\n",
    "                'random_state': 42,\n",
    "                'class_weight': 'balanced',\n",
    "                'ccp_alpha': 0.001  # Minimal cost complexity pruning\n",
    "            },\n",
    "            'svm': {\n",
    "                'C': 0.5,  # Increased regularization (lower C from 2.0)\n",
    "                'kernel': 'rbf',\n",
    "                'gamma': 'scale',\n",
    "                'probability': True,\n",
    "                'random_state': 42,\n",
    "                'decision_function_shape': 'ovr',\n",
    "                'class_weight': 'balanced'\n",
    "            },\n",
    "            'xgb': {\n",
    "                'n_estimators': 150,  # Reduced from 300\n",
    "                'learning_rate': 0.05,  # Reduced from 0.07\n",
    "                'max_depth': 3,\n",
    "                'subsample': 0.7,  # Reduced from 0.8\n",
    "                'colsample_bytree': 0.7,  # Reduced from 0.8\n",
    "                'reg_alpha': 0.5,  # Increased L1 regularization\n",
    "                'reg_lambda': 2.0,  # Increased L2 regularization\n",
    "                'min_child_weight': 3,  # Added constraint\n",
    "                'random_state': 42,\n",
    "                'objective': 'multi:softprob',\n",
    "                'num_class': len(np.unique(y_train)),\n",
    "                'tree_method': 'hist',\n",
    "                'early_stopping_rounds': 20  # Early stopping\n",
    "            }\n",
    "        }\n",
    "\n",
    "    # ----------------- SMOTE -----------------\n",
    "    smote_kwargs = {'random_state': 42, 'k_neighbors': 3}\n",
    "\n",
    "    logit = ImbPipeline([\n",
    "        (\"scaler\", StandardScaler()),\n",
    "        (\"smote\", SMOTE(**smote_kwargs)),\n",
    "        (\"logit\", LogisticRegression(**model_params['logit']))\n",
    "    ]).fit(X_train, y_train)\n",
    "\n",
    "    rf = ImbPipeline([\n",
    "        (\"smote\", SMOTE(**smote_kwargs)),\n",
    "        (\"rf\", RandomForestClassifier(**model_params['rf']))\n",
    "    ]).fit(X_train, y_train)\n",
    "\n",
    "    gb = ImbPipeline([\n",
    "        (\"smote\", SMOTE(**smote_kwargs)),\n",
    "        (\"gb\", GradientBoostingClassifier(**model_params['gb']))\n",
    "    ]).fit(X_train, y_train)\n",
    "\n",
    "    dt = ImbPipeline([\n",
    "        (\"smote\", SMOTE(**smote_kwargs)),\n",
    "        (\"dt\", DecisionTreeClassifier(**model_params['dt']))\n",
    "    ]).fit(X_train, y_train)\n",
    "\n",
    "    svm = ImbPipeline([\n",
    "        (\"scaler\", StandardScaler()),\n",
    "        (\"smote\", SMOTE(**smote_kwargs)),\n",
    "        (\"svm\", SVC(**model_params['svm']))\n",
    "    ]).fit(X_train, y_train)\n",
    "\n",
    "    models = {\n",
    "        \"Logistic Regression\": logit,\n",
    "        \"Random Forest\": rf,\n",
    "        \"Gradient Boosting\": gb,\n",
    "        \"Decision Tree\": dt,\n",
    "        \"SVM\": svm,\n",
    "    }\n",
    "\n",
    "    if use_xgb:\n",
    "        xgb = ImbPipeline([\n",
    "            (\"smote\", SMOTE(**smote_kwargs)),\n",
    "            (\"xgb\", XGBClassifier(**model_params['xgb']))\n",
    "        ]).fit(X_train, y_train)\n",
    "        models[\"XGBoost\"] = xgb\n",
    "\n",
    "    # ----------------- Ensemble -----------------\n",
    "    ensemble_estimators = [\n",
    "        (\"rf\", rf.named_steps[\"rf\"]),\n",
    "        (\"gb\", gb.named_steps[\"gb\"]),\n",
    "        (\"svm\", svm.named_steps[\"svm\"]),\n",
    "        (\"logit\", logit.named_steps[\"logit\"])\n",
    "    ]\n",
    "    ensemble_weights = [2, 2, 1, 1]\n",
    "\n",
    "    if use_xgb:\n",
    "        ensemble_estimators.append((\"xgb\", xgb.named_steps[\"xgb\"]))\n",
    "        ensemble_weights.append(2)\n",
    "\n",
    "    ensemble = VotingClassifier(\n",
    "        estimators=ensemble_estimators,\n",
    "        voting=\"soft\",\n",
    "        weights=ensemble_weights\n",
    "    )\n",
    "    ensemble.fit(X_train, y_train)\n",
    "    models[\"Ensemble (Weighted)\"] = ensemble\n",
    "\n",
    "    # ----------------- Metrics -----------------\n",
    "    results = {}\n",
    "    confusion_mats = {}\n",
    "\n",
    "    for name, m in models.items():\n",
    "        # Predictions\n",
    "        y_pred_train = m.predict(X_train)\n",
    "        y_pred_val = m.predict(X_val)\n",
    "        y_pred_test = m.predict(X_test)\n",
    "\n",
    "        results[name] = {\n",
    "            \"Train Accuracy\": accuracy_score(y_train, y_pred_train),\n",
    "            \"Val Accuracy\": accuracy_score(y_val, y_pred_val),\n",
    "            \"Test Accuracy\": accuracy_score(y_test, y_pred_test),\n",
    "\n",
    "            \"Val F1 (macro)\": f1_score(y_val, y_pred_val, average=\"macro\", zero_division=0),\n",
    "            \"Test F1 (macro)\": f1_score(y_test, y_pred_test, average=\"macro\", zero_division=0),\n",
    "\n",
    "            \"Val F1 (weighted)\": f1_score(y_val, y_pred_val, average=\"weighted\", zero_division=0),\n",
    "            \"Test F1 (weighted)\": f1_score(y_test, y_pred_test, average=\"weighted\", zero_division=0),\n",
    "        }\n",
    "\n",
    "        confusion_mats[name] = {\n",
    "            \"val\": confusion_matrix(y_val, y_pred_val),\n",
    "            \"test\": confusion_matrix(y_test, y_pred_test)\n",
    "        }\n",
    "\n",
    "    results_df = pd.DataFrame(results).T\n",
    "    return models, results_df, confusion_mats\n",
    "\n",
    "# ============================================================\n",
    "#             FEATURE IMPORTANCE (MULTICLASS)\n",
    "# ============================================================\n",
    "def plot_feature_importance(models, feature_names, title_prefix=\"\", top_n=15):\n",
    "    \"\"\"\n",
    "    Display feature importance with improved visualization.\n",
    "    top_n: Show only top N most important features\n",
    "    \"\"\"\n",
    "    logit = models.get(\"Logistic Regression\")\n",
    "    rf = models.get(\"Random Forest\")\n",
    "    gb = models.get(\"Gradient Boosting\")\n",
    "    dt = models.get(\"Decision Tree\")\n",
    "\n",
    "    coef_matrix = logit.named_steps['logit'].coef_\n",
    "    coef_mean_abs = np.mean(np.abs(coef_matrix), axis=0)\n",
    "\n",
    "    logit_importance = pd.DataFrame({\n",
    "        \"Feature\": feature_names,\n",
    "        \"Importance\": coef_mean_abs\n",
    "    }).sort_values(\"Importance\", ascending=False).head(top_n)\n",
    "\n",
    "    rf_importance = pd.DataFrame({\n",
    "        \"Feature\": feature_names,\n",
    "        \"Importance\": rf.named_steps['rf'].feature_importances_\n",
    "    }).sort_values(\"Importance\", ascending=False).head(top_n)\n",
    "\n",
    "    gb_importance = pd.DataFrame({\n",
    "        \"Feature\": feature_names,\n",
    "        \"Importance\": gb.named_steps['gb'].feature_importances_\n",
    "    }).sort_values(\"Importance\", ascending=False).head(top_n)\n",
    "\n",
    "    dt_importance = pd.DataFrame({\n",
    "        \"Feature\": feature_names,\n",
    "        \"Importance\": dt.named_steps['dt'].feature_importances_\n",
    "    }).sort_values(\"Importance\", ascending=False).head(top_n)\n",
    "\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(16, 10))\n",
    "\n",
    "    # Sort in ascending order for better visualization\n",
    "    logit_sorted = logit_importance.sort_values(\"Importance\")\n",
    "    rf_sorted = rf_importance.sort_values(\"Importance\")\n",
    "    gb_sorted = gb_importance.sort_values(\"Importance\")\n",
    "    dt_sorted = dt_importance.sort_values(\"Importance\")\n",
    "\n",
    "    axes[0, 0].barh(logit_sorted[\"Feature\"], logit_sorted[\"Importance\"], color='steelblue')\n",
    "    axes[0, 0].set_title(f\"{title_prefix}Logistic Regression (Top {top_n})\", fontsize=12, fontweight='bold')\n",
    "    axes[0, 0].set_xlabel(\"Mean |Coefficient|\")\n",
    "\n",
    "    axes[0, 1].barh(rf_sorted[\"Feature\"], rf_sorted[\"Importance\"], color='forestgreen')\n",
    "    axes[0, 1].set_title(f\"{title_prefix}Random Forest (Top {top_n})\", fontsize=12, fontweight='bold')\n",
    "    axes[0, 1].set_xlabel(\"Feature Importance\")\n",
    "\n",
    "    axes[1, 0].barh(gb_sorted[\"Feature\"], gb_sorted[\"Importance\"], color='darkorange')\n",
    "    axes[1, 0].set_title(f\"{title_prefix}Gradient Boosting (Top {top_n})\", fontsize=12, fontweight='bold')\n",
    "    axes[1, 0].set_xlabel(\"Feature Importance\")\n",
    "\n",
    "    axes[1, 1].barh(dt_sorted[\"Feature\"], dt_sorted[\"Importance\"], color='crimson')\n",
    "    axes[1, 1].set_title(f\"{title_prefix}Decision Tree (Top {top_n})\", fontsize=12, fontweight='bold')\n",
    "    axes[1, 1].set_xlabel(\"Feature Importance\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "#             CONFUSION MATRIX DISPLAY (MULTICLASS)\n",
    "# ============================================================\n",
    "def show_confusion_matrices(confusion_mats, results_df):\n",
    "    \"\"\"\n",
    "    confusion_mats: dict {\n",
    "        model_name: {\n",
    "            \"val\":  cm_val,\n",
    "            \"test\": cm_test\n",
    "        }\n",
    "    }\n",
    "    \"\"\"\n",
    "\n",
    "    # Force exactly 3 rows × 2 columns = 6 windows\n",
    "    n_rows = 3\n",
    "    n_cols = 2\n",
    "\n",
    "    fig, axes = plt.subplots(n_rows, n_cols, figsize=(12, 15))\n",
    "\n",
    "    # Flatten axes for easy indexing\n",
    "    axes = axes.reshape(n_rows, n_cols)\n",
    "\n",
    "    # Only take the first 3 models (or pad if fewer)\n",
    "    model_items = list(confusion_mats.items())[:3]\n",
    "\n",
    "    for row_idx, (model_name, cms) in enumerate(model_items):\n",
    "\n",
    "        # -------------------------\n",
    "        # VALIDATION\n",
    "        # -------------------------\n",
    "        cm_val = cms[\"val\"]\n",
    "        disp_val = ConfusionMatrixDisplay(confusion_matrix=cm_val)\n",
    "        disp_val.plot(cmap=\"Blues\", ax=axes[row_idx, 0], colorbar=False)\n",
    "\n",
    "        acc_val = results_df.loc[model_name, \"Val Accuracy\"]\n",
    "        f1_val = results_df.loc[model_name, \"Val F1 (macro)\"]\n",
    "\n",
    "        axes[row_idx, 0].set_title(\n",
    "            f\"{model_name} — Validation\\nAcc={acc_val:.3f} | F1_macro={f1_val:.3f}\",\n",
    "            fontsize=11, fontweight=\"bold\"\n",
    "        )\n",
    "\n",
    "        # -------------------------\n",
    "        # TEST\n",
    "        # -------------------------\n",
    "        cm_test = cms[\"test\"]\n",
    "        disp_test = ConfusionMatrixDisplay(confusion_matrix=cm_test)\n",
    "        disp_test.plot(cmap=\"Blues\", ax=axes[row_idx, 1], colorbar=False)\n",
    "\n",
    "        acc_test = results_df.loc[model_name, \"Test Accuracy\"]\n",
    "        f1_test = results_df.loc[model_name, \"Test F1 (macro)\"]\n",
    "\n",
    "        axes[row_idx, 1].set_title(\n",
    "            f\"{model_name} — Test\\nAcc={acc_test:.3f} | F1_macro={f1_test:.3f}\",\n",
    "            fontsize=11, fontweight=\"bold\"\n",
    "        )\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# ============================================================\n",
    "#             ROC CURVES + MACRO AUC (MULTICLASS)\n",
    "# ============================================================\n",
    "def show_roc_curves(models, X, y, split_name=\"Test\"):\n",
    "    \"\"\"\n",
    "    Display ROC curves and macro AUC for a given split (val or test).\n",
    "    \"\"\"\n",
    "    n_models = len(models)\n",
    "    fig, axes = plt.subplots(1, n_models, figsize=(6*n_models, 5))\n",
    "\n",
    "    if n_models == 1:\n",
    "        axes = [axes]\n",
    "\n",
    "    classes = sorted(np.unique(y))\n",
    "\n",
    "    for ax, (name, model) in zip(axes, models.items()):\n",
    "\n",
    "        if not hasattr(model, \"predict_proba\"):\n",
    "            ax.set_title(f\"{name}\\n(No predict_proba)\")\n",
    "            ax.axis(\"off\")\n",
    "            continue\n",
    "\n",
    "        y_proba = model.predict_proba(X)\n",
    "\n",
    "        aucs = []\n",
    "        for c in classes:\n",
    "            y_true_bin = (y == c).astype(int)\n",
    "            y_score = y_proba[:, c]\n",
    "\n",
    "            fpr, tpr, _ = roc_curve(y_true_bin, y_score)\n",
    "            auc_val = auc(fpr, tpr)\n",
    "            aucs.append(auc_val)\n",
    "\n",
    "            ax.plot(fpr, tpr, label=f\"Class {c} (AUC={auc_val:.2f})\", linewidth=2)\n",
    "\n",
    "        macro_auc = np.mean(aucs)\n",
    "\n",
    "        ax.plot([0, 1], [0, 1], \"k--\", alpha=0.5)\n",
    "        ax.set_title(f\"{name} — {split_name} ROC\\nMacro AUC={macro_auc:.3f}\", fontsize=11, fontweight='bold')\n",
    "        ax.set_xlabel(\"False Positive Rate\")\n",
    "        ax.set_ylabel(\"True Positive Rate\")\n",
    "        ax.legend(loc='lower right', fontsize=8)\n",
    "        ax.grid(alpha=0.3)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d2cf4d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# Time-based split (Train / Val / Test)\n",
    "# -----------------------------\n",
    "\n",
    "SPLIT_YEAR = 2020     # Test starts here\n",
    "VAL_YEAR   = 2018     # Validation starts here\n",
    "\n",
    "# Masks\n",
    "train_mask = df_filtered.index < VAL_YEAR\n",
    "val_mask   = (df_filtered.index >= VAL_YEAR) & (df_filtered.index < SPLIT_YEAR)\n",
    "test_mask  = df_filtered.index >= SPLIT_YEAR\n",
    "\n",
    "# Apply masks\n",
    "X_train = X[train_mask]\n",
    "y_train = y[train_mask]\n",
    "\n",
    "X_val = X[val_mask]\n",
    "y_val = y[val_mask]\n",
    "\n",
    "X_test = X[test_mask]\n",
    "y_test = y[test_mask]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "685c21a0",
   "metadata": {},
   "source": [
    "# Training the model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "355300d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "models, summary_df, confusion_mats = train_all_models(\n",
    "    X_train, y_train,\n",
    "    X_val, y_val,\n",
    "    X_test, y_test\n",
    ")\n",
    "\n",
    "print(summary_df)\n",
    "\n",
    "plot_feature_importance(models, X_train.columns.tolist())\n",
    "\n",
    "# Show TEST confusion matrices\n",
    "show_confusion_matrices(confusion_mats, summary_df)\n",
    "\n",
    "# ROC curves on TEST\n",
    "show_roc_curves(models, X_test, y_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56b66c25",
   "metadata": {},
   "source": [
    "### Reduced Global Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65b8167b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#reduced variables set\n",
    "selected_features = ['TM_RPCH', 'NGDP_RPCH', 'TX_RPCH', 'BCA_NGDPD', 'PCPIPCH', 'NGDPRPC']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "107301e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------\n",
    "# YEAR-BASED SPLIT (same logic everywhere)\n",
    "# -----------------------------------------\n",
    "TRAIN_END = 2018     # Train: < 2018 (up to 2017)\n",
    "VAL_END   = 2021     # Validation: 2018-2020\n",
    "                     # Test: >= 2021 (2021-2025)\n",
    "\n",
    "train_mask = X.index < TRAIN_END\n",
    "val_mask   = (X.index >= TRAIN_END) & (X.index < VAL_END)\n",
    "test_mask  = X.index >= VAL_END\n",
    "\n",
    "X_train = X[train_mask]\n",
    "y_train = y[train_mask]\n",
    "\n",
    "X_val = X[val_mask]\n",
    "y_val = y[val_mask]\n",
    "\n",
    "X_test = X[test_mask]\n",
    "y_test = y[test_mask]\n",
    "print(f\"Train: {len(X_train)} samples (years < {TRAIN_END} [up to 2017])\")\n",
    "print(f\"Validation: {len(X_val)} samples ({TRAIN_END}-{VAL_END-1} [2018-2020])\")\n",
    "print(f\"Test: {len(X_test)} samples (years >= {VAL_END} [2021-2025])\")\n",
    "print(f\"Test: {len(X_test)} samples (years >= {VAL_END})\")\n",
    "\n",
    "# -----------------------------------------\n",
    "# REDUCE FEATURES FOR ALL SPLITS\n",
    "# -----------------------------------------\n",
    "X_train_reduced = X_train[selected_features]\n",
    "X_val_reduced   = X_val[selected_features]\n",
    "X_test_reduced  = X_test[selected_features]\n",
    "\n",
    "# -----------------------------------------\n",
    "# TRAIN MODELS WITH REDUCED FEATURES\n",
    "# -----------------------------------------\n",
    "models_reduced, summary_df_reduced, confusion_mats_reduced = train_all_models(\n",
    "    X_train_reduced, y_train,\n",
    "    X_val_reduced,   y_val,\n",
    "    X_test_reduced,  y_test\n",
    ")\n",
    "\n",
    "# -----------------------------------------\n",
    "# METRICS TABLE\n",
    "# -----------------------------------------\n",
    "print(summary_df_reduced)\n",
    "\n",
    "# -----------------------------------------\n",
    "# FEATURE IMPORTANCE\n",
    "# -----------------------------------------\n",
    "plot_feature_importance(\n",
    "    models_reduced,\n",
    "    feature_names=selected_features,\n",
    "    title_prefix=\"Reduced Features - \"\n",
    ")\n",
    "\n",
    "# -----------------------------------------\n",
    "# COMBINED VALIDATION + TEST CONFUSION MATRICES\n",
    "# -----------------------------------------\n",
    "show_confusion_matrices(confusion_mats_reduced, summary_df_reduced)\n",
    "\n",
    "# -----------------------------------------\n",
    "# ROC CURVES (TEST SET)\n",
    "# -----------------------------------------\n",
    "show_roc_curves(models_reduced, X_test_reduced, y_test, split_name=\"Test\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12134a9a",
   "metadata": {},
   "source": [
    "### Split Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8430911",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map countries to continents (same logic as before)\n",
    "try:\n",
    "    import pycountry\n",
    "    import pycountry_convert as pc\n",
    "    \n",
    "    def country_to_continent(name):\n",
    "        try:\n",
    "            lookup_name = name.replace('_', ' ')\n",
    "            country = pycountry.countries.lookup(lookup_name)\n",
    "            alpha2 = country.alpha_2\n",
    "            cc = pc.country_alpha2_to_continent_code(alpha2)\n",
    "            continent_map = {\n",
    "                'AF': 'Africa',\n",
    "                'AS': 'Asia',\n",
    "                'EU': 'Europe',\n",
    "                'NA': 'North_America',\n",
    "                'OC': 'Oceania',\n",
    "                'SA': 'South_America'\n",
    "            }\n",
    "            return continent_map.get(cc, 'Unknown')\n",
    "        except Exception:\n",
    "            return 'Unknown'\n",
    "except ImportError:\n",
    "    # Fallback mapping for common countries (extend as needed)\n",
    "    fallback = {\n",
    "    # --- North America ---\n",
    "    'United_States': 'North_America',\n",
    "    'Canada': 'North_America',\n",
    "    'Mexico': 'North_America',\n",
    "    'Costa_Rica': 'North_America',\n",
    "    'Dominican_Republic': 'North_America',\n",
    "    'Panama': 'North_America',\n",
    "    'Trinidad_and_Tobago': 'North_America',\n",
    "    'Barbados': 'North_America',\n",
    "    'St._Kitts_and_Nevis': 'North_America',\n",
    "    'St._Lucia': 'North_America',\n",
    "    'St._Vincent_and_the_Grenadines': 'North_America',\n",
    "    'Grenada': 'North_America',\n",
    "    'Antigua_and_Barbuda': 'North_America',\n",
    "    'The_Bahamas': 'North_America',\n",
    "    'Belize': 'North_America',\n",
    "    'Haiti': 'North_America',\n",
    "    'Jamaica': 'North_America',\n",
    "    'Puerto_Rico': 'North_America',\n",
    "    'Aruba': 'North_America',\n",
    "    'Dominica': 'North_America',\n",
    "    'Nicaragua': 'North_America',\n",
    "    'Guatemala': 'North_America',\n",
    "    'Honduras': 'North_America',\n",
    "    'El_Salvador': 'North_America',\n",
    "\n",
    "    # --- South America ---\n",
    "    'Brazil': 'South_America',\n",
    "    'Argentina': 'South_America',\n",
    "    'Chile': 'South_America',\n",
    "    'Colombia': 'South_America',\n",
    "    'Peru': 'South_America',\n",
    "    'Venezuela': 'South_America',\n",
    "    'Bolivia': 'South_America',\n",
    "    'Paraguay': 'South_America',\n",
    "    'Uruguay': 'South_America',\n",
    "    'Ecuador': 'South_America',\n",
    "    'Guyana': 'South_America',\n",
    "    'Suriname': 'South_America',\n",
    "\n",
    "    # --- Europe ---\n",
    "    'Germany': 'Europe',\n",
    "    'France': 'Europe',\n",
    "    'United_Kingdom': 'Europe',\n",
    "    'Italy': 'Europe',\n",
    "    'Spain': 'Europe',\n",
    "    'Russia': 'Europe',\n",
    "    'Turkey': 'Europe',\n",
    "    'Türkiye': 'Europe',  # Modern spelling of Turkey\n",
    "    'Poland': 'Europe',\n",
    "    'Albania': 'Europe',\n",
    "    'Austria': 'Europe',\n",
    "    'Belgium': 'Europe',\n",
    "    'Bosnia_and_Herzegovina': 'Europe',\n",
    "    'Bulgaria': 'Europe',\n",
    "    'Croatia': 'Europe',\n",
    "    'Cyprus': 'Europe',\n",
    "    'Czech_Republic': 'Europe',\n",
    "    'Denmark': 'Europe',\n",
    "    'Estonia': 'Europe',\n",
    "    'Finland': 'Europe',\n",
    "    'Hungary': 'Europe',\n",
    "    'Iceland': 'Europe',\n",
    "    'Ireland': 'Europe',\n",
    "    'Latvia': 'Europe',\n",
    "    'Lithuania': 'Europe',\n",
    "    'Luxembourg': 'Europe',\n",
    "    'Malta': 'Europe',\n",
    "    'Netherlands': 'Europe',\n",
    "    'North_Macedonia': 'Europe',\n",
    "    'Norway': 'Europe',\n",
    "    'Portugal': 'Europe',\n",
    "    'Romania': 'Europe',\n",
    "    'Serbia': 'Europe',\n",
    "    'Slovak_Republic': 'Europe',\n",
    "    'Slovenia': 'Europe',\n",
    "    'Sweden': 'Europe',\n",
    "    'Switzerland': 'Europe',\n",
    "    'Greece': 'Europe',\n",
    "    'Kosovo': 'Europe',\n",
    "    'Andorra': 'Europe',\n",
    "    'San_Marino': 'Europe',\n",
    "    'Monaco': 'Europe' if 'Monaco' in locals() else None,\n",
    "    'Belarus': 'Europe',\n",
    "    'Moldova': 'Europe',\n",
    "    'Montenegro': 'Europe',\n",
    "    'Ukraine': 'Europe',\n",
    "\n",
    "    # --- Asia ---\n",
    "    'China': 'Asia',\n",
    "    'India': 'Asia',\n",
    "    'Japan': 'Asia',\n",
    "    'Afghanistan': 'Asia',\n",
    "    'Korea': 'Asia',\n",
    "    'Indonesia': 'Asia',\n",
    "    'Thailand': 'Asia',\n",
    "    'Vietnam': 'Asia',\n",
    "    'Islamic_Republic_of_Iran': 'Asia',\n",
    "    'Israel': 'Asia',\n",
    "    'Jordan': 'Asia',\n",
    "    'Kazakhstan': 'Asia',\n",
    "    'Lebanon': 'Asia',\n",
    "    'Pakistan': 'Asia',\n",
    "    'Saudi_Arabia': 'Asia',\n",
    "    'Syria': 'Asia',\n",
    "    'Taiwan_Province_of_China': 'Asia',\n",
    "    'Oman': 'Asia',\n",
    "    'Yemen': 'Asia',\n",
    "    'United_Arab_Emirates': 'Asia',\n",
    "    'Armenia': 'Asia',\n",
    "    'Azerbaijan': 'Asia',\n",
    "    'Georgia': 'Asia',\n",
    "    'Qatar': 'Asia',\n",
    "    'Bahrain': 'Asia',\n",
    "    'Kuwait': 'Asia',\n",
    "    'Nepal': 'Asia',\n",
    "    'Sri_Lanka': 'Asia',\n",
    "    'Bangladesh': 'Asia',\n",
    "    'Cambodia': 'Asia',\n",
    "    'Myanmar': 'Asia',\n",
    "    'Lao_P.D.R.': 'Asia',\n",
    "    'Bhutan': 'Asia',\n",
    "    'Mongolia': 'Asia',\n",
    "    'Malaysia': 'Asia',\n",
    "    'Philippines': 'Asia',\n",
    "    'Singapore': 'Asia',\n",
    "    'Timor_Leste': 'Asia',\n",
    "    'Brunei_Darussalam': 'Asia',\n",
    "    'Hong_Kong_SAR': 'Asia',\n",
    "    'Macao_SAR': 'Asia',\n",
    "    'West_Bank_and_Gaza': 'Asia',  # Palestinian territories\n",
    "    'Iraq': 'Asia',\n",
    "    'Kyrgyz_Republic': 'Asia',\n",
    "    'Tajikistan': 'Asia',\n",
    "    'Turkmenistan': 'Asia',\n",
    "    'Uzbekistan': 'Asia',\n",
    "    'Maldives': 'Asia',\n",
    "\n",
    "    # --- Africa ---\n",
    "    'South_Africa': 'Africa',\n",
    "    'Nigeria': 'Africa',\n",
    "    'Egypt': 'Africa',\n",
    "    'Zimbabwe': 'Africa',\n",
    "    'Kenya': 'Africa',\n",
    "    'Ethiopia': 'Africa',\n",
    "    'Morocco': 'Africa',\n",
    "    'Algeria': 'Africa',\n",
    "    'Cabo_Verde': 'Africa',\n",
    "    'Seychelles': 'Africa',\n",
    "    'Botswana': 'Africa',\n",
    "    'Cameroon': 'Africa',\n",
    "    'Djibouti': 'Africa',\n",
    "    'Equatorial_Guinea': 'Africa',\n",
    "    'Eswatini': 'Africa',\n",
    "    'Lesotho': 'Africa',\n",
    "    'Mali': 'Africa',\n",
    "    'Mauritania': 'Africa',\n",
    "    'Namibia': 'Africa',\n",
    "    'Niger': 'Africa',\n",
    "    'Zambia': 'Africa',\n",
    "    'Ghana': 'Africa',\n",
    "    'Uganda': 'Africa',\n",
    "    'Tanzania': 'Africa',\n",
    "    'Rwanda': 'Africa',\n",
    "    'Burundi': 'Africa',\n",
    "    'Somalia': 'Africa',\n",
    "    'Sudan': 'Africa',\n",
    "    'South_Sudan': 'Africa',\n",
    "    'Chad': 'Africa',\n",
    "    'Benin': 'Africa',\n",
    "    'Liberia': 'Africa',\n",
    "    'Guinea': 'Africa',\n",
    "    'Guinea_Bissau': 'Africa',\n",
    "    'Sierra_Leone': 'Africa',\n",
    "    'Togo': 'Africa',\n",
    "    'Angola': 'Africa',\n",
    "    'Mozambique': 'Africa',\n",
    "    'Madagascar': 'Africa',\n",
    "    'Comoros': 'Africa',\n",
    "    'São_Tomé_and_Príncipe': 'Africa',\n",
    "    'Republic_of_Congo': 'Africa',\n",
    "    'Democratic_Republic_of_the_Congo': 'Africa',\n",
    "    'Eritrea': 'Africa',\n",
    "    'Libya': 'Africa',\n",
    "    'Tunisia': 'Africa',\n",
    "    'Gabon': 'Africa',\n",
    "    'Central_African_Republic': 'Africa',\n",
    "    'The_Gambia': 'Africa',\n",
    "    'Senegal': 'Africa',\n",
    "    'Mauritius': 'Africa',\n",
    "    'Sao_Tome_and_Principe': 'Africa',\n",
    "    'Côte_dIvoire': 'Africa',  # Ivory Coast\n",
    "    'Burkina_Faso': 'Africa',\n",
    "    'Malawi': 'Africa',\n",
    "    'Congo_Republic': 'Africa',\n",
    "\n",
    "    # --- Oceania ---\n",
    "    'Australia': 'Oceania',\n",
    "    'New_Zealand': 'Oceania',\n",
    "    'Fiji': 'Oceania',\n",
    "    'Tonga': 'Oceania',\n",
    "    'Samoa': 'Oceania',\n",
    "    'Vanuatu': 'Oceania',\n",
    "    'Papua_New_Guinea': 'Oceania',\n",
    "    'Solomon_Islands': 'Oceania',\n",
    "    'Kiribati': 'Oceania',\n",
    "    'Marshall_Islands': 'Oceania',\n",
    "    'Micronesia': 'Oceania',\n",
    "    'Nauru': 'Oceania',\n",
    "    'Tuvalu': 'Oceania',\n",
    "    'Palau': 'Oceania'\n",
    "}\n",
    "\n",
    "    def country_to_continent(name):\n",
    "        return fallback.get(name.replace(' ', '_'), 'Unknown')\n",
    "\n",
    "# --- Add Continent column ---\n",
    "df_filtered_copy = df_pivot.copy()\n",
    "df_filtered_copy['Continent'] = df_filtered_copy['Country'].astype(str).apply(country_to_continent)\n",
    "\n",
    "# --- Map continents to economy groups ---\n",
    "continent_to_economy = {\n",
    "    'Europe': 'Upper_Economies',\n",
    "    'North_America': 'Upper_Economies',\n",
    "    'Oceania': 'Upper_Economies',\n",
    "    'Africa': 'Lower_Economies',\n",
    "    'Asia': 'Lower_Economies',\n",
    "    'South_America': 'Lower_Economies'\n",
    "}\n",
    "\n",
    "df_filtered_copy['EconomyGroup'] = df_filtered_copy['Continent'].map(continent_to_economy)\n",
    "\n",
    "# --- Create Lower and Upper economy DataFrames ---\n",
    "df_Lower_Economies = df_filtered_copy[df_filtered_copy['EconomyGroup'] == 'Lower_Economies'].drop(columns=['Continent','EconomyGroup'])\n",
    "df_Upper_Economies = df_filtered_copy[df_filtered_copy['EconomyGroup'] == 'Upper_Economies'].drop(columns=['Continent','EconomyGroup'])\n",
    "\n",
    "# --- Print summary ---\n",
    "print(\"=\" * 80)\n",
    "print(\" \" * 25 + \"ECONOMY GROUP SUMMARY\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(f\"\\nTotal DataFrames created:\")\n",
    "print(f\"  • Lower Economies: {len(df_Lower_Economies)} rows\")\n",
    "print(f\"  • Upper Economies: {len(df_Upper_Economies)} rows\")\n",
    "print(f\"  • Total: {len(df_filtered_copy)} rows\")\n",
    "\n",
    "# --- Show RecessionRisk distribution per economy group ---\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"RECESSION RISK DISTRIBUTION BY ECONOMY GROUP\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(\"\\n📊 LOWER ECONOMIES (Africa, Asia, South America):\")\n",
    "print(\"-\" * 80)\n",
    "lower_risk_dist = df_Lower_Economies['RecessionRisk'].value_counts().sort_index()\n",
    "print(lower_risk_dist)\n",
    "print(f\"\\nTotal Lower Economy samples: {len(df_Lower_Economies)}\")\n",
    "\n",
    "print(\"\\nDetailed breakdown:\")\n",
    "for risk_class in range(4):\n",
    "    count = lower_risk_dist.get(risk_class, 0)\n",
    "    pct = (count / len(df_Lower_Economies)) * 100 if len(df_Lower_Economies) > 0 else 0\n",
    "    risk_label = ['No Risk', 'Mild', 'Moderate', 'High'][risk_class]\n",
    "    print(f\"  Class {risk_class} ({risk_label:>8}): {count:>5} samples ({pct:>6.2f}%)\")\n",
    "\n",
    "print(\"\\n\" + \"-\" * 80)\n",
    "print(\"\\n📊 UPPER ECONOMIES (Europe, North America, Oceania):\")\n",
    "print(\"-\" * 80)\n",
    "upper_risk_dist = df_Upper_Economies['RecessionRisk'].value_counts().sort_index()\n",
    "print(upper_risk_dist)\n",
    "print(f\"\\nTotal Upper Economy samples: {len(df_Upper_Economies)}\")\n",
    "\n",
    "print(\"\\nDetailed breakdown:\")\n",
    "for risk_class in range(4):\n",
    "    count = upper_risk_dist.get(risk_class, 0)\n",
    "    pct = (count / len(df_Upper_Economies)) * 100 if len(df_Upper_Economies) > 0 else 0\n",
    "    risk_label = ['No Risk', 'Mild', 'Moderate', 'High'][risk_class]\n",
    "    print(f\"  Class {risk_class} ({risk_label:>8}): {count:>5} samples ({pct:>6.2f}%)\")\n",
    "\n",
    "# --- Comparative Analysis ---\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"COMPARATIVE ANALYSIS: LOWER vs UPPER ECONOMIES\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(f\"\\n{'Risk Class':<15} {'Lower Econ':<20} {'Upper Econ':<20} {'Difference':<15}\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "for risk_class in range(4):\n",
    "    lower_count = lower_risk_dist.get(risk_class, 0)\n",
    "    upper_count = upper_risk_dist.get(risk_class, 0)\n",
    "    \n",
    "    lower_pct = (lower_count / len(df_Lower_Economies)) * 100 if len(df_Lower_Economies) > 0 else 0\n",
    "    upper_pct = (upper_count / len(df_Upper_Economies)) * 100 if len(df_Upper_Economies) > 0 else 0\n",
    "    \n",
    "    diff = lower_pct - upper_pct\n",
    "    risk_label = ['No Risk', 'Mild', 'Moderate', 'High'][risk_class]\n",
    "    \n",
    "    print(f\"{risk_label:<15} {lower_count:>5} ({lower_pct:>5.1f}%)     {upper_count:>5} ({upper_pct:>5.1f}%)     {diff:>+6.1f}%\")\n",
    "\n",
    "# --- Visualization ---\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"VISUALIZATION: Recession Risk by Economy Group\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Lower Economies\n",
    "lower_counts = [lower_risk_dist.get(i, 0) for i in range(4)]\n",
    "axes[0].bar(range(4), lower_counts, color=['green', 'yellow', 'orange', 'red'], \n",
    "            edgecolor='black', linewidth=1.5)\n",
    "axes[0].set_xlabel('RecessionRisk Class', fontsize=12, fontweight='bold')\n",
    "axes[0].set_ylabel('Count', fontsize=12, fontweight='bold')\n",
    "axes[0].set_title('Lower Economies\\n(Africa, Asia, South America)', fontsize=14, fontweight='bold')\n",
    "axes[0].set_xticks(range(4))\n",
    "axes[0].set_xticklabels(['No Risk', 'Mild', 'Moderate', 'High'])\n",
    "axes[0].grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Add count labels\n",
    "for i, count in enumerate(lower_counts):\n",
    "    if count > 0:\n",
    "        axes[0].text(i, count + max(lower_counts)*0.02, str(count), \n",
    "                    ha='center', va='bottom', fontsize=11, fontweight='bold')\n",
    "\n",
    "# Upper Economies\n",
    "upper_counts = [upper_risk_dist.get(i, 0) for i in range(4)]\n",
    "axes[1].bar(range(4), upper_counts, color=['green', 'yellow', 'orange', 'red'], \n",
    "            edgecolor='black', linewidth=1.5)\n",
    "axes[1].set_xlabel('RecessionRisk Class', fontsize=12, fontweight='bold')\n",
    "axes[1].set_ylabel('Count', fontsize=12, fontweight='bold')\n",
    "axes[1].set_title('Upper Economies\\n(Europe, North America, Oceania)', fontsize=14, fontweight='bold')\n",
    "axes[1].set_xticks(range(4))\n",
    "axes[1].set_xticklabels(['No Risk', 'Mild', 'Moderate', 'High'])\n",
    "axes[1].grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Add count labels\n",
    "for i, count in enumerate(upper_counts):\n",
    "    if count > 0:\n",
    "        axes[1].text(i, count + max(upper_counts)*0.02, str(count), \n",
    "                    ha='center', va='bottom', fontsize=11, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# --- Countries per economy group ---\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"COUNTRIES BY ECONOMY GROUP\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(\"\\nLower Economies Countries:\")\n",
    "lower_countries = sorted(df_Lower_Economies['Country'].unique())\n",
    "print(f\"  Total: {len(lower_countries)} countries\")\n",
    "print(f\"  {', '.join(lower_countries[:10])}...\")  # Show first 10\n",
    "\n",
    "print(\"\\nUpper Economies Countries:\")\n",
    "upper_countries = sorted(df_Upper_Economies['Country'].unique())\n",
    "print(f\"  Total: {len(upper_countries)} countries\")\n",
    "print(f\"  {', '.join(upper_countries[:10])}...\")  # Show first 10\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9989b926",
   "metadata": {},
   "source": [
    "# 6. Economy-Specific Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afbeadb0",
   "metadata": {},
   "source": [
    "## Upper Economies - Full Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19751b1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data\n",
    "X = df_Upper_Economies.drop(columns=[\"RecessionRisk\", \"Country\"])\n",
    "y = df_Upper_Economies[\"RecessionRisk\"]\n",
    "\n",
    "# -----------------------------\n",
    "# TIME-BASED SPLIT (Train / Val / Test)\n",
    "# -----------------------------\n",
    "TRAIN_END = 2018     # Train: < 2018 (up to 2017)\n",
    "VAL_END   = 2021     # Validation: 2018-2020\n",
    "                     # Test: >= 2021 (2021-2025)\n",
    "\n",
    "train_mask = X.index < TRAIN_END\n",
    "val_mask   = (X.index >= TRAIN_END) & (X.index < VAL_END)\n",
    "test_mask  = X.index >= VAL_END\n",
    "\n",
    "X_train = X[train_mask]\n",
    "y_train = y[train_mask]\n",
    "\n",
    "X_val = X[val_mask]\n",
    "y_val = y[val_mask]\n",
    "\n",
    "X_test = X[test_mask]\n",
    "y_test = y[test_mask]\n",
    "\n",
    "print(f\"Train: {len(X_train)} samples (years < {TRAIN_END} [up to 2017])\")\n",
    "print(f\"Validation: {len(X_val)} samples ({TRAIN_END}-{VAL_END-1} [2018-2020])\")\n",
    "print(f\"Test: {len(X_test)} samples (years >= {VAL_END} [2021-2025])\")\n",
    "models_upper, summary_df_upper, confusion_mats_upper = train_all_models(\n",
    "    X_train, y_train,\n",
    "    X_val,   y_val,\n",
    "    X_test,  y_test\n",
    ")\n",
    "print(summary_df_upper)\n",
    "plot_feature_importance(\n",
    "    models_upper,\n",
    "    X_train.columns.tolist(),\n",
    "    title_prefix=\"Upper Economies - \"\n",
    ")\n",
    "show_confusion_matrices(confusion_mats_upper, summary_df_upper)\n",
    "show_roc_curves(models_upper, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd4b3137",
   "metadata": {},
   "source": [
    "## Lower Economies - Full Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24ecfe97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------\n",
    "# PREPARE DATA FOR LOWER ECONOMIES\n",
    "# -----------------------------------------\n",
    "X = df_Lower_Economies.drop(columns=[\"RecessionRisk\", \"Country\"])\n",
    "y = df_Lower_Economies[\"RecessionRisk\"]\n",
    "\n",
    "# -----------------------------------------\n",
    "# YEAR-BASED SPLIT (same logic as Upper Economies)\n",
    "# -----------------------------------------\n",
    "TRAIN_END = 2018     # Train: < 2018 (up to 2017)\n",
    "VAL_END   = 2021     # Validation: 2018-2020\n",
    "                     # Test: >= 2021 (2021-2025)\n",
    "\n",
    "train_mask = X.index < TRAIN_END\n",
    "val_mask   = (X.index >= TRAIN_END) & (X.index < VAL_END)\n",
    "test_mask  = X.index >= VAL_END\n",
    "\n",
    "X_train = X[train_mask]\n",
    "y_train = y[train_mask]\n",
    "\n",
    "X_val = X[val_mask]\n",
    "y_val = y[val_mask]\n",
    "\n",
    "X_test = X[test_mask]\n",
    "y_test = y[test_mask]\n",
    "print(f\"Train: {len(X_train)} samples (years < {TRAIN_END} [up to 2017])\")\n",
    "print(f\"Validation: {len(X_val)} samples ({TRAIN_END}-{VAL_END-1} [2018-2020])\")\n",
    "print(f\"Test: {len(X_test)} samples (years >= {VAL_END} [2021-2025])\")\n",
    "print(f\"Test: {len(X_test)} samples (years >= {VAL_END})\")\n",
    "\n",
    "# -----------------------------------------\n",
    "# TRAIN MODELS (NOW WITH VALIDATION)\n",
    "# -----------------------------------------\n",
    "models_lower, summary_df_lower, confusion_mats_lower = train_all_models(\n",
    "    X_train, y_train,\n",
    "    X_val,   y_val,\n",
    "    X_test,  y_test\n",
    ")\n",
    "\n",
    "# -----------------------------------------\n",
    "# METRICS TABLE\n",
    "# -----------------------------------------\n",
    "print(summary_df_lower)\n",
    "\n",
    "# -----------------------------------------\n",
    "# FEATURE IMPORTANCE\n",
    "# -----------------------------------------\n",
    "plot_feature_importance(\n",
    "    models_lower,\n",
    "    X_train.columns.tolist(),\n",
    "    title_prefix=\"Lower Economies - \"\n",
    ")\n",
    "\n",
    "# -----------------------------------------\n",
    "# COMBINED VALIDATION + TEST CONFUSION MATRICES\n",
    "# -----------------------------------------\n",
    "show_confusion_matrices(confusion_mats_lower, summary_df_lower)\n",
    "\n",
    "# -----------------------------------------\n",
    "# ROC CURVES (TEST SET)\n",
    "# -----------------------------------------\n",
    "show_roc_curves(models_lower, X_test, y_test, split_name=\"Test\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cfc2538",
   "metadata": {},
   "source": [
    "## Upper Economies - Reduced Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a25f6b34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------\n",
    "# PREPARE DATA (REDUCED FEATURES)\n",
    "# -----------------------------------------\n",
    "X_upper = df_Upper_Economies[selected_features]\n",
    "y_upper = df_Upper_Economies[\"RecessionRisk\"]\n",
    "\n",
    "# -----------------------------------------\n",
    "# YEAR-BASED SPLIT (same logic everywhere)\n",
    "# -----------------------------------------\n",
    "TRAIN_END = 2018     # Train: < 2018 (up to 2017)\n",
    "VAL_END   = 2021     # Validation: 2018-2020\n",
    "                     # Test: >= 2021 (2021-2025)\n",
    "\n",
    "train_mask = X_upper.index < TRAIN_END\n",
    "val_mask   = (X_upper.index >= TRAIN_END) & (X_upper.index < VAL_END)\n",
    "test_mask  = X_upper.index >= VAL_END\n",
    "\n",
    "X_train_upper = X_upper[train_mask]\n",
    "y_train_upper = y_upper[train_mask]\n",
    "\n",
    "X_val_upper = X_upper[val_mask]\n",
    "y_val_upper = y_upper[val_mask]\n",
    "\n",
    "X_test_upper = X_upper[test_mask]\n",
    "y_test_upper = y_upper[test_mask]\n",
    "print(f\"Train: {len(X_train_upper)} samples (years < {TRAIN_END} [up to 2017])\")\n",
    "print(f\"Validation: {len(X_val_upper)} samples ({TRAIN_END}-{VAL_END-1} [2018-2020])\")\n",
    "print(f\"Test: {len(X_test_upper)} samples (years >= {VAL_END} [2021-2025])\")\n",
    "print(f\"Test: {len(X_test_upper)} samples (years >= {VAL_END})\")\n",
    "\n",
    "# -----------------------------------------\n",
    "# TRAIN MODELS (REDUCED FEATURES)\n",
    "# -----------------------------------------\n",
    "models_upper_reduced, summary_df_upper_reduced, confusion_mats_upper_reduced = train_all_models(\n",
    "    X_train_upper, y_train_upper,\n",
    "    X_val_upper,   y_val_upper,\n",
    "    X_test_upper,  y_test_upper\n",
    ")\n",
    "\n",
    "# -----------------------------------------\n",
    "# METRICS TABLE\n",
    "# -----------------------------------------\n",
    "print(\"Upper Economies Accuracy (Reduced Features):\")\n",
    "print(summary_df_upper_reduced)\n",
    "\n",
    "# -----------------------------------------\n",
    "# FEATURE IMPORTANCE\n",
    "# -----------------------------------------\n",
    "plot_feature_importance(\n",
    "    models_upper_reduced,\n",
    "    feature_names=selected_features,\n",
    "    title_prefix=\"Upper Economies - Reduced Features - \"\n",
    ")\n",
    "\n",
    "# -----------------------------------------\n",
    "# COMBINED VALIDATION + TEST CONFUSION MATRICES\n",
    "# -----------------------------------------\n",
    "show_confusion_matrices(confusion_mats_upper_reduced, summary_df_upper_reduced)\n",
    "\n",
    "# -----------------------------------------\n",
    "# ROC CURVES (TEST SET)\n",
    "# -----------------------------------------\n",
    "show_roc_curves(models_upper_reduced, X_test_upper, y_test_upper, split_name=\"Test\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57f45978",
   "metadata": {},
   "source": [
    "## Lower Economies - Reduced Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff38534b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------\n",
    "# PREPARE DATA (REDUCED FEATURES)\n",
    "# -----------------------------------------\n",
    "X_lower = df_Lower_Economies[selected_features]\n",
    "y_lower = df_Lower_Economies[\"RecessionRisk\"]\n",
    "\n",
    "# -----------------------------------------\n",
    "# YEAR-BASED SPLIT (same logic everywhere)\n",
    "# -----------------------------------------\n",
    "TRAIN_END = 2018     # Train: < 2018 (up to 2017)\n",
    "VAL_END   = 2021     # Validation: 2018-2020\n",
    "                     # Test: >= 2021 (2021-2025)\n",
    "\n",
    "train_mask = X_lower.index < TRAIN_END\n",
    "val_mask   = (X_lower.index >= TRAIN_END) & (X_lower.index < VAL_END)\n",
    "test_mask  = X_lower.index >= VAL_END\n",
    "\n",
    "X_train_lower = X_lower[train_mask]\n",
    "y_train_lower = y_lower[train_mask]\n",
    "\n",
    "X_val_lower = X_lower[val_mask]\n",
    "y_val_lower = y_lower[val_mask]\n",
    "\n",
    "X_test_lower = X_lower[test_mask]\n",
    "y_test_lower = y_lower[test_mask]\n",
    "print(f\"Train: {len(X_train_lower)} samples (years < {TRAIN_END} [up to 2017])\")\n",
    "print(f\"Validation: {len(X_val_lower)} samples ({TRAIN_END}-{VAL_END-1} [2018-2020])\")\n",
    "print(f\"Test: {len(X_test_lower)} samples (years >= {VAL_END} [2021-2025])\")\n",
    "print(f\"Test: {len(X_test_lower)} samples (years >= {VAL_END})\")\n",
    "\n",
    "# -----------------------------------------\n",
    "# TRAIN MODELS (REDUCED FEATURES)\n",
    "# -----------------------------------------\n",
    "models_lower_reduced, summary_df_lower_reduced, confusion_mats_lower_reduced = train_all_models(\n",
    "    X_train_lower, y_train_lower,\n",
    "    X_val_lower,   y_val_lower,\n",
    "    X_test_lower,  y_test_lower\n",
    ")\n",
    "\n",
    "# -----------------------------------------\n",
    "# METRICS TABLE\n",
    "# -----------------------------------------\n",
    "print(\"Lower Economies Accuracy (Reduced Features):\")\n",
    "print(summary_df_lower_reduced)\n",
    "\n",
    "# -----------------------------------------\n",
    "# FEATURE IMPORTANCE\n",
    "# -----------------------------------------\n",
    "plot_feature_importance(\n",
    "    models_lower_reduced,\n",
    "    feature_names=selected_features,\n",
    "    title_prefix=\"Lower Economies - Reduced Features - \"\n",
    ")\n",
    "\n",
    "# -----------------------------------------\n",
    "# COMBINED VALIDATION + TEST CONFUSION MATRICES\n",
    "# -----------------------------------------\n",
    "show_confusion_matrices(confusion_mats_lower_reduced, summary_df_lower_reduced)\n",
    "\n",
    "# -----------------------------------------\n",
    "# ROC CURVES (TEST SET)\n",
    "# -----------------------------------------\n",
    "show_roc_curves(models_lower_reduced, X_test_lower, y_test_lower, split_name=\"Test\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9ca2ee4",
   "metadata": {},
   "source": [
    "# LTSM Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92ce9106",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "# Core layers\n",
    "from tensorflow.keras.layers import (\n",
    "    Input,\n",
    "    Dense,\n",
    "    LSTM,\n",
    "    GRU,\n",
    "    Dropout,\n",
    "    BatchNormalization,\n",
    "    LayerNormalization,\n",
    "    Bidirectional\n",
    ")\n",
    "\n",
    "# Advanced sequence layers\n",
    "from tensorflow.keras.layers import (\n",
    "    Conv1D,\n",
    "    GlobalAveragePooling1D,\n",
    "    GlobalMaxPooling1D,\n",
    "    MultiHeadAttention\n",
    ")\n",
    "\n",
    "# Model utilities\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "\n",
    "# Callbacks\n",
    "from tensorflow.keras.callbacks import (\n",
    "    EarlyStopping,\n",
    "    ReduceLROnPlateau,\n",
    "    ModelCheckpoint\n",
    ")\n",
    "\n",
    "# Optimizers\n",
    "from tensorflow.keras.optimizers import Adam, RMSprop\n",
    "\n",
    "# Losses & metrics\n",
    "from tensorflow.keras.losses import SparseCategoricalCrossentropy\n",
    "from tensorflow.keras.metrics import Accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32da196f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_until_2025 = df_pivot[df_pivot.index <= 2025]\n",
    "df_after_2025 = df_pivot[df_pivot.index > 2025]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "478dda10",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_until_2025"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d2a6b7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_after_2025"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "281dd3b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reshape_for_lstm(df, window=3, min_sequences=1, return_feature_names=False):\n",
    "    \"\"\"\n",
    "    Convert df_pivot (Year index, Country column) into LSTM-ready sequences.\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame with Year index and Country column\n",
    "        window: Number of historical years to use as input\n",
    "        min_sequences: Minimum sequences required per country (default 1)\n",
    "        return_feature_names: If True, return feature column names\n",
    "        \n",
    "    Returns:\n",
    "        X_seq: (num_sequences, window, num_features)\n",
    "        y_seq: (num_sequences,)\n",
    "        countries_seq: country for each sequence\n",
    "        years_seq: ending year for each sequence\n",
    "        feature_names: (optional) list of feature column names\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    \n",
    "    # Handle index - reset if needed for sorting\n",
    "    if isinstance(df.index, pd.RangeIndex):\n",
    "        raise ValueError(\"DataFrame must have Year as index\")\n",
    "    \n",
    "    # Reset index to make sorting easier, then sort\n",
    "    df_sorted = df.reset_index().sort_values([\"Country\", df.index.name])\n",
    "    year_col = df.index.name or \"Year\"\n",
    "    \n",
    "    # Define features (exclude target and identifiers)\n",
    "    feature_cols = [c for c in df_sorted.columns \n",
    "                   if c not in [\"RecessionRisk\", \"Country\", year_col]]\n",
    "    \n",
    "    X_seq = []\n",
    "    y_seq = []\n",
    "    countries_seq = []\n",
    "    years_seq = []\n",
    "    \n",
    "    # Group by country\n",
    "    for country, group in df_sorted.groupby(\"Country\"):\n",
    "        group = group.sort_values(year_col).reset_index(drop=True)\n",
    "        \n",
    "        # Check if country has enough data\n",
    "        if len(group) < window + 1:\n",
    "            print(f\"Warning: {country} has only {len(group)} years, need {window + 1}. Skipping.\")\n",
    "            continue\n",
    "        \n",
    "        X_values = group[feature_cols].values\n",
    "        y_values = group[\"RecessionRisk\"].values\n",
    "        years = group[year_col].values\n",
    "        \n",
    "        # Sliding window\n",
    "        num_sequences = len(group) - window\n",
    "        for i in range(num_sequences):\n",
    "            X_seq.append(X_values[i:i+window])\n",
    "            y_seq.append(y_values[i+window])\n",
    "            countries_seq.append(country)\n",
    "            years_seq.append(years[i+window])\n",
    "    \n",
    "    if len(X_seq) < min_sequences:\n",
    "        raise ValueError(f\"Only generated {len(X_seq)} sequences, need at least {min_sequences}\")\n",
    "    \n",
    "    results = (\n",
    "        np.array(X_seq),\n",
    "        np.array(y_seq),\n",
    "        np.array(countries_seq),\n",
    "        np.array(years_seq)\n",
    "    )\n",
    "    \n",
    "    if return_feature_names:\n",
    "        results = results + (feature_cols,)\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19fa586c",
   "metadata": {},
   "outputs": [],
   "source": [
    "WINDOW = 3\n",
    "\n",
    "X_seq, y_seq, countries_seq, years_seq = reshape_for_lstm(\n",
    "    df_until_2025,\n",
    "    window=WINDOW\n",
    ")\n",
    "\n",
    "print(\"X_seq shape:\", X_seq.shape)\n",
    "print(\"y_seq shape:\", y_seq.shape)\n",
    "print(\"Example sequence shape:\", X_seq[0].shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f1805a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# 1. First split: Train vs Test\n",
    "# -----------------------------\n",
    "SPLIT_YEAR = 2020\n",
    "\n",
    "train_mask = years_seq <= SPLIT_YEAR\n",
    "test_mask  = years_seq > SPLIT_YEAR\n",
    "\n",
    "X_train_full = X_seq[train_mask]\n",
    "y_train_full = y_seq[train_mask]\n",
    "years_train_full = years_seq[train_mask]\n",
    "\n",
    "X_test = X_seq[test_mask]\n",
    "y_test = y_seq[test_mask]\n",
    "years_test = years_seq[test_mask]\n",
    "\n",
    "# -----------------------------\n",
    "# 2. Second split: Train vs Validation\n",
    "# -----------------------------\n",
    "VAL_YEAR = 2017  # You can adjust this\n",
    "\n",
    "train_mask_2 = years_train_full <= VAL_YEAR\n",
    "val_mask_2   = (years_train_full > VAL_YEAR) & (years_train_full <= SPLIT_YEAR)\n",
    "\n",
    "X_train = X_train_full[train_mask_2]\n",
    "y_train = y_train_full[train_mask_2]\n",
    "\n",
    "X_val   = X_train_full[val_mask_2]\n",
    "y_val   = y_train_full[val_mask_2]\n",
    "\n",
    "# -----------------------------\n",
    "# 3. Print summary\n",
    "# -----------------------------\n",
    "print(f\"Train sequences: {len(X_train)} (years {years_train_full[train_mask_2].min()}–{years_train_full[train_mask_2].max()})\")\n",
    "print(f\"Validation sequences: {len(X_val)} (years {years_train_full[val_mask_2].min()}–{years_train_full[val_mask_2].max()})\")\n",
    "print(f\"Test sequences: {len(X_test)} (years {years_test.min()}–{years_test.max()})\")\n",
    "\n",
    "total = len(X_train) + len(X_val) + len(X_test)\n",
    "print(f\"Split: {len(X_train)/total*100:.1f}% train, {len(X_val)/total*100:.1f}% val, {len(X_test)/total*100:.1f}% test\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "348dd846",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# 1. SCALE FEATURES (minimal)\n",
    "# -----------------------------\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit only on TRAIN\n",
    "X_train_scaled = scaler.fit_transform(\n",
    "    X_train.reshape(-1, X_train.shape[-1])\n",
    ").reshape(X_train.shape)\n",
    "\n",
    "# Transform VAL + TEST\n",
    "X_val_scaled = scaler.transform(\n",
    "    X_val.reshape(-1, X_val.shape[-1])\n",
    ").reshape(X_val.shape)\n",
    "\n",
    "X_test_scaled = scaler.transform(\n",
    "    X_test.reshape(-1, X_test.shape[-1])\n",
    ").reshape(X_test.shape)\n",
    "\n",
    "num_classes = len(np.unique(y_seq))\n",
    "\n",
    "# -----------------------------\n",
    "# 2. LSTM MODEL\n",
    "# -----------------------------\n",
    "model = keras.Sequential([\n",
    "    keras.layers.Input(shape=(WINDOW, X_train.shape[2])),\n",
    "    keras.layers.LSTM(32),\n",
    "    keras.layers.Dense(num_classes, activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# -----------------------------\n",
    "# 3. TRAIN (now uses VALIDATION SET)\n",
    "# -----------------------------\n",
    "history = model.fit(\n",
    "    X_train_scaled, y_train,\n",
    "    validation_data=(X_val_scaled, y_val),\n",
    "    epochs=20,\n",
    "    batch_size=32,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# -----------------------------\n",
    "# 4. EVALUATE ON TEST SET\n",
    "# -----------------------------\n",
    "test_loss, test_acc = model.evaluate(X_test_scaled, y_test, verbose=0)\n",
    "print(f\"Test accuracy: {test_acc:.4f}\")\n",
    "\n",
    "# -----------------------------\n",
    "# 5. PREDICT ON TEST SET\n",
    "# -----------------------------\n",
    "y_pred = np.argmax(model.predict(X_test_scaled), axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b080a14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate predictions for all splits\n",
    "y_train_pred = np.argmax(model.predict(X_train_scaled), axis=1)\n",
    "y_val_pred   = np.argmax(model.predict(X_val_scaled), axis=1)\n",
    "y_test_pred  = y_pred\n",
    "\n",
    "# Quick summary\n",
    "print(f\"\\nAccuracy - Train: {accuracy_score(y_train, y_train_pred):.3f} | Val: {accuracy_score(y_val, y_val_pred):.3f} | Test: {accuracy_score(y_test, y_test_pred):.3f}\")\n",
    "print(f\"F1 (macro) - Train: {f1_score(y_train, y_train_pred, average='macro', zero_division=0):.3f} | Val: {f1_score(y_val, y_val_pred, average='macro', zero_division=0):.3f} | Test: {f1_score(y_test, y_test_pred, average='macro', zero_division=0):.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29e8c9cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# -----------------------------\n",
    "# CONFUSION MATRICES\n",
    "# -----------------------------\n",
    "cm_train = confusion_matrix(y_train, y_train_pred)\n",
    "cm_val   = confusion_matrix(y_val, y_val_pred)\n",
    "cm_test  = confusion_matrix(y_test, y_test_pred)\n",
    "\n",
    "plt.figure(figsize=(18, 5))\n",
    "\n",
    "# Train\n",
    "plt.subplot(1, 3, 1)\n",
    "sns.heatmap(cm_train, annot=True, fmt='d', cmap='Blues')\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"Actual\")\n",
    "plt.title(\"Train Confusion Matrix\")\n",
    "\n",
    "# Validation\n",
    "plt.subplot(1, 3, 2)\n",
    "sns.heatmap(cm_val, annot=True, fmt='d', cmap='Blues')\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"Actual\")\n",
    "plt.title(\"Validation Confusion Matrix\")\n",
    "\n",
    "# Test\n",
    "plt.subplot(1, 3, 3)\n",
    "sns.heatmap(cm_test, annot=True, fmt='d', cmap='Blues')\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"Actual\")\n",
    "plt.title(\"Test Confusion Matrix\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# -----------------------------\n",
    "# TRAINING CURVES\n",
    "# -----------------------------\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "# Loss\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['loss'], label='Train Loss')\n",
    "plt.plot(history.history['val_loss'], label='Val Loss')\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Loss Curve\")\n",
    "plt.legend()\n",
    "\n",
    "# Accuracy\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['accuracy'], label='Train Acc')\n",
    "plt.plot(history.history['val_accuracy'], label='Val Acc')\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.title(\"Accuracy Curve\")\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
