{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f072c436",
   "metadata": {},
   "source": [
    "# WEO Economic Data Analysis & Recession Prediction\n",
    "\n",
    "**Objective:** Load World Economic Outlook (WEO) data, clean and transform it, then use machine learning models to predict global recessions.\n",
    "\n",
    "**Workflow:**\n",
    "1. Data loading and cleaning\n",
    "2. Feature engineering and recession flagging\n",
    "3. Exploratory data analysis\n",
    "4. Model training with full and reduced feature sets (comparing 13 vs 5 features)\n",
    "5. Economy-specific analysis (Upper vs Lower economies with both feature sets)\n",
    "6. Future predictions for all scenarios\n",
    "\n",
    "**Models Used:** Logistic Regression, Random Forest, Gradient Boosting, Linear SVM, Decision Tree, and Ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c77a1c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#1)Multiclass classification target distribution - aanpassen -done\n",
    "#2)Threshhold meerdere waardes vullen en dat in Overleaf plaatsen ter discussie -fail\n",
    "#3)ROC, AUC laten zien -done\n",
    "#4) Zoek verder op de auteur naar vergelijkbaar werk -wip\n",
    "#5) Use Arima as baseline -wip\n",
    "#6) Try neural network -wip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0881101a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core data manipulation and visualization\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# File handling\n",
    "import csv\n",
    "from pathlib import Path\n",
    "\n",
    "# Machine learning - preprocessing\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Machine learning - models\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, VotingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Machine learning - metrics\n",
    "from sklearn.metrics import (\n",
    "    classification_report, \n",
    "    accuracy_score, \n",
    "    precision_score, \n",
    "    recall_score, \n",
    "    f1_score, \n",
    "    confusion_matrix,\n",
    "    ConfusionMatrixDisplay,\n",
    "    roc_curve,\n",
    "    auc\n",
    ")\n",
    "\n",
    "# Imbalanced learning\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.pipeline import Pipeline as ImbPipeline\n",
    "\n",
    "# TensorFlow/Keras\n",
    "from tensorflow.keras.callbacks import LearningRateScheduler\n",
    "\n",
    "# Optional pycountry for continent mapping\n",
    "try:\n",
    "    import pycountry\n",
    "    import pycountry_convert as pc\n",
    "    HAS_PYCOUNTRY = True\n",
    "except ImportError:\n",
    "    HAS_PYCOUNTRY = False\n",
    "\n",
    "pd.set_option(\"display.max_rows\", 25)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5987b26",
   "metadata": {},
   "source": [
    "# 1. Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e640272a",
   "metadata": {},
   "outputs": [],
   "source": [
    "p = Path(r\"data.csv\")\n",
    "if not p.exists():\n",
    "    raise FileNotFoundError(p)\n",
    "\n",
    "# Detect encoding and delimiter\n",
    "encoding = \"utf-8\"\n",
    "try:\n",
    "    sample = p.read_text(encoding=encoding)[:8192]\n",
    "except UnicodeDecodeError:\n",
    "    encoding = \"latin-1\"\n",
    "    sample = p.read_text(encoding=encoding)[:8192]\n",
    "\n",
    "try:\n",
    "    delim = csv.Sniffer().sniff(sample).delimiter\n",
    "except Exception:\n",
    "    delim = \",\"\n",
    "\n",
    "df = pd.read_csv(p, sep=delim, encoding=encoding, low_memory=False, parse_dates=True)\n",
    "print(\"Shape:\", df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f200ae17",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Number of columns: {df.shape[1]}\")\n",
    "print(f\"Number of rows: {df.shape[0]}\")\n",
    "print(\"\\nColumn names:\", df.columns.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5a530cd",
   "metadata": {},
   "source": [
    "# 2. Data Cleaning & Transformation\n",
    "\n",
    "## Filter to Selected Economic Indicators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be117543",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(columns=[\"WEO Country Code\", \"ISO\", \"Country/Series-specific Notes\", \"Subject Notes\", \n",
    "                 \"Units\", \"Scale\", \"Estimates Start After\", \"Subject Descriptor\"], inplace=True)\n",
    "\n",
    "codes = {\n",
    "    # Core growth & external\n",
    "    \"NGDP_RPCH\", \"NGDPRPC\", \"PCPIPCH\", \"TX_RPCH\", \"TM_RPCH\", \"BCA_NGDPD\",\n",
    "    # Fiscal & debt aggregates\n",
    "    \"GGR_NGDP\", \"GGX_NGDP\", \"GGXWDN_NGDP\", \"GGXWDG_NGDP\",\n",
    "    # Savings & investment\n",
    "    \"NGSD_NGDP\", \"NID_NGDP\",\n",
    "    # Prices\n",
    "    \"PCPI\", \"LUR\"\n",
    "}\n",
    "\n",
    "col = \"WEO Subject Code\"\n",
    "\n",
    "if col not in df.columns:\n",
    "    raise KeyError(f\"Column {col!r} not found in dataframe\")\n",
    "\n",
    "df = df[df[col].astype(str).str.strip().isin(codes)].copy()\n",
    "print(\"shape after filter:\", df.shape)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60010051",
   "metadata": {},
   "source": [
    "## Data Reshaping: Wide to Long to Wide"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab8289b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "year_cols = df.columns[2:]\n",
    "\n",
    "df[year_cols] = df[year_cols].replace({',': ''}, regex=True)\n",
    "df[year_cols] = df[year_cols].apply(pd.to_numeric, errors=\"coerce\")\n",
    "\n",
    "df[\"Country\"] = (\n",
    "    df[\"Country\"]\n",
    "    .str.replace(\" \", \"_\")\n",
    "    .str.replace(\"'\", \"\")\n",
    "    .str.replace(\"-\", \"_\")\n",
    ")\n",
    "\n",
    "df_long = df.melt(id_vars=[\"WEO Subject Code\", \"Country\"],\n",
    "                  var_name=\"Year\", value_name=\"Value\")\n",
    "\n",
    "df_long[\"Year\"] = df_long[\"Year\"].astype(str).str.strip()\n",
    "df_long = df_long[df_long[\"Year\"].str.fullmatch(r\"\\d{4}\")].copy()\n",
    "df_long[\"Year\"] = df_long[\"Year\"].astype(int)\n",
    "\n",
    "df_long[\"Value\"] = (\n",
    "    df_long[\"Value\"].astype(str)\n",
    "    .str.replace(\",\", \"\")\n",
    "    .replace({\"\": None, \"nan\": None})\n",
    "    .astype(float)\n",
    ")\n",
    "\n",
    "df_pivot = df_long.pivot_table(\n",
    "    index=[\"Country\", \"Year\"],\n",
    "    columns=\"WEO Subject Code\",\n",
    "    values=\"Value\",\n",
    "    aggfunc=\"first\"\n",
    ").reset_index()\n",
    "\n",
    "df_pivot.columns.name = None\n",
    "df_pivot = df_pivot.set_index(\"Year\")\n",
    "\n",
    "df_pivot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e2b1eba",
   "metadata": {},
   "source": [
    "# 3. Feature Engineering\n",
    "\n",
    "## Add Recession Target Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d940258",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Step 1: Ensure chronological order ---\n",
    "df_pivot = df_pivot.sort_index()\n",
    "\n",
    "# --- Step 2: Construct diagnostic flags (not stored in df) ---\n",
    "\n",
    "# 1. GDP-based recession (two consecutive annual declines)\n",
    "flag_gdp = (\n",
    "    (df_pivot.groupby(\"Country\")[\"NGDPRPC\"].transform(lambda x: x.pct_change() < 0)) &\n",
    "    (df_pivot.groupby(\"Country\")[\"NGDPRPC\"].transform(lambda x: x.pct_change().shift(-1) < 0))\n",
    ").astype(int)\n",
    "\n",
    "# 2. Investment collapse (sharp drop in investment)\n",
    "flag_invest = (\n",
    "    df_pivot.groupby(\"Country\")[\"NID_NGDP\"].transform(lambda x: x.diff() < -2)\n",
    ").astype(int)\n",
    "\n",
    "# 3. Savings decline (household/national savings falling)\n",
    "flag_savings = (\n",
    "    df_pivot.groupby(\"Country\")[\"NGSD_NGDP\"].transform(lambda x: x.diff() < -2)\n",
    ").astype(int)\n",
    "\n",
    "# 4. Trade shock (both exports and imports contracting)\n",
    "flag_trade = (\n",
    "    (df_pivot.groupby(\"Country\")[\"TX_RPCH\"].transform(lambda x: x < 0)) &\n",
    "    (df_pivot.groupby(\"Country\")[\"TM_RPCH\"].transform(lambda x: x < 0))\n",
    ").astype(int)\n",
    "\n",
    "# 5. Inflation shock (stagflation: high inflation + negative growth)\n",
    "flag_inflation = (\n",
    "    (df_pivot.groupby(\"Country\")[\"PCPIPCH\"].transform(lambda x: x > 10)) &\n",
    "    (df_pivot.groupby(\"Country\")[\"NGDPRPC\"].transform(lambda x: x.pct_change() < 0))\n",
    ").astype(int)\n",
    "\n",
    "# 6. Debt crisis (gross debt rising sharply above 90% of GDP)\n",
    "flag_debt = (\n",
    "    (df_pivot[\"GGXWDG_NGDP\"] > 90) &\n",
    "    (df_pivot.groupby(\"Country\")[\"GGXWDG_NGDP\"].transform(lambda x: x.diff() > 10))\n",
    ").astype(int)\n",
    "\n",
    "# 7. Fiscal crisis (large and growing deficit)\n",
    "flag_fiscal = (\n",
    "    (df_pivot.groupby(\"Country\")[\"GGR_NGDP\"].transform(lambda x: x) < \n",
    "     df_pivot.groupby(\"Country\")[\"GGX_NGDP\"].transform(lambda x: x) - 5) &  # Deficit > 5% GDP\n",
    "    (df_pivot.groupby(\"Country\")[\"GGX_NGDP\"].transform(lambda x: x.diff()) > 2)  # Rising spending\n",
    ").astype(int)\n",
    "\n",
    "# 8. Current account crisis (large deficit deteriorating)\n",
    "flag_current_account = (\n",
    "    (df_pivot[\"BCA_NGDPD\"] < -5) &  # Deficit > 5% of GDP\n",
    "    (df_pivot.groupby(\"Country\")[\"BCA_NGDPD\"].transform(lambda x: x.diff() < -2))  # Worsening\n",
    ").astype(int)\n",
    "\n",
    "# 9. Real GDP growth collapse (severe contraction)\n",
    "flag_growth_collapse = (\n",
    "    df_pivot.groupby(\"Country\")[\"NGDP_RPCH\"].transform(lambda x: x < -3)  # Growth < -3%\n",
    ").astype(int)\n",
    "\n",
    "# 10. Deflation risk (falling prices with economic weakness)\n",
    "flag_deflation = (\n",
    "    (df_pivot.groupby(\"Country\")[\"PCPIPCH\"].transform(lambda x: x < 0)) &  # Negative inflation\n",
    "    (df_pivot.groupby(\"Country\")[\"NGDP_RPCH\"].transform(lambda x: x < 1))  # Weak growth\n",
    ").astype(int)\n",
    "\n",
    "# 11. Credit crunch (investment falling while debt rising)\n",
    "flag_credit_crunch = (\n",
    "    (df_pivot.groupby(\"Country\")[\"NID_NGDP\"].transform(lambda x: x.diff() < -1)) &\n",
    "    (df_pivot.groupby(\"Country\")[\"GGXWDG_NGDP\"].transform(lambda x: x.diff() > 5))\n",
    ").astype(int)\n",
    "\n",
    "# 12. External shock (exports collapsing while imports stable/rising)\n",
    "flag_external_shock = (\n",
    "    (df_pivot.groupby(\"Country\")[\"TX_RPCH\"].transform(lambda x: x < -5)) &\n",
    "    (df_pivot.groupby(\"Country\")[\"TM_RPCH\"].transform(lambda x: x > -2))\n",
    ").astype(int)\n",
    "\n",
    "# --- Step 3: Combine signals into a single severity score ---\n",
    "local_signal_count = (\n",
    "    flag_gdp + \n",
    "    flag_invest + \n",
    "    flag_savings + \n",
    "    flag_trade + \n",
    "    flag_inflation +\n",
    "    flag_debt +\n",
    "    flag_fiscal +\n",
    "    flag_current_account +\n",
    "    flag_growth_collapse +\n",
    "    flag_deflation +\n",
    "    flag_credit_crunch +\n",
    "    flag_external_shock\n",
    ")\n",
    "\n",
    "# --- Step 4: Multiclass recession risk label ---\n",
    "def classify_risk(local_count):\n",
    "    \"\"\"\n",
    "    Same thresholds as original:\n",
    "    - 3+ signals: High risk\n",
    "    - 2 signals: Moderate risk  \n",
    "    - 1 signal: Mild risk\n",
    "    - 0 signals: No risk\n",
    "    \"\"\"\n",
    "    if local_count >= 3:\n",
    "        return 3  # High risk\n",
    "    if local_count == 2:\n",
    "        return 2  # Moderate risk\n",
    "    if local_count == 1:\n",
    "        return 1  # Mild risk\n",
    "    return 0      # No risk\n",
    "\n",
    "df_pivot[\"RecessionRisk\"] = [\n",
    "    classify_risk(l) for l in local_signal_count\n",
    "]\n",
    "\n",
    "# Store signal count\n",
    "df_pivot[\"SignalCount\"] = local_signal_count\n",
    "\n",
    "# --- Step 5: Fill NaN with mean instead of dropping ---\n",
    "# Fill numeric columns with their mean\n",
    "numeric_cols = df_pivot.select_dtypes(include=[np.number]).columns\n",
    "for col in numeric_cols:\n",
    "    df_pivot[col] = df_pivot[col].fillna(df_pivot[col].mean())\n",
    "\n",
    "# Fill non-numeric columns with forward fill or mode\n",
    "non_numeric_cols = df_pivot.select_dtypes(exclude=[np.number]).columns\n",
    "for col in non_numeric_cols:\n",
    "    if col == 'Country':\n",
    "        continue  # Don't fill Country column\n",
    "    df_pivot[col] = df_pivot[col].fillna(method='ffill').fillna(method='bfill')\n",
    "\n",
    "df_pivot = df_pivot.sort_index(ascending=True)\n",
    "\n",
    "# --- Step 6: Show distribution ---\n",
    "print(\"=\" * 70)\n",
    "print(\"RECESSION RISK CLASSIFICATION SUMMARY (12 Indicators)\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(\"\\nRecessionRisk class distribution:\")\n",
    "print(df_pivot[\"RecessionRisk\"].value_counts().sort_index())\n",
    "print(f\"\\nTotal samples: {len(df_pivot)}\")\n",
    "\n",
    "# Show average signals per risk class\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"AVERAGE SIGNALS TRIGGERED PER RISK CLASS:\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "signal_summary = df_pivot.groupby('RecessionRisk')['SignalCount'].agg(['mean', 'min', 'max', 'count'])\n",
    "print(signal_summary)\n",
    "\n",
    "# Show which signals are most common\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"INDIVIDUAL SIGNAL FREQUENCIES:\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Recalculate flags for cleaned data\n",
    "flag_gdp_clean = (\n",
    "    (df_pivot.groupby(\"Country\")[\"NGDPRPC\"].transform(lambda x: x.pct_change() < 0)) &\n",
    "    (df_pivot.groupby(\"Country\")[\"NGDPRPC\"].transform(lambda x: x.pct_change().shift(-1) < 0))\n",
    ").astype(int)\n",
    "\n",
    "flag_invest_clean = (\n",
    "    df_pivot.groupby(\"Country\")[\"NID_NGDP\"].transform(lambda x: x.diff() < -2)\n",
    ").astype(int)\n",
    "\n",
    "flag_savings_clean = (\n",
    "    df_pivot.groupby(\"Country\")[\"NGSD_NGDP\"].transform(lambda x: x.diff() < -2)\n",
    ").astype(int)\n",
    "\n",
    "flag_trade_clean = (\n",
    "    (df_pivot.groupby(\"Country\")[\"TX_RPCH\"].transform(lambda x: x < 0)) &\n",
    "    (df_pivot.groupby(\"Country\")[\"TM_RPCH\"].transform(lambda x: x < 0))\n",
    ").astype(int)\n",
    "\n",
    "flag_inflation_clean = (\n",
    "    (df_pivot.groupby(\"Country\")[\"PCPIPCH\"].transform(lambda x: x > 10)) &\n",
    "    (df_pivot.groupby(\"Country\")[\"NGDPRPC\"].transform(lambda x: x.pct_change() < 0))\n",
    ").astype(int)\n",
    "\n",
    "flag_debt_clean = (\n",
    "    (df_pivot[\"GGXWDG_NGDP\"] > 90) &\n",
    "    (df_pivot.groupby(\"Country\")[\"GGXWDG_NGDP\"].transform(lambda x: x.diff() > 10))\n",
    ").astype(int)\n",
    "\n",
    "flag_fiscal_clean = (\n",
    "    (df_pivot.groupby(\"Country\")[\"GGR_NGDP\"].transform(lambda x: x) < \n",
    "     df_pivot.groupby(\"Country\")[\"GGX_NGDP\"].transform(lambda x: x) - 5) &\n",
    "    (df_pivot.groupby(\"Country\")[\"GGX_NGDP\"].transform(lambda x: x.diff()) > 2)\n",
    ").astype(int)\n",
    "\n",
    "flag_current_account_clean = (\n",
    "    (df_pivot[\"BCA_NGDPD\"] < -5) &\n",
    "    (df_pivot.groupby(\"Country\")[\"BCA_NGDPD\"].transform(lambda x: x.diff() < -2))\n",
    ").astype(int)\n",
    "\n",
    "flag_growth_collapse_clean = (\n",
    "    df_pivot.groupby(\"Country\")[\"NGDP_RPCH\"].transform(lambda x: x < -3)\n",
    ").astype(int)\n",
    "\n",
    "flag_deflation_clean = (\n",
    "    (df_pivot.groupby(\"Country\")[\"PCPIPCH\"].transform(lambda x: x < 0)) &\n",
    "    (df_pivot.groupby(\"Country\")[\"NGDP_RPCH\"].transform(lambda x: x < 1))\n",
    ").astype(int)\n",
    "\n",
    "flag_credit_crunch_clean = (\n",
    "    (df_pivot.groupby(\"Country\")[\"NID_NGDP\"].transform(lambda x: x.diff() < -1)) &\n",
    "    (df_pivot.groupby(\"Country\")[\"GGXWDG_NGDP\"].transform(lambda x: x.diff() > 5))\n",
    ").astype(int)\n",
    "\n",
    "flag_external_shock_clean = (\n",
    "    (df_pivot.groupby(\"Country\")[\"TX_RPCH\"].transform(lambda x: x < -5)) &\n",
    "    (df_pivot.groupby(\"Country\")[\"TM_RPCH\"].transform(lambda x: x > -2))\n",
    ").astype(int)\n",
    "\n",
    "signal_names = [\n",
    "    'GDP Decline', 'Investment Collapse', 'Savings Decline', 'Trade Shock',\n",
    "    'Inflation Shock', 'Debt Crisis', 'Fiscal Crisis', 'Current Account Crisis',\n",
    "    'Growth Collapse', 'Deflation Risk', 'Credit Crunch', 'External Shock'\n",
    "]\n",
    "\n",
    "signals_clean = [\n",
    "    flag_gdp_clean, flag_invest_clean, flag_savings_clean, flag_trade_clean, \n",
    "    flag_inflation_clean, flag_debt_clean, flag_fiscal_clean, flag_current_account_clean,\n",
    "    flag_growth_collapse_clean, flag_deflation_clean, flag_credit_crunch_clean, \n",
    "    flag_external_shock_clean\n",
    "]\n",
    "\n",
    "for name, signal in zip(signal_names, signals_clean):\n",
    "    count = signal.sum()\n",
    "    pct = (count / len(df_pivot)) * 100\n",
    "    print(f\"{name:<25} {count:>5} ({pct:>5.2f}%)\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "\n",
    "# Remove temporary column\n",
    "df_pivot = df_pivot.drop(columns=['SignalCount'])\n",
    "\n",
    "# --- Output: dataframe with multiclass label ---\n",
    "df_pivot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "138dee31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RecessionRisk Class Distribution - Compact Plot (up to 2025)\n",
    "df_until_2025 = df_pivot[df_pivot.index <= 2025]\n",
    "risk_counts = df_until_2025[\"RecessionRisk\"].value_counts().sort_index()\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "colors = ['green', 'yellow', 'orange', 'red']\n",
    "plt.bar(risk_counts.index, risk_counts.values, color=colors, edgecolor='black', linewidth=1.5)\n",
    "plt.xlabel('RecessionRisk Class', fontsize=12, fontweight='bold')\n",
    "plt.ylabel('Count', fontsize=12, fontweight='bold')\n",
    "plt.title('RecessionRisk Class Distribution (up to 2025)', fontsize=14, fontweight='bold')\n",
    "plt.xticks([0, 1, 2, 3], ['No Risk', 'Mild', 'Moderate', 'High'])\n",
    "plt.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Add count labels on bars\n",
    "for cls, count in risk_counts.items():\n",
    "    plt.text(cls, count + 10, str(count), ha='center', va='bottom', fontsize=11, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Total samples (up to 2025): {len(df_until_2025)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a9c6b19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# All RecessionRisk Classes by Year - Line Graph (up to 2025)\n",
    "# Calculate counts for each class by year\n",
    "df0 = df_until_2025[df_until_2025[\"RecessionRisk\"] == 0].copy()\n",
    "df1 = df_until_2025[df_until_2025[\"RecessionRisk\"] == 1].copy()\n",
    "df2 = df_until_2025[df_until_2025[\"RecessionRisk\"] == 2].copy()\n",
    "df3 = df_until_2025[df_until_2025[\"RecessionRisk\"] == 3].copy()\n",
    "\n",
    "class0_by_year = df0.groupby(df0.index).size()\n",
    "class1_by_year = df1.groupby(df1.index).size()\n",
    "class2_by_year = df2.groupby(df2.index).size()\n",
    "class3_by_year = df3.groupby(df3.index).size()\n",
    "\n",
    "# Get all years (up to 2025)\n",
    "all_years = sorted(df_until_2025.index.unique())\n",
    "\n",
    "# Reindex to include all years (fill missing with 0)\n",
    "class0_by_year = class0_by_year.reindex(all_years, fill_value=0)\n",
    "class1_by_year = class1_by_year.reindex(all_years, fill_value=0)\n",
    "class2_by_year = class2_by_year.reindex(all_years, fill_value=0)\n",
    "class3_by_year = class3_by_year.reindex(all_years, fill_value=0)\n",
    "\n",
    "plt.figure(figsize=(14, 7))\n",
    "plt.plot(all_years, class0_by_year.values, color='green', linewidth=2.5, marker='o', label='Class 0 (No Risk)', markersize=5)\n",
    "plt.plot(all_years, class1_by_year.values, color='gold', linewidth=2.5, marker='s', label='Class 1 (Mild)', markersize=5)\n",
    "plt.plot(all_years, class2_by_year.values, color='orange', linewidth=2.5, marker='^', label='Class 2 (Moderate)', markersize=5)\n",
    "plt.plot(all_years, class3_by_year.values, color='red', linewidth=2.5, marker='D', label='Class 3 (High)', markersize=5)\n",
    "\n",
    "plt.xlabel('Year', fontsize=12, fontweight='bold')\n",
    "plt.ylabel('Number of Countries', fontsize=12, fontweight='bold')\n",
    "plt.title('RecessionRisk Classes Over Time (up to 2025)', fontsize=14, fontweight='bold')\n",
    "plt.legend(fontsize=11, loc='best')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d627b020",
   "metadata": {},
   "source": [
    "## Cutting Out Data that leads to biasness & Review Remaining Countries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf4aca09",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filtered = df_pivot.loc[(df_pivot.index >= 2000) & (df_pivot.index <= 2025)]\n",
    "df_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "422b2731",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filtered[\"Country\"].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec37a376",
   "metadata": {},
   "source": [
    "# 4. Exploratory Data Analysis\n",
    "\n",
    "## Correlation Heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "364d1ffb",
   "metadata": {},
   "outputs": [],
   "source": [
    "corr = df_filtered.drop(columns=[\"Country\"]).corr()\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(corr, annot=True, fmt=\".2f\", cmap=\"coolwarm\", square=True)\n",
    "plt.title(\"Correlation Heatmap of Features\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59c5b2f5",
   "metadata": {},
   "source": [
    "## Prepare Features and Target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3382119f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_filtered.drop(columns=[\"RecessionRisk\", \"Country\"])\n",
    "y = df_filtered[\"RecessionRisk\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5baccf90",
   "metadata": {},
   "source": [
    "# 5. Machine Learning Models\n",
    "\n",
    "## Global Dataset - Full Features (13 Features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb849a40",
   "metadata": {},
   "source": [
    "### Define and Train All Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e8774c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "#             TRAIN ALL MODELS (MULTICLASS) - IMPROVED\n",
    "# ============================================================\n",
    "def train_all_models(X_train, y_train, X_test, y_test, model_params=None, use_xgb=False):\n",
    "    \"\"\"\n",
    "    Enhanced model training with:\n",
    "    - Improved hyperparameters\n",
    "    - Better class weight handling\n",
    "    - Optimized ensemble strategy\n",
    "    \"\"\"\n",
    "    if model_params is None:\n",
    "        model_params = {\n",
    "            'logit': {\n",
    "                'C': 0.5,  # Increased from 0.2 for better fit\n",
    "                'penalty': 'l2',\n",
    "                'solver': 'saga',  # Better for multiclass\n",
    "                'max_iter': 10000,  # Increased iterations\n",
    "                'random_state': 42,\n",
    "                'class_weight': 'balanced'  # Handle imbalance\n",
    "            },\n",
    "            'rf': {\n",
    "                'n_estimators': 300,  # Increased trees\n",
    "                'max_depth': 6,  # Slightly deeper\n",
    "                'min_samples_leaf': 15,  # Reduced for more flexibility\n",
    "                'min_samples_split': 30,\n",
    "                'max_features': 'sqrt',  # Better than fixed 0.3\n",
    "                'random_state': 42,\n",
    "                'class_weight': 'balanced',\n",
    "                'n_jobs': -1\n",
    "            },\n",
    "            'gb': {\n",
    "                'n_estimators': 300,  # More estimators\n",
    "                'learning_rate': 0.05,  # Slightly increased\n",
    "                'max_depth': 3,  # Deeper trees\n",
    "                'min_samples_leaf': 15,\n",
    "                'subsample': 0.7,  # Increased\n",
    "                'max_features': 'sqrt',\n",
    "                'random_state': 42\n",
    "            },\n",
    "            'dt': {\n",
    "                'max_depth': 4,  # Slightly deeper\n",
    "                'min_samples_leaf': 25,\n",
    "                'min_samples_split': 50,\n",
    "                'random_state': 42,\n",
    "                'class_weight': 'balanced'\n",
    "            },\n",
    "            'svm': {\n",
    "                'C': 2.0,  # Increased regularization\n",
    "                'kernel': 'rbf',\n",
    "                'gamma': 'scale',\n",
    "                'probability': True,\n",
    "                'random_state': 42,\n",
    "                'decision_function_shape': 'ovr',\n",
    "                'class_weight': 'balanced'\n",
    "            },\n",
    "            'xgb': {\n",
    "                'n_estimators': 300,\n",
    "                'learning_rate': 0.07,\n",
    "                'max_depth': 3,\n",
    "                'subsample': 0.8,\n",
    "                'colsample_bytree': 0.8,\n",
    "                'reg_alpha': 0.3,\n",
    "                'reg_lambda': 1.5,\n",
    "                'random_state': 42,\n",
    "                'objective': 'multi:softprob',\n",
    "                'num_class': len(np.unique(y_train)),\n",
    "                'tree_method': 'hist'\n",
    "            }\n",
    "        }\n",
    "\n",
    "    # ----------------- Train models with improved SMOTE strategy -----------------\n",
    "    # Use stratified SMOTE with k_neighbors adjusted for minority class\n",
    "    smote_kwargs = {'random_state': 42, 'k_neighbors': 3}  # Reduced k for small classes\n",
    "    \n",
    "    logit = ImbPipeline([\n",
    "        (\"scaler\", StandardScaler()),\n",
    "        (\"smote\", SMOTE(**smote_kwargs)),\n",
    "        (\"logit\", LogisticRegression(**model_params['logit']))\n",
    "    ]).fit(X_train, y_train)\n",
    "\n",
    "    rf = ImbPipeline([\n",
    "        (\"smote\", SMOTE(**smote_kwargs)),\n",
    "        (\"rf\", RandomForestClassifier(**model_params['rf']))\n",
    "    ]).fit(X_train, y_train)\n",
    "\n",
    "    gb = ImbPipeline([\n",
    "        (\"smote\", SMOTE(**smote_kwargs)),\n",
    "        (\"gb\", GradientBoostingClassifier(**model_params['gb']))\n",
    "    ]).fit(X_train, y_train)\n",
    "\n",
    "    dt = ImbPipeline([\n",
    "        (\"smote\", SMOTE(**smote_kwargs)),\n",
    "        (\"dt\", DecisionTreeClassifier(**model_params['dt']))\n",
    "    ]).fit(X_train, y_train)\n",
    "\n",
    "    svm = ImbPipeline([\n",
    "        (\"scaler\", StandardScaler()),\n",
    "        (\"smote\", SMOTE(**smote_kwargs)),\n",
    "        (\"svm\", SVC(**model_params['svm']))\n",
    "    ]).fit(X_train, y_train)\n",
    "\n",
    "    models = {\n",
    "        \"Logistic Regression\": logit,\n",
    "        \"Random Forest\": rf,\n",
    "        \"Gradient Boosting\": gb,\n",
    "        \"Decision Tree\": dt,\n",
    "        \"SVM\": svm,\n",
    "    }\n",
    "\n",
    "    if use_xgb:\n",
    "        xgb = ImbPipeline([\n",
    "            (\"smote\", SMOTE(**smote_kwargs)),\n",
    "            (\"xgb\", XGBClassifier(**model_params['xgb']))\n",
    "        ]).fit(X_train, y_train)\n",
    "        models[\"XGBoost\"] = xgb\n",
    "\n",
    "    # ----------------- Improved Ensemble with weighted voting -----------------\n",
    "    # Use best performing models with weights\n",
    "    ensemble_estimators = [\n",
    "        (\"rf\", rf.named_steps[\"rf\"]),  # RF typically performs well\n",
    "        (\"gb\", gb.named_steps[\"gb\"]),  # GB is strong\n",
    "        (\"svm\", svm.named_steps[\"svm\"]),  # SVM adds diversity\n",
    "        (\"logit\", logit.named_steps[\"logit\"])  # Linear baseline\n",
    "    ]\n",
    "    \n",
    "    # Weight models based on typical performance (can tune after first run)\n",
    "    ensemble_weights = [2, 2, 1, 1]  # Give more weight to tree models\n",
    "    \n",
    "    if use_xgb:\n",
    "        ensemble_estimators.append((\"xgb\", xgb.named_steps[\"xgb\"]))\n",
    "        ensemble_weights.append(2)\n",
    "\n",
    "    ensemble = VotingClassifier(\n",
    "        estimators=ensemble_estimators, \n",
    "        voting=\"soft\",\n",
    "        weights=ensemble_weights\n",
    "    )\n",
    "    ensemble.fit(X_train, y_train)\n",
    "    models[\"Ensemble (Weighted)\"] = ensemble\n",
    "\n",
    "    # ----------------- Metrics with additional evaluation -----------------\n",
    "    results = {}\n",
    "    confusion_mats = {}\n",
    "\n",
    "    for name, m in models.items():\n",
    "        y_pred_train = m.predict(X_train)\n",
    "        y_pred_test = m.predict(X_test)\n",
    "\n",
    "        results[name] = {\n",
    "            \"Train Accuracy\": accuracy_score(y_train, y_pred_train),\n",
    "            \"Test Accuracy\": accuracy_score(y_test, y_pred_test),\n",
    "            \"Precision (macro)\": precision_score(y_test, y_pred_test, average=\"macro\", zero_division=0),\n",
    "            \"Recall (macro)\": recall_score(y_test, y_pred_test, average=\"macro\", zero_division=0),\n",
    "            \"F1 (macro)\": f1_score(y_test, y_pred_test, average=\"macro\", zero_division=0),\n",
    "            # Add weighted F1 for imbalanced datasets\n",
    "            \"F1 (weighted)\": f1_score(y_test, y_pred_test, average=\"weighted\", zero_division=0)\n",
    "        }\n",
    "\n",
    "        confusion_mats[name] = confusion_matrix(y_test, y_pred_test)\n",
    "\n",
    "    results_df = pd.DataFrame(results).T\n",
    "    return models, results_df, confusion_mats\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "#             FEATURE IMPORTANCE (MULTICLASS)\n",
    "# ============================================================\n",
    "def plot_feature_importance(models, feature_names, title_prefix=\"\", top_n=15):\n",
    "    \"\"\"\n",
    "    Display feature importance with improved visualization.\n",
    "    top_n: Show only top N most important features\n",
    "    \"\"\"\n",
    "    logit = models.get(\"Logistic Regression\")\n",
    "    rf = models.get(\"Random Forest\")\n",
    "    gb = models.get(\"Gradient Boosting\")\n",
    "    dt = models.get(\"Decision Tree\")\n",
    "\n",
    "    coef_matrix = logit.named_steps['logit'].coef_\n",
    "    coef_mean_abs = np.mean(np.abs(coef_matrix), axis=0)\n",
    "\n",
    "    logit_importance = pd.DataFrame({\n",
    "        \"Feature\": feature_names,\n",
    "        \"Importance\": coef_mean_abs\n",
    "    }).sort_values(\"Importance\", ascending=False).head(top_n)\n",
    "\n",
    "    rf_importance = pd.DataFrame({\n",
    "        \"Feature\": feature_names,\n",
    "        \"Importance\": rf.named_steps['rf'].feature_importances_\n",
    "    }).sort_values(\"Importance\", ascending=False).head(top_n)\n",
    "\n",
    "    gb_importance = pd.DataFrame({\n",
    "        \"Feature\": feature_names,\n",
    "        \"Importance\": gb.named_steps['gb'].feature_importances_\n",
    "    }).sort_values(\"Importance\", ascending=False).head(top_n)\n",
    "\n",
    "    dt_importance = pd.DataFrame({\n",
    "        \"Feature\": feature_names,\n",
    "        \"Importance\": dt.named_steps['dt'].feature_importances_\n",
    "    }).sort_values(\"Importance\", ascending=False).head(top_n)\n",
    "\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(16, 10))\n",
    "\n",
    "    # Sort in ascending order for better visualization\n",
    "    logit_sorted = logit_importance.sort_values(\"Importance\")\n",
    "    rf_sorted = rf_importance.sort_values(\"Importance\")\n",
    "    gb_sorted = gb_importance.sort_values(\"Importance\")\n",
    "    dt_sorted = dt_importance.sort_values(\"Importance\")\n",
    "\n",
    "    axes[0, 0].barh(logit_sorted[\"Feature\"], logit_sorted[\"Importance\"], color='steelblue')\n",
    "    axes[0, 0].set_title(f\"{title_prefix}Logistic Regression (Top {top_n})\", fontsize=12, fontweight='bold')\n",
    "    axes[0, 0].set_xlabel(\"Mean |Coefficient|\")\n",
    "\n",
    "    axes[0, 1].barh(rf_sorted[\"Feature\"], rf_sorted[\"Importance\"], color='forestgreen')\n",
    "    axes[0, 1].set_title(f\"{title_prefix}Random Forest (Top {top_n})\", fontsize=12, fontweight='bold')\n",
    "    axes[0, 1].set_xlabel(\"Feature Importance\")\n",
    "\n",
    "    axes[1, 0].barh(gb_sorted[\"Feature\"], gb_sorted[\"Importance\"], color='darkorange')\n",
    "    axes[1, 0].set_title(f\"{title_prefix}Gradient Boosting (Top {top_n})\", fontsize=12, fontweight='bold')\n",
    "    axes[1, 0].set_xlabel(\"Feature Importance\")\n",
    "\n",
    "    axes[1, 1].barh(dt_sorted[\"Feature\"], dt_sorted[\"Importance\"], color='crimson')\n",
    "    axes[1, 1].set_title(f\"{title_prefix}Decision Tree (Top {top_n})\", fontsize=12, fontweight='bold')\n",
    "    axes[1, 1].set_xlabel(\"Feature Importance\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "#             CONFUSION MATRIX DISPLAY (MULTICLASS)\n",
    "# ============================================================\n",
    "def show_confusion_matrices(confusion_mats, results_df):\n",
    "    \"\"\"\n",
    "    Display confusion matrices with percentage annotations for better interpretability.\n",
    "    \"\"\"\n",
    "    n_models = len(confusion_mats)\n",
    "    fig, axes = plt.subplots(1, n_models, figsize=(5*n_models, 5))\n",
    "\n",
    "    if n_models == 1:\n",
    "        axes = [axes]\n",
    "\n",
    "    for ax, (name, cm) in zip(axes, confusion_mats.items()):\n",
    "        disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
    "        disp.plot(cmap=\"Blues\", ax=ax, colorbar=False)\n",
    "\n",
    "        acc = results_df.loc[name, \"Test Accuracy\"]\n",
    "        prec = results_df.loc[name, \"Precision (macro)\"]\n",
    "        rec = results_df.loc[name, \"Recall (macro)\"]\n",
    "        f1 = results_df.loc[name, \"F1 (macro)\"]\n",
    "\n",
    "        ax.set_title(\n",
    "            f\"{name}\\nAcc={acc:.3f} | Prec={prec:.3f} | Rec={rec:.3f} | F1={f1:.3f}\",\n",
    "            fontsize=10,\n",
    "            fontweight='bold'\n",
    "        )\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# ============================================================\n",
    "#             ROC CURVES + MACRO AUC (MULTICLASS)\n",
    "# ============================================================\n",
    "def show_roc_curves(models, X_test, y_test):\n",
    "    \"\"\"\n",
    "    Display ROC curves and macro AUC for all models.\n",
    "    Works for multiclass classification.\n",
    "    \"\"\"\n",
    "    n_models = len(models)\n",
    "    fig, axes = plt.subplots(1, n_models, figsize=(6*n_models, 5))\n",
    "\n",
    "    if n_models == 1:\n",
    "        axes = [axes]\n",
    "\n",
    "    classes = sorted(np.unique(y_test))\n",
    "\n",
    "    for ax, (name, model) in zip(axes, models.items()):\n",
    "\n",
    "        if not hasattr(model, \"predict_proba\"):\n",
    "            ax.set_title(f\"{name}\\n(No predict_proba)\")\n",
    "            ax.axis(\"off\")\n",
    "            continue\n",
    "\n",
    "        # Predict probabilities\n",
    "        y_proba = model.predict_proba(X_test)\n",
    "\n",
    "        aucs = []\n",
    "        for c in classes:\n",
    "            y_true_bin = (y_test == c).astype(int)\n",
    "            y_score = y_proba[:, c]\n",
    "\n",
    "            fpr, tpr, _ = roc_curve(y_true_bin, y_score)\n",
    "            auc_val = auc(fpr, tpr)\n",
    "            aucs.append(auc_val)\n",
    "\n",
    "            ax.plot(fpr, tpr, label=f\"Class {c} (AUC={auc_val:.2f})\", linewidth=2)\n",
    "\n",
    "        macro_auc = np.mean(aucs)\n",
    "\n",
    "        ax.plot([0, 1], [0, 1], \"k--\", alpha=0.5, linewidth=1.5, label='Random')\n",
    "        ax.set_title(f\"{name}\\nMacro AUC={macro_auc:.3f}\", fontsize=11, fontweight='bold')\n",
    "        ax.set_xlabel(\"False Positive Rate\", fontsize=10)\n",
    "        ax.set_ylabel(\"True Positive Rate\", fontsize=10)\n",
    "        ax.legend(loc='lower right', fontsize=8)\n",
    "        ax.grid(alpha=0.3)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d2cf4d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "split_index = int(len(X) * 0.8)\n",
    "X_train, X_test = X.iloc[:split_index], X.iloc[split_index:]\n",
    "y_train, y_test = y.iloc[:split_index], y.iloc[split_index:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "685c21a0",
   "metadata": {},
   "source": [
    "# Training the model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "355300d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "models, summary_df, confusion_mats = train_all_models(X_train, y_train, X_test, y_test)\n",
    "\n",
    "print(summary_df)\n",
    "plot_feature_importance(models, X_train.columns.tolist())\n",
    "# Show confusion matrices with metrics underneath\n",
    "show_confusion_matrices(confusion_mats, summary_df)\n",
    "show_roc_curves(models, X_test, y_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56b66c25",
   "metadata": {},
   "source": [
    "### Reduced Global Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65b8167b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#reduced variables set\n",
    "selected_features = ['TM_RPCH', 'NGDP_RPCH', 'TX_RPCH', 'BCA_NGDPD']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "107301e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_reduced = X_train[selected_features]\n",
    "X_test_reduced = X_test[selected_features]\n",
    "\n",
    "# Unpack all three return values\n",
    "models_reduced, summary_df_reduced, confusion_mats_reduced = train_all_models(\n",
    "    X_train_reduced, y_train, X_test_reduced, y_test\n",
    ")\n",
    "\n",
    "# Show metrics table\n",
    "print(summary_df_reduced)\n",
    "# Plot feature importance\n",
    "plot_feature_importance(models_reduced, feature_names=selected_features, title_prefix=\"Reduced Features - \")\n",
    "\n",
    "# Show confusion matrices (all in one window)\n",
    "show_confusion_matrices(confusion_mats_reduced, summary_df_reduced)\n",
    "show_roc_curves(models_reduced, X_test_reduced, y_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12134a9a",
   "metadata": {},
   "source": [
    "### Split Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8430911",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map countries to continents (same logic as before)\n",
    "try:\n",
    "    import pycountry\n",
    "    import pycountry_convert as pc\n",
    "    \n",
    "    def country_to_continent(name):\n",
    "        try:\n",
    "            lookup_name = name.replace('_', ' ')\n",
    "            country = pycountry.countries.lookup(lookup_name)\n",
    "            alpha2 = country.alpha_2\n",
    "            cc = pc.country_alpha2_to_continent_code(alpha2)\n",
    "            continent_map = {\n",
    "                'AF': 'Africa',\n",
    "                'AS': 'Asia',\n",
    "                'EU': 'Europe',\n",
    "                'NA': 'North_America',\n",
    "                'OC': 'Oceania',\n",
    "                'SA': 'South_America'\n",
    "            }\n",
    "            return continent_map.get(cc, 'Unknown')\n",
    "        except Exception:\n",
    "            return 'Unknown'\n",
    "except ImportError:\n",
    "    # Fallback mapping for common countries (extend as needed)\n",
    "    fallback = {\n",
    "    'United_States': 'North_America', 'Canada': 'North_America', 'Mexico': 'North_America',\n",
    "    'China': 'Asia', 'India': 'Asia', 'Japan': 'Asia', 'Afghanistan': 'Asia',\n",
    "    'Korea': 'Asia', 'Indonesia': 'Asia', 'Thailand': 'Asia', 'Vietnam': 'Asia',\n",
    "    'Germany': 'Europe', 'France': 'Europe', 'United_Kingdom': 'Europe', 'Italy': 'Europe',\n",
    "    'Spain': 'Europe', 'Russia': 'Europe', 'Turkey': 'Europe', 'Poland': 'Europe',\n",
    "    'Brazil': 'South_America', 'Argentina': 'South_America', 'Chile': 'South_America',\n",
    "    'Colombia': 'South_America', 'Peru': 'South_America', 'Venezuela': 'South_America',\n",
    "    'Australia': 'Oceania', 'New_Zealand': 'Oceania',\n",
    "    'South_Africa': 'Africa', 'Nigeria': 'Africa', 'Egypt': 'Africa', 'Zimbabwe': 'Africa',\n",
    "    'Kenya': 'Africa', 'Ethiopia': 'Africa', 'Morocco': 'Africa',\n",
    "\n",
    "    'Albania': 'Europe', 'Algeria': 'Africa', 'Austria': 'Europe', 'Barbados': 'North_America',\n",
    "    'Belgium': 'Europe', 'Bolivia': 'South_America', 'Bosnia_and_Herzegovina': 'Europe',\n",
    "    'Bulgaria': 'Europe', 'Cabo_Verde': 'Africa', 'Costa_Rica': 'North_America',\n",
    "    'Croatia': 'Europe', 'Cyprus': 'Europe', 'Czech_Republic': 'Europe', 'Denmark': 'Europe',\n",
    "    'Dominican_Republic': 'North_America', 'Estonia': 'Europe', 'Finland': 'Europe',\n",
    "    'Hungary': 'Europe', 'Iceland': 'Europe', 'Ireland': 'Europe',\n",
    "    'Islamic_Republic_of_Iran': 'Asia', 'Israel': 'Asia', 'Jordan': 'Asia',\n",
    "    'Kazakhstan': 'Asia', 'Latvia': 'Europe', 'Lebanon': 'Asia', 'Lithuania': 'Europe',\n",
    "    'Luxembourg': 'Europe', 'Malta': 'Europe', 'Netherlands': 'Europe',\n",
    "    'North_Macedonia': 'Europe', 'Norway': 'Europe', 'Pakistan': 'Asia',\n",
    "    'Panama': 'North_America', 'Paraguay': 'South_America', 'Portugal': 'Europe',\n",
    "    'Romania': 'Europe', 'Saudi_Arabia': 'Asia', 'Serbia': 'Europe', 'Seychelles': 'Africa',\n",
    "    'Slovak_Republic': 'Europe', 'Slovenia': 'Europe', 'Sweden': 'Europe',\n",
    "    'Switzerland': 'Europe', 'Syria': 'Asia', 'Taiwan_Province_of_China': 'Asia',\n",
    "    'Trinidad_and_Tobago': 'North_America', 'TÃ¼rkiye': 'Europe', 'Uruguay': 'South_America',\n",
    "    'Botswana': 'Africa', 'Cameroon': 'Africa', 'Djibouti': 'Africa', 'Equatorial_Guinea': 'Africa',\n",
    "    'Eswatini': 'Africa','Guyana': 'South_America','Lesotho': 'Africa','Mali': 'Africa',\n",
    "    'Mauritania': 'Africa','Namibia': 'Africa','Niger': 'Africa','Oman': 'Asia','Yemen': 'Asia',\n",
    "    'Zambia': 'Africa','St._Kitts_and_Nevis': 'North_America'\n",
    "}\n",
    "\n",
    "    def country_to_continent(name):\n",
    "        return fallback.get(name.replace(' ', '_'), 'Unknown')\n",
    "\n",
    "# --- Add Continent column ---\n",
    "df_filtered_copy = df_pivot.copy()\n",
    "df_filtered_copy['Continent'] = df_filtered_copy['Country'].astype(str).apply(country_to_continent)\n",
    "\n",
    "# --- Map continents to economy groups ---\n",
    "continent_to_economy = {\n",
    "    'Europe': 'Upper_Economies',\n",
    "    'North_America': 'Upper_Economies',\n",
    "    'Oceania': 'Upper_Economies',\n",
    "    'Africa': 'Lower_Economies',\n",
    "    'Asia': 'Lower_Economies',\n",
    "    'South_America': 'Lower_Economies'\n",
    "}\n",
    "\n",
    "df_filtered_copy['EconomyGroup'] = df_filtered_copy['Continent'].map(continent_to_economy)\n",
    "\n",
    "# --- Create Lower and Upper economy DataFrames ---\n",
    "df_Lower_Economies = df_filtered_copy[df_filtered_copy['EconomyGroup'] == 'Lower_Economies'].drop(columns=['Continent','EconomyGroup'])\n",
    "df_Upper_Economies = df_filtered_copy[df_filtered_copy['EconomyGroup'] == 'Upper_Economies'].drop(columns=['Continent','EconomyGroup'])\n",
    "\n",
    "# --- Print summary ---\n",
    "print(\"=\" * 80)\n",
    "print(\" \" * 25 + \"ECONOMY GROUP SUMMARY\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(f\"\\nTotal DataFrames created:\")\n",
    "print(f\"  â€¢ Lower Economies: {len(df_Lower_Economies)} rows\")\n",
    "print(f\"  â€¢ Upper Economies: {len(df_Upper_Economies)} rows\")\n",
    "print(f\"  â€¢ Total: {len(df_filtered_copy)} rows\")\n",
    "\n",
    "# --- Show RecessionRisk distribution per economy group ---\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"RECESSION RISK DISTRIBUTION BY ECONOMY GROUP\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(\"\\nðŸ“Š LOWER ECONOMIES (Africa, Asia, South America):\")\n",
    "print(\"-\" * 80)\n",
    "lower_risk_dist = df_Lower_Economies['RecessionRisk'].value_counts().sort_index()\n",
    "print(lower_risk_dist)\n",
    "print(f\"\\nTotal Lower Economy samples: {len(df_Lower_Economies)}\")\n",
    "\n",
    "print(\"\\nDetailed breakdown:\")\n",
    "for risk_class in range(4):\n",
    "    count = lower_risk_dist.get(risk_class, 0)\n",
    "    pct = (count / len(df_Lower_Economies)) * 100 if len(df_Lower_Economies) > 0 else 0\n",
    "    risk_label = ['No Risk', 'Mild', 'Moderate', 'High'][risk_class]\n",
    "    print(f\"  Class {risk_class} ({risk_label:>8}): {count:>5} samples ({pct:>6.2f}%)\")\n",
    "\n",
    "print(\"\\n\" + \"-\" * 80)\n",
    "print(\"\\nðŸ“Š UPPER ECONOMIES (Europe, North America, Oceania):\")\n",
    "print(\"-\" * 80)\n",
    "upper_risk_dist = df_Upper_Economies['RecessionRisk'].value_counts().sort_index()\n",
    "print(upper_risk_dist)\n",
    "print(f\"\\nTotal Upper Economy samples: {len(df_Upper_Economies)}\")\n",
    "\n",
    "print(\"\\nDetailed breakdown:\")\n",
    "for risk_class in range(4):\n",
    "    count = upper_risk_dist.get(risk_class, 0)\n",
    "    pct = (count / len(df_Upper_Economies)) * 100 if len(df_Upper_Economies) > 0 else 0\n",
    "    risk_label = ['No Risk', 'Mild', 'Moderate', 'High'][risk_class]\n",
    "    print(f\"  Class {risk_class} ({risk_label:>8}): {count:>5} samples ({pct:>6.2f}%)\")\n",
    "\n",
    "# --- Comparative Analysis ---\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"COMPARATIVE ANALYSIS: LOWER vs UPPER ECONOMIES\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(f\"\\n{'Risk Class':<15} {'Lower Econ':<20} {'Upper Econ':<20} {'Difference':<15}\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "for risk_class in range(4):\n",
    "    lower_count = lower_risk_dist.get(risk_class, 0)\n",
    "    upper_count = upper_risk_dist.get(risk_class, 0)\n",
    "    \n",
    "    lower_pct = (lower_count / len(df_Lower_Economies)) * 100 if len(df_Lower_Economies) > 0 else 0\n",
    "    upper_pct = (upper_count / len(df_Upper_Economies)) * 100 if len(df_Upper_Economies) > 0 else 0\n",
    "    \n",
    "    diff = lower_pct - upper_pct\n",
    "    risk_label = ['No Risk', 'Mild', 'Moderate', 'High'][risk_class]\n",
    "    \n",
    "    print(f\"{risk_label:<15} {lower_count:>5} ({lower_pct:>5.1f}%)     {upper_count:>5} ({upper_pct:>5.1f}%)     {diff:>+6.1f}%\")\n",
    "\n",
    "# --- Visualization ---\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"VISUALIZATION: Recession Risk by Economy Group\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Lower Economies\n",
    "lower_counts = [lower_risk_dist.get(i, 0) for i in range(4)]\n",
    "axes[0].bar(range(4), lower_counts, color=['green', 'yellow', 'orange', 'red'], \n",
    "            edgecolor='black', linewidth=1.5)\n",
    "axes[0].set_xlabel('RecessionRisk Class', fontsize=12, fontweight='bold')\n",
    "axes[0].set_ylabel('Count', fontsize=12, fontweight='bold')\n",
    "axes[0].set_title('Lower Economies\\n(Africa, Asia, South America)', fontsize=14, fontweight='bold')\n",
    "axes[0].set_xticks(range(4))\n",
    "axes[0].set_xticklabels(['No Risk', 'Mild', 'Moderate', 'High'])\n",
    "axes[0].grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Add count labels\n",
    "for i, count in enumerate(lower_counts):\n",
    "    if count > 0:\n",
    "        axes[0].text(i, count + max(lower_counts)*0.02, str(count), \n",
    "                    ha='center', va='bottom', fontsize=11, fontweight='bold')\n",
    "\n",
    "# Upper Economies\n",
    "upper_counts = [upper_risk_dist.get(i, 0) for i in range(4)]\n",
    "axes[1].bar(range(4), upper_counts, color=['green', 'yellow', 'orange', 'red'], \n",
    "            edgecolor='black', linewidth=1.5)\n",
    "axes[1].set_xlabel('RecessionRisk Class', fontsize=12, fontweight='bold')\n",
    "axes[1].set_ylabel('Count', fontsize=12, fontweight='bold')\n",
    "axes[1].set_title('Upper Economies\\n(Europe, North America, Oceania)', fontsize=14, fontweight='bold')\n",
    "axes[1].set_xticks(range(4))\n",
    "axes[1].set_xticklabels(['No Risk', 'Mild', 'Moderate', 'High'])\n",
    "axes[1].grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Add count labels\n",
    "for i, count in enumerate(upper_counts):\n",
    "    if count > 0:\n",
    "        axes[1].text(i, count + max(upper_counts)*0.02, str(count), \n",
    "                    ha='center', va='bottom', fontsize=11, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# --- Countries per economy group ---\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"COUNTRIES BY ECONOMY GROUP\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(\"\\nLower Economies Countries:\")\n",
    "lower_countries = sorted(df_Lower_Economies['Country'].unique())\n",
    "print(f\"  Total: {len(lower_countries)} countries\")\n",
    "print(f\"  {', '.join(lower_countries[:10])}...\")  # Show first 10\n",
    "\n",
    "print(\"\\nUpper Economies Countries:\")\n",
    "upper_countries = sorted(df_Upper_Economies['Country'].unique())\n",
    "print(f\"  Total: {len(upper_countries)} countries\")\n",
    "print(f\"  {', '.join(upper_countries[:10])}...\")  # Show first 10\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9989b926",
   "metadata": {},
   "source": [
    "# 6. Economy-Specific Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afbeadb0",
   "metadata": {},
   "source": [
    "## Upper Economies - Full Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19751b1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data\n",
    "X = df_Upper_Economies.drop(columns=[\"RecessionRisk\", \"Country\"])\n",
    "y = df_Upper_Economies[\"RecessionRisk\"]\n",
    "\n",
    "split_index = int(len(X) * 0.8)\n",
    "X_train, X_test = X.iloc[:split_index], X.iloc[split_index:]\n",
    "y_train, y_test = y.iloc[:split_index], y.iloc[split_index:]\n",
    "\n",
    "# Train models and unpack all three return values\n",
    "models_upper, summary_df_upper, confusion_mats_upper = train_all_models(\n",
    "    X_train, y_train, X_test, y_test\n",
    ")\n",
    "\n",
    "# Show metrics table\n",
    "print(summary_df_upper)\n",
    "\n",
    "# Plot feature importance\n",
    "plot_feature_importance(models_upper, X_train.columns.tolist(), title_prefix=\"Upper Economies - \")\n",
    "\n",
    "# Show confusion matrices (all in one window)\n",
    "show_confusion_matrices(confusion_mats_upper, summary_df_upper)\n",
    "# âœ… Show ROC curves + AUC for upper-economy models\n",
    "show_roc_curves(models_upper, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd4b3137",
   "metadata": {},
   "source": [
    "## Lower Economies - Full Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24ecfe97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for Lower Economies\n",
    "X = df_Lower_Economies.drop(columns=[\"RecessionRisk\", \"Country\"])\n",
    "y = df_Lower_Economies[\"RecessionRisk\"]\n",
    "\n",
    "split_index = int(len(X) * 0.8)\n",
    "X_train, X_test = X.iloc[:split_index], X.iloc[split_index:]\n",
    "y_train, y_test = y.iloc[:split_index], y.iloc[split_index:]\n",
    "\n",
    "# Train models and unpack all three return values\n",
    "models_lower, summary_df_lower, confusion_mats_lower = train_all_models(\n",
    "    X_train, y_train, X_test, y_test\n",
    ")\n",
    "\n",
    "# Show metrics table\n",
    "print(summary_df_lower)\n",
    "\n",
    "# Plot feature importance\n",
    "plot_feature_importance(models_lower, X_train.columns.tolist(), title_prefix=\"Lower Economies - \")\n",
    "\n",
    "\n",
    "# Show confusion matrices (all in one window)\n",
    "show_confusion_matrices(confusion_mats_lower, summary_df_lower)\n",
    "# âœ… Show ROC curves + AUC for lower-economy models\n",
    "show_roc_curves(models_lower, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cfc2538",
   "metadata": {},
   "source": [
    "## Upper Economies - Reduced Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a25f6b34",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_upper = df_Upper_Economies[selected_features]\n",
    "y_upper = df_Upper_Economies[\"RecessionRisk\"]\n",
    "\n",
    "split_index_upper = int(len(X_upper) * 0.8)\n",
    "X_train_upper = X_upper.iloc[:split_index_upper]\n",
    "X_test_upper = X_upper.iloc[split_index_upper:]\n",
    "y_train_upper = y_upper.iloc[:split_index_upper]\n",
    "y_test_upper = y_upper.iloc[split_index_upper:]\n",
    "\n",
    "# âœ… Unpack all three return values\n",
    "models_upper_reduced, summary_df_upper_reduced, confusion_mats_upper_reduced = train_all_models(\n",
    "    X_train_upper, y_train_upper, X_test_upper, y_test_upper\n",
    ")\n",
    "\n",
    "print(\"Upper Economies Accuracy (Reduced Features):\")\n",
    "print(summary_df_upper_reduced)\n",
    "\n",
    "# Feature importance\n",
    "plot_feature_importance(\n",
    "    models_upper_reduced,\n",
    "    feature_names=selected_features,\n",
    "    title_prefix=\"Upper Economies - Reduced Features - \"\n",
    ")\n",
    "\n",
    "# âœ… Show confusion matrices (all in one window)\n",
    "show_confusion_matrices(confusion_mats_upper_reduced, summary_df_upper_reduced)\n",
    "\n",
    "# âœ… Show ROC curves + AUC for reduced-feature upper-economy models\n",
    "show_roc_curves(models_upper_reduced, X_test_upper, y_test_upper)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57f45978",
   "metadata": {},
   "source": [
    "## Lower Economies - Reduced Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff38534b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_lower = df_Lower_Economies[selected_features]\n",
    "y_lower = df_Lower_Economies[\"RecessionRisk\"]\n",
    "\n",
    "split_index_lower = int(len(X_lower) * 0.8)\n",
    "X_train_lower = X_lower.iloc[:split_index_lower]\n",
    "X_test_lower = X_lower.iloc[split_index_lower:]\n",
    "y_train_lower = y_lower.iloc[:split_index_lower]\n",
    "y_test_lower = y_lower.iloc[split_index_lower:]\n",
    "\n",
    "# âœ… Unpack all three return values\n",
    "models_lower_reduced, summary_df_lower_reduced, confusion_mats_lower_reduced = train_all_models(\n",
    "    X_train_lower, y_train_lower, X_test_lower, y_test_lower\n",
    ")\n",
    "\n",
    "print(\"Lower Economies Accuracy (Reduced Features):\")\n",
    "print(summary_df_lower_reduced)\n",
    "\n",
    "# Feature importance\n",
    "plot_feature_importance(\n",
    "    models_lower_reduced,\n",
    "    feature_names=selected_features,\n",
    "    title_prefix=\"Lower Economies - Reduced Features - \"\n",
    ")\n",
    "\n",
    "# âœ… Show confusion matrices (all in one window)\n",
    "show_confusion_matrices(confusion_mats_lower_reduced, summary_df_lower_reduced)\n",
    "\n",
    "# âœ… Show ROC curves + AUC for reduced-feature lower-economy models\n",
    "show_roc_curves(models_lower_reduced, X_test_lower, y_test_lower)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf5c244f",
   "metadata": {},
   "source": [
    "# Prediction 2026-2030"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a269a6cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_predict = df_pivot.loc[df_pivot.index > 2025]\n",
    "df_predict_original = df_predict.copy()\n",
    "df_predict = df_predict.drop(columns=[\"RecessionRisk\", \"Country\"])\n",
    "\n",
    "df_predict_original['Continent'] = df_predict_original['Country'].astype(str).apply(country_to_continent)\n",
    "\n",
    "continent_to_economy = {\n",
    "    'Europe': 'Upper_Economies',\n",
    "    'North_America': 'Upper_Economies',\n",
    "    'Oceania': 'Upper_Economies',\n",
    "    'Africa': 'Lower_Economies',\n",
    "    'Asia': 'Lower_Economies',\n",
    "    'South_America': 'Lower_Economies'\n",
    "}\n",
    "\n",
    "df_predict_original['EconomyGroup'] = df_predict_original['Continent'].map(continent_to_economy)\n",
    "\n",
    "df_predict_lower = df_predict_original[df_predict_original['EconomyGroup'] == 'Lower_Economies']\n",
    "\n",
    "df_predict_upper = df_predict_original[df_predict_original['EconomyGroup'] == 'Upper_Economies']\n",
    "\n",
    "print(\"Created economy-specific prediction DataFrames from df_predict_original:\")\n",
    "print(f\" - Lower_Economies predictions: {len(df_predict_lower)} rows\")\n",
    "print(f\" - Upper_Economies predictions: {len(df_predict_upper)} rows\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fb556a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "#                  PREDICTION FUNCTION (MULTICLASS)\n",
    "# ============================================================\n",
    "def make_predictions(models, df_predict):\n",
    "    \"\"\"\n",
    "    Return multiclass predictions (0,1,2,3) from every model in one dataframe.\n",
    "    \"\"\"\n",
    "    predictions = {}\n",
    "\n",
    "    for name, model in models.items():\n",
    "        # If model supports predict_proba â†’ use argmax over classes\n",
    "        if hasattr(model, \"predict_proba\"):\n",
    "            proba = model.predict_proba(df_predict)\n",
    "            predictions[name] = proba.argmax(axis=1)\n",
    "        else:\n",
    "            predictions[name] = model.predict(df_predict)\n",
    "\n",
    "    return pd.DataFrame(predictions, index=df_predict.index)\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "#      MULTICLASS RECESSION RISK COUNTS PER MODEL (PLOT)\n",
    "# ============================================================\n",
    "def plot_recession_counts_per_model(df_with_country, title):\n",
    "    \"\"\"\n",
    "    Grouped bar plot per year per model:\n",
    "    - X: Year\n",
    "    - Bars: counts of each RecessionRisk class (0,1,2,3)\n",
    "    - Facets: one subplot per model\n",
    "\n",
    "    Expects df_with_country with columns:\n",
    "        ['Year','Country', <model prediction columns>]\n",
    "    where model columns contain multiclass predictions (0â€“3).\n",
    "    \"\"\"\n",
    "\n",
    "    # Melt to long format\n",
    "    df_long = df_with_country.melt(\n",
    "        id_vars=['Year', 'Country'],\n",
    "        var_name='Model',\n",
    "        value_name='Prediction'\n",
    "    )\n",
    "\n",
    "    # Count per (Year, Model, Prediction)\n",
    "    counts = (\n",
    "        df_long.groupby(['Year', 'Model', 'Prediction'])\n",
    "               .size()\n",
    "               .reset_index(name='Count')\n",
    "    )\n",
    "\n",
    "    # Pivot â†’ columns become risk classes (0,1,2,3)\n",
    "    counts_pivot = (\n",
    "        counts.pivot(index=['Year', 'Model'], columns='Prediction', values='Count')\n",
    "              .fillna(0)\n",
    "    ).reset_index()\n",
    "\n",
    "    # Identify risk classes present\n",
    "    risk_classes = sorted([c for c in counts_pivot.columns if isinstance(c, int)])\n",
    "\n",
    "    # Reorder columns: Year, Model, then risk classes\n",
    "    counts_pivot = counts_pivot[['Year', 'Model'] + risk_classes]\n",
    "\n",
    "    # Plotting layout\n",
    "    models = counts_pivot['Model'].unique()\n",
    "    n_models = len(models)\n",
    "    n_cols = 3\n",
    "    n_rows = (n_models + n_cols - 1) // n_cols\n",
    "\n",
    "    fig, axes = plt.subplots(n_rows, n_cols, figsize=(5 * n_cols, 4 * n_rows), sharex=False, sharey=True)\n",
    "    axes = axes.flatten()\n",
    "\n",
    "    for i, model in enumerate(models):\n",
    "        ax = axes[i]\n",
    "\n",
    "        # Extract model-specific data\n",
    "        mdf = counts_pivot[counts_pivot['Model'] == model].set_index('Year')[risk_classes]\n",
    "\n",
    "        # Plot grouped bars\n",
    "        plot = mdf.plot(kind='bar', ax=ax)\n",
    "\n",
    "        ax.set_title(model)\n",
    "        ax.set_ylabel('Number of countries')\n",
    "        ax.set_xlabel('Year')\n",
    "        ax.legend(title='RecessionRisk')\n",
    "\n",
    "        # Annotate bars\n",
    "        for p in plot.patches:\n",
    "            height = p.get_height()\n",
    "            if height > 0:\n",
    "                ax.annotate(\n",
    "                    f'{int(height)}',\n",
    "                    (p.get_x() + p.get_width() / 2., height),\n",
    "                    ha='center', va='bottom',\n",
    "                    fontsize=8, color='black', xytext=(0, 2),\n",
    "                    textcoords='offset points'\n",
    "                )\n",
    "\n",
    "    # Hide unused axes\n",
    "    for j in range(i + 1, len(axes)):\n",
    "        axes[j].set_visible(False)\n",
    "\n",
    "    fig.suptitle(title, y=1.02, fontsize=12)\n",
    "    fig.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17f81a29",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_predict_original.drop(columns=[\"RecessionRisk\"], inplace=True)\n",
    "df_predict_original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bfef5b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_predict_upper.drop(columns=[\"RecessionRisk\"], inplace=True)\n",
    "df_predict_upper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b88263da",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_predict_lower.drop(columns=[\"RecessionRisk\"], inplace=True)\n",
    "df_predict_lower"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c7d54fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_predict_upper_copy = df_predict_upper.copy()\n",
    "df_predict_lower_copy = df_predict_lower.copy()\n",
    "df_predict_upper_copy.drop(columns=['Continent', 'EconomyGroup', 'Country'], inplace=True)\n",
    "df_predict_lower_copy.drop(columns=['Continent', 'EconomyGroup', 'Country'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54efeef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset index once for reuse (assumes Year is the index in df_predict_original)\n",
    "df_predict_original_reset = df_predict_original.reset_index()\n",
    "df_predict_upper = df_predict_upper.reset_index()\n",
    "df_predict_lower = df_predict_lower.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40231818",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Global (all countries, full feature set)\n",
    "# ============================================================\n",
    "predictions = make_predictions(models, df_predict)\n",
    "predictions_global_features_with_country = pd.concat(\n",
    "    [df_predict_original_reset[['Year', 'Country']], predictions.reset_index(drop=True)],\n",
    "    axis=1\n",
    ")\n",
    "print(predictions_global_features_with_country.head())\n",
    "plot_recession_counts_per_model(predictions_global_features_with_country, \"Global (Full features)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "792b3997",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Global Restricted (all countries, restricted feature set)\n",
    "# ============================================================\n",
    "df_predict_restricted = df_predict[selected_features]\n",
    "predictions_restricted_features = make_predictions(models_reduced, df_predict_restricted)\n",
    "predictions_global_restricted_features_with_country = pd.concat(\n",
    "    [df_predict_original_reset[['Year', 'Country']], predictions_restricted_features.reset_index(drop=True)],\n",
    "    axis=1\n",
    ")\n",
    "print(predictions_global_restricted_features_with_country.head())\n",
    "plot_recession_counts_per_model(predictions_global_restricted_features_with_country, \"Global (Restricted features)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4193f779",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ============================================================\n",
    "# Upper Economies (full feature set)\n",
    "# ============================================================\n",
    "df_predict_upper_reset = df_predict_upper_copy.reset_index(drop=True)\n",
    "\n",
    "X_predict_upper = df_predict_upper_reset.drop(columns=[\"RecessionRisk\", \"Country\"], errors=\"ignore\")\n",
    "predictions_upper_features = make_predictions(models_upper, X_predict_upper)\n",
    "\n",
    "predictions_upper_features_with_country = pd.concat(\n",
    "    [df_predict_upper[[\"Year\", \"Country\"]], predictions_upper_features.reset_index(drop=True)],\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "print(predictions_upper_features_with_country.head())\n",
    "plot_recession_counts_per_model(predictions_upper_features_with_country, \"Upper economies (Full features)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2742e92a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Lower Economies (full feature set)\n",
    "# ============================================================\n",
    "df_predict_lower_reset = df_predict_lower_copy.reset_index(drop=True)\n",
    "\n",
    "X_predict_lower = df_predict_lower_reset.drop(columns=[\"RecessionRisk\", \"Country\"], errors=\"ignore\")\n",
    "predictions_lower_features = make_predictions(models_lower, X_predict_lower)\n",
    "\n",
    "predictions_lower_features_with_country = pd.concat(\n",
    "    [df_predict_lower[[\"Year\", \"Country\"]], predictions_lower_features.reset_index(drop=True)],\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "print(predictions_lower_features_with_country.head())\n",
    "plot_recession_counts_per_model(predictions_lower_features_with_country, \"Lower economies (Full features)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aa3a9cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Upper Economies (restricted feature set)\n",
    "# ============================================================\n",
    "X_predict_upper_reduced = df_predict_upper[selected_features]\n",
    "df_predict_upper_reset = df_predict_upper_copy.reset_index(drop=True)\n",
    "\n",
    "X_predict_upper_reduced = df_predict_upper_reset[selected_features]\n",
    "predictions_upper_restricted_features = make_predictions(models_upper_reduced, X_predict_upper_reduced)\n",
    "\n",
    "predictions_upper_restricted_features_with_country = pd.concat(\n",
    "    [df_predict_upper[[\"Year\", \"Country\"]], predictions_upper_restricted_features.reset_index(drop=True)],\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "print(predictions_upper_restricted_features_with_country.head())\n",
    "plot_recession_counts_per_model(predictions_upper_restricted_features_with_country, \"Upper economies (Restricted features)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08238524",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Lower Economies (restricted feature set)\n",
    "# ============================================================\n",
    "X_predict_lower_reduced = df_predict_lower[selected_features]\n",
    "df_predict_lower_reset = df_predict_lower_copy.reset_index(drop=True)\n",
    "X_predict_lower_reduced = df_predict_lower_reset[selected_features]\n",
    "predictions_lower_restricted_features = make_predictions(models_lower_reduced, X_predict_lower_reduced)\n",
    "\n",
    "predictions_lower_restricted_features_with_country = pd.concat(\n",
    "    [df_predict_lower[[\"Year\", \"Country\"]], predictions_lower_restricted_features.reset_index(drop=True)],\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "print(predictions_lower_restricted_features_with_country.head())\n",
    "plot_recession_counts_per_model(predictions_lower_restricted_features_with_country, \"Lower economies (Restricted features)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9ca2ee4",
   "metadata": {},
   "source": [
    "# LTSM Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92ce9106",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "# Core layers\n",
    "from tensorflow.keras.layers import (\n",
    "    Input,\n",
    "    Dense,\n",
    "    LSTM,\n",
    "    GRU,\n",
    "    Dropout,\n",
    "    BatchNormalization,\n",
    "    LayerNormalization,\n",
    "    Bidirectional\n",
    ")\n",
    "\n",
    "# Advanced sequence layers\n",
    "from tensorflow.keras.layers import (\n",
    "    Conv1D,\n",
    "    GlobalAveragePooling1D,\n",
    "    GlobalMaxPooling1D,\n",
    "    MultiHeadAttention\n",
    ")\n",
    "\n",
    "# Model utilities\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "\n",
    "# Callbacks\n",
    "from tensorflow.keras.callbacks import (\n",
    "    EarlyStopping,\n",
    "    ReduceLROnPlateau,\n",
    "    ModelCheckpoint\n",
    ")\n",
    "\n",
    "# Optimizers\n",
    "from tensorflow.keras.optimizers import Adam, RMSprop\n",
    "\n",
    "# Losses & metrics\n",
    "from tensorflow.keras.losses import SparseCategoricalCrossentropy\n",
    "from tensorflow.keras.metrics import Accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32da196f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_until_2025 = df_pivot[df_pivot.index <= 2025]\n",
    "df_after_2025 = df_pivot[df_pivot.index > 2025]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "478dda10",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_until_2025"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d2a6b7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_after_2025"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "281dd3b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reshape_for_lstm(df, window=5, min_sequences=1, return_feature_names=False):\n",
    "    \"\"\"\n",
    "    Convert df_pivot (Year index, Country column) into LSTM-ready sequences.\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame with Year index and Country column\n",
    "        window: Number of historical years to use as input\n",
    "        min_sequences: Minimum sequences required per country (default 1)\n",
    "        return_feature_names: If True, return feature column names\n",
    "        \n",
    "    Returns:\n",
    "        X_seq: (num_sequences, window, num_features)\n",
    "        y_seq: (num_sequences,)\n",
    "        countries_seq: country for each sequence\n",
    "        years_seq: ending year for each sequence\n",
    "        feature_names: (optional) list of feature column names\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    \n",
    "    # Handle index - reset if needed for sorting\n",
    "    if isinstance(df.index, pd.RangeIndex):\n",
    "        raise ValueError(\"DataFrame must have Year as index\")\n",
    "    \n",
    "    # Reset index to make sorting easier, then sort\n",
    "    df_sorted = df.reset_index().sort_values([\"Country\", df.index.name])\n",
    "    year_col = df.index.name or \"Year\"\n",
    "    \n",
    "    # Define features (exclude target and identifiers)\n",
    "    feature_cols = [c for c in df_sorted.columns \n",
    "                   if c not in [\"RecessionRisk\", \"Country\", year_col]]\n",
    "    \n",
    "    X_seq = []\n",
    "    y_seq = []\n",
    "    countries_seq = []\n",
    "    years_seq = []\n",
    "    \n",
    "    # Group by country\n",
    "    for country, group in df_sorted.groupby(\"Country\"):\n",
    "        group = group.sort_values(year_col).reset_index(drop=True)\n",
    "        \n",
    "        # Check if country has enough data\n",
    "        if len(group) < window + 1:\n",
    "            print(f\"Warning: {country} has only {len(group)} years, need {window + 1}. Skipping.\")\n",
    "            continue\n",
    "        \n",
    "        X_values = group[feature_cols].values\n",
    "        y_values = group[\"RecessionRisk\"].values\n",
    "        years = group[year_col].values\n",
    "        \n",
    "        # Sliding window\n",
    "        num_sequences = len(group) - window\n",
    "        for i in range(num_sequences):\n",
    "            X_seq.append(X_values[i:i+window])\n",
    "            y_seq.append(y_values[i+window])\n",
    "            countries_seq.append(country)\n",
    "            years_seq.append(years[i+window])\n",
    "    \n",
    "    if len(X_seq) < min_sequences:\n",
    "        raise ValueError(f\"Only generated {len(X_seq)} sequences, need at least {min_sequences}\")\n",
    "    \n",
    "    results = (\n",
    "        np.array(X_seq),\n",
    "        np.array(y_seq),\n",
    "        np.array(countries_seq),\n",
    "        np.array(years_seq)\n",
    "    )\n",
    "    \n",
    "    if return_feature_names:\n",
    "        results = results + (feature_cols,)\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19fa586c",
   "metadata": {},
   "outputs": [],
   "source": [
    "WINDOW = 5\n",
    "\n",
    "X_seq, y_seq, countries_seq, years_seq = reshape_for_lstm(\n",
    "    df_until_2025,\n",
    "    window=WINDOW\n",
    ")\n",
    "\n",
    "print(\"X_seq shape:\", X_seq.shape)\n",
    "print(\"y_seq shape:\", y_seq.shape)\n",
    "print(\"Example sequence shape:\", X_seq[0].shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f1805a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option 1: Simple time-based split (most common for time series)\n",
    "# Train on earlier years, test on most recent years\n",
    "SPLIT_YEAR = 2019  # Adjust based on your data\n",
    "\n",
    "train_mask = years_seq <= SPLIT_YEAR\n",
    "test_mask = years_seq > SPLIT_YEAR\n",
    "\n",
    "# This gives you:\n",
    "# Train: all sequences predicting up to 2019\n",
    "# Test: sequences predicting 2020,2021,2022,2023, 2024, 2025\n",
    "\n",
    "X_train, X_test = X_seq[train_mask], X_seq[test_mask]\n",
    "y_train, y_test = y_seq[train_mask], y_seq[test_mask]\n",
    "countries_train = countries_seq[train_mask]\n",
    "countries_test  = countries_seq[test_mask]\n",
    "years_train = years_seq[train_mask]\n",
    "years_test  = years_seq[test_mask]\n",
    "\n",
    "print(f\"Train sequences: {len(X_train)} (predicting years {years_train.min()}-{years_train.max()})\")\n",
    "print(f\"Test sequences: {len(X_test)} (predicting years {years_test.min()}-{years_test.max()})\")\n",
    "print(f\"Train: {np.sum(train_mask)} sequences, Test: {np.sum(test_mask)} sequences\")\n",
    "print(f\"Split: {np.sum(train_mask)/(np.sum(train_mask)+np.sum(test_mask))*100:.1f}% train, {np.sum(test_mask)/(np.sum(train_mask)+np.sum(test_mask))*100:.1f}% test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc06604a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPROVED LSTM MODEL - ENHANCED PERFORMANCE\n",
    "# Key improvements:\n",
    "# 1. Better regularization (L2, gradient clipping)\n",
    "# 2. Improved architecture with residual connections\n",
    "# 3. AdamW optimizer with weight decay\n",
    "# 4. Cosine annealing learning rate\n",
    "# 5. Model checkpointing\n",
    "# 6. Label smoothing\n",
    "# 7. Better dropout strategy\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "# Step 1: Scale the features\n",
    "X_train_reshaped = X_train.reshape(-1, X_train.shape[-1])\n",
    "X_test_reshaped = X_test.reshape(-1, X_test.shape[-1])\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train_reshaped)\n",
    "X_test_scaled = scaler.transform(X_test_reshaped)\n",
    "\n",
    "X_train_scaled = X_train_scaled.reshape(X_train.shape)\n",
    "X_test_scaled = X_test_scaled.reshape(X_test.shape)\n",
    "\n",
    "print(\"Data scaled successfully\")\n",
    "print(f\"X_train_scaled shape: {X_train_scaled.shape}\")\n",
    "print(f\"X_test_scaled shape: {X_test_scaled.shape}\")\n",
    "print(\"\\n\" + \"=\"*70 + \"\\n\")\n",
    "\n",
    "# Step 2: Check class distribution\n",
    "print(\"Unique RecessionRisk classes:\", np.unique(y_seq))\n",
    "print(\"\\nClass distribution in train:\")\n",
    "train_dist = pd.Series(y_train).value_counts().sort_index()\n",
    "print(train_dist)\n",
    "print(\"\\nClass distribution in test:\")\n",
    "test_dist = pd.Series(y_test).value_counts().sort_index()\n",
    "print(test_dist)\n",
    "\n",
    "num_classes = len(np.unique(y_seq))\n",
    "print(f\"\\nNumber of classes: {num_classes}\")\n",
    "print(\"\\n\" + \"=\"*70 + \"\\n\")\n",
    "\n",
    "# Step 3: Enhanced SMOTE with strategic resampling\n",
    "print(\"Applying SMOTE to balance training data...\")\n",
    "X_train_flat = X_train_scaled.reshape(X_train_scaled.shape[0], -1)\n",
    "\n",
    "# Use SMOTE with adaptive k_neighbors\n",
    "min_class_count = train_dist.min()\n",
    "k_neighbors = min(3, min_class_count - 1) if min_class_count > 1 else 1\n",
    "\n",
    "smote = SMOTE(random_state=42, k_neighbors=k_neighbors)\n",
    "X_train_resampled, y_train_resampled = smote.fit_resample(X_train_flat, y_train)\n",
    "X_train_resampled = X_train_resampled.reshape(-1, WINDOW, X_train_scaled.shape[2])\n",
    "\n",
    "print(f\"Original training size: {len(X_train_scaled)}\")\n",
    "print(f\"Resampled training size: {len(X_train_resampled)}\")\n",
    "print(\"\\nResampled class distribution:\")\n",
    "print(pd.Series(y_train_resampled).value_counts().sort_index())\n",
    "print(\"\\n\" + \"=\"*70 + \"\\n\")\n",
    "\n",
    "# Step 4: Build improved model with better architecture\n",
    "print(\"Building improved LSTM model...\")\n",
    "\n",
    "# Input layer\n",
    "inputs = keras.Input(shape=(WINDOW, X_train_scaled.shape[2]), name='input')\n",
    "\n",
    "# Feature extraction with 1D convolution (helps capture local patterns)\n",
    "x = keras.layers.Conv1D(64, kernel_size=3, padding='same', activation='relu', \n",
    "                        kernel_regularizer=keras.regularizers.l2(0.001))(inputs)\n",
    "x = keras.layers.BatchNormalization()(x)\n",
    "x = keras.layers.Dropout(0.2)(x)\n",
    "\n",
    "# First BiLSTM block with residual connection\n",
    "lstm1 = keras.layers.Bidirectional(\n",
    "    keras.layers.LSTM(128, return_sequences=True, \n",
    "                     kernel_regularizer=keras.regularizers.l2(0.001),\n",
    "                     recurrent_regularizer=keras.regularizers.l2(0.001))\n",
    ")(x)\n",
    "lstm1 = keras.layers.LayerNormalization()(lstm1)\n",
    "lstm1 = keras.layers.Dropout(0.3)(lstm1)\n",
    "\n",
    "# Second BiLSTM block\n",
    "lstm2 = keras.layers.Bidirectional(\n",
    "    keras.layers.LSTM(64, return_sequences=True,\n",
    "                     kernel_regularizer=keras.regularizers.l2(0.001),\n",
    "                     recurrent_regularizer=keras.regularizers.l2(0.001))\n",
    ")(lstm1)\n",
    "lstm2 = keras.layers.LayerNormalization()(lstm2)\n",
    "lstm2 = keras.layers.Dropout(0.3)(lstm2)\n",
    "\n",
    "# Enhanced multi-head attention with residual\n",
    "attention = keras.layers.MultiHeadAttention(\n",
    "    num_heads=4, key_dim=32, dropout=0.1\n",
    ")(lstm2, lstm2)\n",
    "attention = keras.layers.LayerNormalization()(attention)\n",
    "x = keras.layers.Add()([lstm2, attention])  # Residual connection\n",
    "x = keras.layers.Dropout(0.2)(x)\n",
    "\n",
    "# Dual pooling strategy (both average and max)\n",
    "avg_pool = keras.layers.GlobalAveragePooling1D()(x)\n",
    "max_pool = keras.layers.GlobalMaxPooling1D()(x)\n",
    "x = keras.layers.Concatenate()([avg_pool, max_pool])\n",
    "\n",
    "# Dense layers with better regularization\n",
    "x = keras.layers.Dense(128, activation='relu', \n",
    "                      kernel_regularizer=keras.regularizers.l2(0.001))(x)\n",
    "x = keras.layers.BatchNormalization()(x)\n",
    "x = keras.layers.Dropout(0.4)(x)\n",
    "\n",
    "x = keras.layers.Dense(64, activation='relu',\n",
    "                      kernel_regularizer=keras.regularizers.l2(0.001))(x)\n",
    "x = keras.layers.BatchNormalization()(x)\n",
    "x = keras.layers.Dropout(0.3)(x)\n",
    "\n",
    "x = keras.layers.Dense(32, activation='relu',\n",
    "                      kernel_regularizer=keras.regularizers.l2(0.001))(x)\n",
    "x = keras.layers.Dropout(0.2)(x)\n",
    "\n",
    "# Output layer\n",
    "outputs = keras.layers.Dense(num_classes, activation='softmax', name='output')(x)\n",
    "\n",
    "model = keras.Model(inputs=inputs, outputs=outputs, name='ImprovedLSTM')\n",
    "model.summary()\n",
    "print(\"\\n\" + \"=\"*70 + \"\\n\")\n",
    "\n",
    "# Step 5: Improved Focal Loss with label smoothing\n",
    "class ImprovedFocalLoss(keras.losses.Loss):\n",
    "    \"\"\"Enhanced Focal Loss with label smoothing\"\"\"\n",
    "    def __init__(self, gamma=2.0, alpha=None, label_smoothing=0.1):\n",
    "        super().__init__()\n",
    "        self.gamma = gamma\n",
    "        self.alpha = alpha\n",
    "        self.label_smoothing = label_smoothing\n",
    "        \n",
    "    def call(self, y_true, y_pred):\n",
    "        y_true = tf.cast(y_true, tf.int32)\n",
    "        y_true = tf.reshape(y_true, [-1])\n",
    "        \n",
    "        # Apply label smoothing\n",
    "        y_true_onehot = tf.one_hot(y_true, depth=num_classes)\n",
    "        y_true_smooth = y_true_onehot * (1 - self.label_smoothing) + \\\n",
    "                       (self.label_smoothing / num_classes)\n",
    "        \n",
    "        epsilon = tf.keras.backend.epsilon()\n",
    "        y_pred = tf.clip_by_value(y_pred, epsilon, 1.0 - epsilon)\n",
    "        \n",
    "        # Focal loss calculation\n",
    "        cross_entropy = -y_true_smooth * tf.math.log(y_pred)\n",
    "        weight = tf.math.pow(1.0 - y_pred, self.gamma)\n",
    "        \n",
    "        # Apply class weights\n",
    "        if self.alpha is not None:\n",
    "            alpha_t = tf.gather(self.alpha, y_true)\n",
    "            alpha_t = tf.reshape(alpha_t, [-1, 1])\n",
    "            weight = weight * alpha_t\n",
    "        \n",
    "        focal_loss = weight * cross_entropy\n",
    "        return tf.reduce_mean(tf.reduce_sum(focal_loss, axis=-1))\n",
    "\n",
    "# Calculate balanced class weights\n",
    "total_samples = len(y_train_resampled)\n",
    "class_counts = np.bincount(y_train_resampled)\n",
    "alpha_weights = tf.constant([\n",
    "    np.sqrt(total_samples / (num_classes * count)) for count in class_counts\n",
    "], dtype=tf.float32)\n",
    "\n",
    "print(f\"Class weights: {alpha_weights.numpy()}\")\n",
    "print(\"\\n\" + \"=\"*70 + \"\\n\")\n",
    "\n",
    "# Step 6: Compile with AdamW optimizer (Adam with weight decay)\n",
    "# Using clipnorm for gradient clipping\n",
    "model.compile(\n",
    "    optimizer=keras.optimizers.AdamW(\n",
    "        learning_rate=0.001,\n",
    "        weight_decay=0.0001,  # L2 regularization via weight decay\n",
    "        clipnorm=1.0  # Gradient clipping\n",
    "    ),\n",
    "    loss=ImprovedFocalLoss(gamma=2.0, alpha=alpha_weights, label_smoothing=0.05),\n",
    "    metrics=['accuracy', \n",
    "             keras.metrics.Precision(name='precision'),\n",
    "             keras.metrics.Recall(name='recall')]\n",
    ")\n",
    "\n",
    "print(\"Model compiled with AdamW + Improved Focal Loss + Label Smoothing\")\n",
    "print(\"\\n\" + \"=\"*70 + \"\\n\")\n",
    "\n",
    "# Step 7: Enhanced callbacks with cosine annealing\n",
    "import tempfile\n",
    "import os\n",
    "\n",
    "# Create temp directory for model checkpoint\n",
    "checkpoint_dir = tempfile.mkdtemp()\n",
    "checkpoint_path = os.path.join(checkpoint_dir, 'best_model.keras')\n",
    "\n",
    "# Cosine annealing schedule\n",
    "def cosine_annealing(epoch, lr, total_epochs=150, min_lr=1e-7):\n",
    "    \"\"\"Cosine annealing learning rate schedule\"\"\"\n",
    "    import math\n",
    "    if epoch < 10:  # Warmup period\n",
    "        return lr\n",
    "    cos_inner = math.pi * (epoch - 10) / (total_epochs - 10)\n",
    "    return min_lr + (0.001 - min_lr) / 2 * (1 + math.cos(cos_inner))\n",
    "\n",
    "callbacks = [\n",
    "    keras.callbacks.EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        patience=30,\n",
    "        restore_best_weights=True,\n",
    "        verbose=1,\n",
    "        mode='min'\n",
    "    ),\n",
    "    keras.callbacks.ModelCheckpoint(\n",
    "        filepath=checkpoint_path,\n",
    "        monitor='val_loss',\n",
    "        save_best_only=True,\n",
    "        verbose=1,\n",
    "        mode='min'\n",
    "    ),\n",
    "    keras.callbacks.ReduceLROnPlateau(\n",
    "        monitor='val_loss',\n",
    "        factor=0.5,\n",
    "        patience=12,\n",
    "        min_lr=1e-7,\n",
    "        verbose=1,\n",
    "        mode='min'\n",
    "    ),\n",
    "    keras.callbacks.LearningRateScheduler(\n",
    "        lambda epoch, lr: cosine_annealing(epoch, lr),\n",
    "        verbose=0\n",
    "    )\n",
    "]\n",
    "\n",
    "print(\"Enhanced callbacks configured:\")\n",
    "print(\"- Early Stopping (patience=30)\")\n",
    "print(\"- Model Checkpoint (saves best model)\")\n",
    "print(\"- ReduceLROnPlateau (patience=12)\")\n",
    "print(\"- Cosine Annealing LR Schedule\")\n",
    "print(\"\\n\" + \"=\"*70 + \"\\n\")\n",
    "\n",
    "# Step 8: Train with improved settings\n",
    "print(\"Starting training...\")\n",
    "history = model.fit(\n",
    "    X_train_resampled, y_train_resampled,\n",
    "    validation_data=(X_test_scaled, y_test),\n",
    "    epochs=150,\n",
    "    batch_size=32,\n",
    "    shuffle=True,\n",
    "    callbacks=callbacks,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70 + \"\\n\")\n",
    "\n",
    "# Step 9: Load best model and evaluate\n",
    "if os.path.exists(checkpoint_path):\n",
    "    print(\"Loading best model from checkpoint...\")\n",
    "    model = keras.models.load_model(\n",
    "        checkpoint_path,\n",
    "        custom_objects={'ImprovedFocalLoss': ImprovedFocalLoss}\n",
    "    )\n",
    "\n",
    "test_results = model.evaluate(X_test_scaled, y_test, verbose=0)\n",
    "print(f\"Test Loss: {test_results[0]:.4f}\")\n",
    "print(f\"Test Accuracy: {test_results[1]:.4f}\")\n",
    "print(f\"Test Precision: {test_results[2]:.4f}\")\n",
    "print(f\"Test Recall: {test_results[3]:.4f}\")\n",
    "print(\"\\n\" + \"=\"*70 + \"\\n\")\n",
    "\n",
    "# Step 10: Generate predictions\n",
    "y_pred = model.predict(X_test_scaled, verbose=0)\n",
    "y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "\n",
    "# Step 11: Calculate comprehensive metrics\n",
    "precision_weighted = precision_score(y_test, y_pred_classes, average='weighted', zero_division=0)\n",
    "recall_weighted = recall_score(y_test, y_pred_classes, average='weighted', zero_division=0)\n",
    "f1_weighted = f1_score(y_test, y_pred_classes, average='weighted', zero_division=0)\n",
    "f1_macro = f1_score(y_test, y_pred_classes, average='macro', zero_division=0)\n",
    "\n",
    "print(f\"Test Precision (weighted): {precision_weighted:.4f}\")\n",
    "print(f\"Test Recall (weighted): {recall_weighted:.4f}\")\n",
    "print(f\"Test F1-Score (weighted): {f1_weighted:.4f}\")\n",
    "print(f\"Test F1-Score (macro): {f1_macro:.4f}\")\n",
    "print(\"\\n\" + \"=\"*70 + \"\\n\")\n",
    "\n",
    "# Step 12: Per-class performance\n",
    "print(\"Per-class performance:\")\n",
    "print(\"-\" * 70)\n",
    "for cls in range(num_classes):\n",
    "    cls_mask = y_test == cls\n",
    "    if np.sum(cls_mask) > 0:\n",
    "        cls_acc = np.mean(y_pred_classes[cls_mask] == cls)\n",
    "        cls_count = np.sum(cls_mask)\n",
    "        predicted_as_cls = np.sum(y_pred_classes == cls)\n",
    "        \n",
    "        cls_precision = precision_score(y_test == cls, y_pred_classes == cls, zero_division=0)\n",
    "        cls_recall = recall_score(y_test == cls, y_pred_classes == cls, zero_division=0)\n",
    "        cls_f1 = f1_score(y_test == cls, y_pred_classes == cls, zero_division=0)\n",
    "        \n",
    "        print(f\"Class {cls}:\")\n",
    "        print(f\"  Actual: {cls_count:>3} | Predicted: {predicted_as_cls:>3} | \" + \n",
    "              f\"Acc: {cls_acc:.3f} | P: {cls_precision:.3f} | R: {cls_recall:.3f} | F1: {cls_f1:.3f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70 + \"\\n\")\n",
    "\n",
    "# Step 13: Enhanced confusion matrix visualization\n",
    "cm = confusion_matrix(y_test, y_pred_classes)\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Absolute numbers\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='RdYlGn_r', ax=ax1,\n",
    "            xticklabels=[f'Class {i}' for i in range(num_classes)],\n",
    "            yticklabels=[f'Class {i}' for i in range(num_classes)],\n",
    "            linewidths=0.5, linecolor='gray')\n",
    "ax1.set_xlabel('Predicted', fontsize=12, fontweight='bold')\n",
    "ax1.set_ylabel('Actual', fontsize=12, fontweight='bold')\n",
    "ax1.set_title(f'Confusion Matrix (Counts)\\nAccuracy: {test_results[1]:.3f}', \n",
    "              fontsize=14, fontweight='bold')\n",
    "\n",
    "# Percentages\n",
    "cm_percent = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis] * 100\n",
    "sns.heatmap(cm_percent, annot=True, fmt='.1f', cmap='RdYlGn_r', ax=ax2,\n",
    "            xticklabels=[f'Class {i}' for i in range(num_classes)],\n",
    "            yticklabels=[f'Class {i}' for i in range(num_classes)],\n",
    "            linewidths=0.5, linecolor='gray')\n",
    "ax2.set_xlabel('Predicted', fontsize=12, fontweight='bold')\n",
    "ax2.set_ylabel('Actual', fontsize=12, fontweight='bold')\n",
    "ax2.set_title(f'Confusion Matrix (Percentages)\\nF1 (macro): {f1_macro:.3f}', \n",
    "              fontsize=14, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Step 14: Classification report\n",
    "print(\"Classification Report:\")\n",
    "print(\"=\" * 70)\n",
    "print(classification_report(y_test, y_pred_classes, \n",
    "                          target_names=[f'Class {i}' for i in range(num_classes)],\n",
    "                          zero_division=0))\n",
    "\n",
    "# Step 15: Enhanced training history visualization\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 10))\n",
    "\n",
    "# Loss\n",
    "axes[0, 0].plot(history.history['loss'], label='Train Loss', linewidth=2, alpha=0.8)\n",
    "axes[0, 0].plot(history.history['val_loss'], label='Val Loss', linewidth=2, alpha=0.8)\n",
    "axes[0, 0].set_xlabel('Epoch', fontsize=11)\n",
    "axes[0, 0].set_ylabel('Loss', fontsize=11)\n",
    "axes[0, 0].legend(fontsize=10)\n",
    "axes[0, 0].set_title('Training and Validation Loss', fontsize=13, fontweight='bold')\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Accuracy\n",
    "axes[0, 1].plot(history.history['accuracy'], label='Train Accuracy', linewidth=2, alpha=0.8)\n",
    "axes[0, 1].plot(history.history['val_accuracy'], label='Val Accuracy', linewidth=2, alpha=0.8)\n",
    "axes[0, 1].set_xlabel('Epoch', fontsize=11)\n",
    "axes[0, 1].set_ylabel('Accuracy', fontsize=11)\n",
    "axes[0, 1].legend(fontsize=10)\n",
    "axes[0, 1].set_title('Training and Validation Accuracy', fontsize=13, fontweight='bold')\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# Precision\n",
    "axes[1, 0].plot(history.history['precision'], label='Train Precision', linewidth=2, alpha=0.8)\n",
    "axes[1, 0].plot(history.history['val_precision'], label='Val Precision', linewidth=2, alpha=0.8)\n",
    "axes[1, 0].set_xlabel('Epoch', fontsize=11)\n",
    "axes[1, 0].set_ylabel('Precision', fontsize=11)\n",
    "axes[1, 0].legend(fontsize=10)\n",
    "axes[1, 0].set_title('Training and Validation Precision', fontsize=13, fontweight='bold')\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Recall\n",
    "axes[1, 1].plot(history.history['recall'], label='Train Recall', linewidth=2, alpha=0.8)\n",
    "axes[1, 1].plot(history.history['val_recall'], label='Val Recall', linewidth=2, alpha=0.8)\n",
    "axes[1, 1].set_xlabel('Epoch', fontsize=11)\n",
    "axes[1, 1].set_ylabel('Recall', fontsize=11)\n",
    "axes[1, 1].legend(fontsize=10)\n",
    "axes[1, 1].set_title('Training and Validation Recall', fontsize=13, fontweight='bold')\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Step 16: Prediction distribution analysis\n",
    "print(\"\\nPrediction Distribution Analysis:\")\n",
    "print(\"-\" * 70)\n",
    "print(f\"{'Class':<10} {'Actual':<15} {'Predicted':<15} {'Difference':<15}\")\n",
    "print(\"-\" * 70)\n",
    "for cls in range(num_classes):\n",
    "    actual = np.sum(y_test == cls)\n",
    "    predicted = np.sum(y_pred_classes == cls)\n",
    "    diff = predicted - actual\n",
    "    actual_pct = actual / len(y_test) * 100\n",
    "    pred_pct = predicted / len(y_pred_classes) * 100\n",
    "    print(f\"Class {cls}    {actual} ({actual_pct:>5.1f}%)    \" + \n",
    "          f\"{predicted} ({pred_pct:>5.1f}%)    {diff:+4d} ({pred_pct-actual_pct:+5.1f}%)\")\n",
    "\n",
    "# Step 17: Enhanced confidence analysis\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"\\nPrediction Confidence Analysis:\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "correct_mask = y_pred_classes == y_test\n",
    "if np.sum(correct_mask) > 0:\n",
    "    correct_conf = np.max(y_pred[correct_mask], axis=1)\n",
    "    print(f\"Correct predictions   (n={np.sum(correct_mask):>3}): \" + \n",
    "          f\"avg={np.mean(correct_conf):.3f}, median={np.median(correct_conf):.3f}, \" +\n",
    "          f\"std={np.std(correct_conf):.3f}\")\n",
    "          \n",
    "if np.sum(~correct_mask) > 0:\n",
    "    incorrect_conf = np.max(y_pred[~correct_mask], axis=1)\n",
    "    print(f\"Incorrect predictions (n={np.sum(~correct_mask):>3}): \" + \n",
    "          f\"avg={np.mean(incorrect_conf):.3f}, median={np.median(incorrect_conf):.3f}, \" +\n",
    "          f\"std={np.std(incorrect_conf):.3f}\")\n",
    "\n",
    "print(\"\\nPer-class confidence:\")\n",
    "for cls in range(num_classes):\n",
    "    cls_pred_mask = y_pred_classes == cls\n",
    "    if np.sum(cls_pred_mask) > 0:\n",
    "        cls_confidences = np.max(y_pred[cls_pred_mask], axis=1)\n",
    "        print(f\"Class {cls}: n={np.sum(cls_pred_mask):>3}, \" +\n",
    "              f\"avg={np.mean(cls_confidences):.3f}, \" +\n",
    "              f\"min={np.min(cls_confidences):.3f}, \" +\n",
    "              f\"max={np.max(cls_confidences):.3f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"\\nâœ… IMPROVED MODEL TRAINING COMPLETE\")\n",
    "print(\"Key improvements implemented:\")\n",
    "print(\"  â€¢ L2 regularization on all layers\")\n",
    "print(\"  â€¢ AdamW optimizer with weight decay\")\n",
    "print(\"  â€¢ Gradient clipping (clipnorm=1.0)\")\n",
    "print(\"  â€¢ Label smoothing (0.05)\")\n",
    "print(\"  â€¢ Cosine annealing LR schedule\")\n",
    "print(\"  â€¢ Dual pooling (avg + max)\")\n",
    "print(\"  â€¢ Enhanced multi-head attention\")\n",
    "print(\"  â€¢ Model checkpointing\")\n",
    "print(\"=\"*70)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
